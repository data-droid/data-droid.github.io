---
layout: post
lang: ko
title: "Apache Airflow ì‹¬í™” ê°€ì´ë“œ: DAG ìµœì í™”ë¶€í„° ëª¨ë‹ˆí„°ë§ê¹Œì§€"
description: "ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Apache Airflowì˜ ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ëª¨ë²” ì‚¬ë¡€ë¥¼ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤."
date: 2025-09-08
author: Data Droid
category: data-engineering
tags: [Apache-Airflow, ë°ì´í„°íŒŒì´í”„ë¼ì¸, ì›Œí¬í”Œë¡œìš°, DAG, ìŠ¤ì¼€ì¤„ë§, ëª¨ë‹ˆí„°ë§, ë°ì´í„°ì—”ì§€ë‹ˆì–´ë§]
reading_time: "25ë¶„"
difficulty: "ê³ ê¸‰"
---

# Apache Airflow ì‹¬í™” ê°€ì´ë“œ: DAG ìµœì í™”ë¶€í„° ëª¨ë‹ˆí„°ë§ê¹Œì§€

> ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Apache Airflowì˜ ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ëª¨ë²” ì‚¬ë¡€ë¥¼ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Airflow ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´](#airflow-ì•„í‚¤í…ì²˜-ì‹¬í™”-ì´í•´)
2. [DAG ìµœì í™” ì „ëµ](#dag-ìµœì í™”-ì „ëµ)
3. [ê³ ê¸‰ ìŠ¤ì¼€ì¤„ë§ê³¼ íŠ¸ë¦¬ê±°](#ê³ ê¸‰-ìŠ¤ì¼€ì¤„ë§ê³¼-íŠ¸ë¦¬ê±°)
4. [ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„ ì „ëµ](#ì—ëŸ¬-ì²˜ë¦¬ì™€-ì¬ì‹œë„-ì „ëµ)
5. [ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ](#ëª¨ë‹ˆí„°ë§ê³¼-ì•Œë¦¼-ì‹œìŠ¤í…œ)
6. [ì‹¤ìŠµ: ì‹¤ë¬´ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•](#ì‹¤ìŠµ-ì‹¤ë¬´ê¸‰-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-êµ¬ì¶•)
7. [ì„±ëŠ¥ íŠœë‹ê³¼ í™•ì¥ì„±](#ì„±ëŠ¥-íŠœë‹ê³¼-í™•ì¥ì„±)
8. [ë³´ì•ˆê³¼ ê¶Œí•œ ê´€ë¦¬](#ë³´ì•ˆê³¼-ê¶Œí•œ-ê´€ë¦¬)
9. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ— ï¸ Airflow ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´ {#airflow-ì•„í‚¤í…ì²˜-ì‹¬í™”-ì´í•´}

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„

Apache AirflowëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

#### **1. Scheduler**
- **ì—­í• **: DAG íŒŒì‹±, Task ì¸ìŠ¤í„´ìŠ¤ ìƒì„±, ì‹¤í–‰ ìŠ¤ì¼€ì¤„ë§
- **ë™ì‘ ì›ë¦¬**: ë©”íƒ€ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ DAG ìƒíƒœë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸
- **ì„±ëŠ¥ ìµœì í™”**: ë³‘ë ¬ ì²˜ë¦¬, DAG íŒŒì‹± ìºì‹±

#### **2. Executor**
- **LocalExecutor**: ë‹¨ì¼ ë¨¸ì‹ ì—ì„œ ìˆœì°¨ ì‹¤í–‰
- **CeleryExecutor**: ë¶„ì‚° í™˜ê²½ì—ì„œ ë³‘ë ¬ ì‹¤í–‰
- **KubernetesExecutor**: ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì‹¤í–‰
- **LocalKubernetesExecutor**: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ë°©ì‹

#### **3. Webserver**
- **UI ì œê³µ**: DAG ëª¨ë‹ˆí„°ë§, ë¡œê·¸ í™•ì¸, ìˆ˜ë™ ì‹¤í–‰
- **REST API**: ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™
- **ì¸ì¦/ê¶Œí•œ**: ë³´ì•ˆ ê´€ë¦¬

#### **4. Metadata Database**
- **PostgreSQL/MySQL**: ë©”íƒ€ë°ì´í„° ì €ì¥
- **ì—°ê²° ì •ë³´**: Connection, Variable, XCom
- **ì‹¤í–‰ íˆìŠ¤í† ë¦¬**: Task ì¸ìŠ¤í„´ìŠ¤, ë¡œê·¸ ì •ë³´

### ë©”íƒ€ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´í•´

```sql
-- ì£¼ìš” í…Œì´ë¸” êµ¬ì¡°
-- dag: DAG ë©”íƒ€ë°ì´í„°
-- dag_run: DAG ì‹¤í–‰ ì¸ìŠ¤í„´ìŠ¤
-- task_instance: Task ì‹¤í–‰ ì¸ìŠ¤í„´ìŠ¤
-- log: ì‹¤í–‰ ë¡œê·¸
-- connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
-- variable: ì „ì—­ ë³€ìˆ˜
-- xcom: Task ê°„ ë°ì´í„° ì „ë‹¬
```

## âš¡ DAG ìµœì í™” ì „ëµ {#dag-ìµœì í™”-ì „ëµ}

### 1. DAG êµ¬ì¡° ìµœì í™”

#### **íš¨ìœ¨ì ì¸ Task ì •ì˜**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.task_group import TaskGroup

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,  # ê³¼ê±° ì‹¤í–‰ ê±´ë„ˆë›°ê¸°
    'max_active_runs': 1,  # ë™ì‹œ ì‹¤í–‰ ì œí•œ
}

# DAG ì •ì˜
dag = DAG(
    'advanced_data_pipeline',
    default_args=default_args,
    description='ê³ ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    max_active_runs=1,
    tags=['data-engineering', 'etl'],
)

# Task Groupì„ í™œìš©í•œ êµ¬ì¡°í™”
with TaskGroup("data_extraction", tooltip="ë°ì´í„° ì¶”ì¶œ") as extraction_group:
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    file_sensor = FileSensor(
        task_id='check_source_file',
        filepath='/data/input/sales_data.csv',
        poke_interval=30,
        timeout=300,
    )
    
    # ë°ì´í„° ê²€ì¦
    validate_data = PythonOperator(
        task_id='validate_data',
        python_callable=validate_source_data,
        op_kwargs={'file_path': '/data/input/sales_data.csv'},
    )

# ë©”ì¸ ì²˜ë¦¬ ë¡œì§
with TaskGroup("data_processing", tooltip="ë°ì´í„° ì²˜ë¦¬") as processing_group:
    # ë°ì´í„° ë³€í™˜
    transform_data = PythonOperator(
        task_id='transform_data',
        python_callable=transform_sales_data,
        pool='data_processing_pool',  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš©
        pool_slots=2,
    )
    
    # ë°ì´í„° ë¡œë“œ
    load_data = PythonOperator(
        task_id='load_data',
        python_callable=load_to_warehouse,
        trigger_rule='all_success',
    )

# ì˜ì¡´ì„± ì„¤ì •
extraction_group >> processing_group
```

### 2. ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ìµœì í™”

#### **Poolê³¼ Queue í™œìš©**

```python
# airflow.cfg ì„¤ì •
[celery]
worker_concurrency = 16
worker_precheck = True

# Pool ì •ì˜
from airflow.models import Pool

# ë¦¬ì†ŒìŠ¤ í’€ ìƒì„±
Pool.create(
    pool_name='data_processing_pool',
    slots=4,
    description='ë°ì´í„° ì²˜ë¦¬ ì „ìš© í’€'
)

Pool.create(
    pool_name='heavy_computation_pool',
    slots=2,
    description='ë¬´ê±°ìš´ ì—°ì‚° ì „ìš© í’€'
)

# Taskì—ì„œ Pool ì‚¬ìš©
heavy_task = PythonOperator(
    task_id='heavy_computation',
    python_callable=heavy_computation_function,
    pool='heavy_computation_pool',
    pool_slots=1,
)
```

### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
def process_large_dataset(**context):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
    chunk_size = 10000
    
    for chunk in pd.read_csv(
        '/data/large_dataset.csv', 
        chunksize=chunk_size
    ):
        # ì²­í¬ë³„ ì²˜ë¦¬
        processed_chunk = process_chunk(chunk)
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        save_intermediate_result(processed_chunk)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del processed_chunk
        gc.collect()

# XCom ëŒ€ì‹  íŒŒì¼ ê¸°ë°˜ ë°ì´í„° ì „ë‹¬
def save_large_data(**context):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    large_data = context['task_instance'].xcom_pull(task_ids='previous_task')
    
    # íŒŒì¼ë¡œ ì €ì¥
    with open('/tmp/large_data.pkl', 'wb') as f:
        pickle.dump(large_data, f)
    
    return '/tmp/large_data.pkl'

def load_large_data(**context):
    """íŒŒì¼ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë“œ"""
    file_path = context['task_instance'].xcom_pull(task_ids='save_task')
    
    with open(file_path, 'rb') as f:
        return pickle.load(f)
```

## â° ê³ ê¸‰ ìŠ¤ì¼€ì¤„ë§ê³¼ íŠ¸ë¦¬ê±°

### 1. ë™ì  ìŠ¤ì¼€ì¤„ë§

```python
# ì¡°ê±´ë¶€ ìŠ¤ì¼€ì¤„ë§
def should_run_dag(**context):
    """DAG ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
    # íŠ¹ì • ì¡°ê±´ì—ì„œë§Œ ì‹¤í–‰
    if datetime.now().weekday() in [0, 2, 4]:  # ì›”, ìˆ˜, ê¸ˆ
        return True
    return False

# ì¡°ê±´ë¶€ DAG
dag = DAG(
    'conditional_dag',
    schedule_interval='0 9 * * *',
    catchup=False,
)

# ì¡°ê±´ë¶€ ì‹¤í–‰
conditional_task = BranchPythonOperator(
    task_id='check_conditions',
    python_callable=should_run_dag,
    dag=dag,
)

# ë™ì  Task ìƒì„±
def create_dynamic_tasks(**context):
    """ì‹¤í–‰ ì‹œì ì— ë™ì ìœ¼ë¡œ Task ìƒì„±"""
    # ì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ Task ëª©ë¡ ì¡°íšŒ
    task_list = get_external_task_list()
    
    for task_info in task_list:
        task = PythonOperator(
            task_id=f"dynamic_task_{task_info['id']}",
            python_callable=execute_dynamic_task,
            op_kwargs={'task_info': task_info},
        )
        yield task
```

### 2. ì™¸ë¶€ íŠ¸ë¦¬ê±° í™œìš©

```python
# REST APIë¥¼ í†µí•œ ì™¸ë¶€ íŠ¸ë¦¬ê±°
from airflow.api_connexion.endpoints import dag_run_endpoint
from airflow.models import DagRun

# ì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ DAG ì‹¤í–‰
def trigger_dag_externally(dag_id, conf=None):
    """ì™¸ë¶€ì—ì„œ DAG ì‹¤í–‰"""
    dag_run = DagRun.create(
        dag_id=dag_id,
        execution_date=datetime.now(),
        state='running',
        conf=conf,
    )
    return dag_run

# ì›¹í›…ì„ í†µí•œ íŠ¸ë¦¬ê±°
from airflow.operators.http_operator import SimpleHttpOperator

webhook_trigger = SimpleHttpOperator(
    task_id='webhook_trigger',
    http_conn_id='webhook_connection',
    endpoint='trigger',
    method='POST',
    data='{"dag_id": "target_dag"}',
    dag=dag,
)
```

### 3. ìŠ¤ì¼€ì¤„ ìµœì í™”

```python
# Cron í‘œí˜„ì‹ í™œìš©
from airflow.timetables.trigger import CronTriggerTimetable

# ë³µì¡í•œ ìŠ¤ì¼€ì¤„ë§
custom_schedule = CronTriggerTimetable(
    cron='0 2 * * 1-5',  # í‰ì¼ ì˜¤ì „ 2ì‹œ
    timezone='Asia/Seoul'
)

dag = DAG(
    'business_hours_dag',
    timetable=custom_schedule,
    catchup=False,
)

# ë°ì´í„° ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
def get_next_run_time(**context):
    """ë°ì´í„° ìƒíƒœì— ë”°ë¼ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê²°ì •"""
    last_processed = get_last_processed_time()
    data_available = check_data_availability()
    
    if data_available:
        return datetime.now()
    else:
        return datetime.now() + timedelta(hours=1)
```

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„ ì „ëµ

### 1. ê³ ê¸‰ ì¬ì‹œë„ ë¡œì§

```python
# ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
import random

class SmartRetryOperator(BaseOperator):
    """ì§€ëŠ¥í˜• ì¬ì‹œë„ ì—°ì‚°ì"""
    
    @apply_defaults
    def __init__(
        self,
        max_retries=3,
        base_delay=60,
        max_delay=300,
        exponential_base=2,
        jitter=True,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
    
    def execute(self, context):
        """ì‹¤í–‰ ë¡œì§"""
        try:
            return self._execute_logic(context)
        except Exception as e:
            if self._should_retry(e, context):
                self._schedule_retry(context)
            else:
                raise e
    
    def _should_retry(self, exception, context):
        """ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨"""
        # íŠ¹ì • ì˜ˆì™¸ë§Œ ì¬ì‹œë„
        retryable_exceptions = (ConnectionError, TimeoutError)
        return isinstance(exception, retryable_exceptions)
    
    def _schedule_retry(self, context):
        """ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§"""
        retry_count = context['task_instance'].try_number - 1
        delay = min(
            self.base_delay * (self.exponential_base ** retry_count),
            self.max_delay
        )
        
        if self.jitter:
            delay += random.uniform(0, delay * 0.1)
        
        # ì¬ì‹œë„ ì˜ˆì•½
        from airflow.models import TaskInstance
        ti = context['task_instance']
        ti.state = 'up_for_retry'
        ti.next_method = 'retry'
        ti.next_kwargs = {'delay': delay}
```

### 2. ì—ëŸ¬ ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
# ìŠ¬ë™ ì•Œë¦¼
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

def send_slack_alert(context):
    """ìŠ¬ë™ìœ¼ë¡œ ì—ëŸ¬ ì•Œë¦¼"""
    task_instance = context['task_instance']
    
    message = f"""
    ğŸš¨ Airflow Task ì‹¤íŒ¨ ì•Œë¦¼
    
    DAG: {task_instance.dag_id}
    Task: {task_instance.task_id}
    ì‹¤í–‰ ì‹œê°„: {task_instance.start_date}
    ì—ëŸ¬: {context.get('exception', 'Unknown error')}
    ë¡œê·¸: {task_instance.log_url}
    """
    
    return SlackWebhookOperator(
        task_id='slack_alert',
        http_conn_id='slack_webhook',
        message=message,
    )

# ì´ë©”ì¼ ì•Œë¦¼
from airflow.operators.email_operator import EmailOperator

email_alert = EmailOperator(
    task_id='email_alert',
    to=['data-team@company.com'],
    subject='Airflow Task ì‹¤íŒ¨ - {{ task_instance.dag_id }}',
    html_content="""
    <h2>Task ì‹¤íŒ¨ ì•Œë¦¼</h2>
    <p><strong>DAG:</strong> {task_instance.dag_id}</p>
    <p><strong>Task:</strong> {task_instance.task_id}</p>
    <p><strong>ì‹¤í–‰ ì‹œê°„:</strong> {task_instance.start_date}</p>
    <p><strong>ì—ëŸ¬:</strong> {context.get('exception', 'Unknown error')}</p>
    """,
    trigger_rule='one_failed',
)
```

### 3. Circuit Breaker íŒ¨í„´

```python
class CircuitBreakerOperator(BaseOperator):
    """ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´ êµ¬í˜„"""
    
    def __init__(
        self,
        failure_threshold=5,
        recovery_timeout=300,
        expected_exception=Exception,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
    
    def execute(self, context):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ ë¡œì§"""
        circuit_state = self._get_circuit_state()
        
        if circuit_state == 'OPEN':
            if self._should_attempt_reset():
                circuit_state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = self._execute_task(context)
            self._record_success()
            return result
        except self.expected_exception as e:
            self._record_failure()
            raise e
    
    def _get_circuit_state(self):
        """ì„œí‚· ìƒíƒœ í™•ì¸"""
        # ë©”íƒ€ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒíƒœ ì¡°íšŒ
        # êµ¬í˜„ ìƒëµ
        pass
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ {#ëª¨ë‹ˆí„°ë§ê³¼-ì•Œë¦¼-ì‹œìŠ¤í…œ}

### 1. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
from airflow.configuration import conf
from airflow.stats import Stats

def collect_custom_metrics(**context):
    """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    task_instance = context['task_instance']
    
    # ì‹¤í–‰ ì‹œê°„ ë©”íŠ¸ë¦­
    execution_time = (task_instance.end_date - task_instance.start_date).total_seconds()
    Stats.gauge('task_execution_time', execution_time, tags={
        'dag_id': task_instance.dag_id,
        'task_id': task_instance.task_id
    })
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­
    import psutil
    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    Stats.gauge('task_memory_usage', memory_usage, tags={
        'dag_id': task_instance.dag_id,
        'task_id': task_instance.task_id
    })
    
    # ë°ì´í„° ì²˜ë¦¬ëŸ‰ ë©”íŠ¸ë¦­
    processed_records = context.get('processed_records', 0)
    Stats.gauge('processed_records', processed_records, tags={
        'dag_id': task_instance.dag_id,
        'task_id': task_instance.task_id
    })

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ Task
metrics_task = PythonOperator(
    task_id='collect_metrics',
    python_callable=collect_custom_metrics,
    trigger_rule='all_done',  # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ì‹¤í–‰
)
```

### 2. ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

```python
# Grafana ëŒ€ì‹œë³´ë“œìš© ì¿¼ë¦¬
def get_dag_performance_metrics():
    """DAG ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    query = """
    SELECT 
        dag_id,
        COUNT(*) as total_runs,
        AVG(EXTRACT(EPOCH FROM (end_date - start_date))) as avg_duration,
        COUNT(CASE WHEN state = 'success' THEN 1 END) as success_count,
        COUNT(CASE WHEN state = 'failed' THEN 1 END) as failed_count
    FROM dag_run 
    WHERE start_date >= NOW() - INTERVAL '7 days'
    GROUP BY dag_id
    ORDER BY avg_duration DESC
    """
    return query

# Airflow UI ì»¤ìŠ¤í…€ ë·°
from airflow.plugins_manager import AirflowPlugin
from flask import Blueprint, render_template

class CustomViewPlugin(AirflowPlugin):
    name = "custom_view_plugin"
    
    def __init__(self):
        self.appbuilder_views = [
            {
                "name": "Custom Dashboard",
                "category": "Custom",
                "view": CustomDashboardView(),
            }
        ]

class CustomDashboardView:
    @expose("/custom-dashboard")
    def custom_dashboard(self):
        """ì»¤ìŠ¤í…€ ëŒ€ì‹œë³´ë“œ"""
        # ë©”íŠ¸ë¦­ ë°ì´í„° ì¡°íšŒ
        metrics = get_dag_performance_metrics()
        
        return self.render_template(
            "custom_dashboard.html",
            metrics=metrics
        )
```

### 3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
# ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults

class RealTimeMonitorOperator(BaseOperator):
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì—°ì‚°ì"""
    
    @apply_defaults
    def __init__(
        self,
        monitoring_interval=60,
        alert_thresholds=None,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.monitoring_interval = monitoring_interval
        self.alert_thresholds = alert_thresholds or {}
    
    def execute(self, context):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        while True:
            try:
                # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
                system_health = self._check_system_health()
                
                # ì„ê³„ê°’ í™•ì¸
                alerts = self._check_thresholds(system_health)
                
                # ì•Œë¦¼ ë°œì†¡
                for alert in alerts:
                    self._send_alert(alert)
                
                # ëŒ€ê¸°
                time.sleep(self.monitoring_interval)
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                self.log.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(self.monitoring_interval)
    
    def _check_system_health(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        return {
            'cpu_usage': psutil.cpu_percent(),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'active_dags': self._count_active_dags(),
            'failed_tasks': self._count_failed_tasks(),
        }
    
    def _check_thresholds(self, health_data):
        """ì„ê³„ê°’ í™•ì¸"""
        alerts = []
        
        if health_data['cpu_usage'] > self.alert_thresholds.get('cpu', 80):
            alerts.append({
                'type': 'cpu_high',
                'message': f"CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {health_data['cpu_usage']}%"
            })
        
        if health_data['memory_usage'] > self.alert_thresholds.get('memory', 85):
            alerts.append({
                'type': 'memory_high',
                'message': f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {health_data['memory_usage']}%"
            })
        
        return alerts
```

## ğŸ›  ï¸ ì‹¤ìŠµ: ì‹¤ë¬´ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• {#ì‹¤ìŠµ-ì‹¤ë¬´ê¸‰-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-êµ¬ì¶•}

### 1. í™˜ê²½ ì„¤ì •

```bash
# Airflow ì„¤ì¹˜ ë° ì„¤ì •
pip install apache-airflow[postgres,celery,redis,slack]
pip install apache-airflow-providers-postgres
pip install apache-airflow-providers-slack
pip install apache-airflow-providers-http

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
export AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql://airflow:airflow@localhost/airflow"
export AIRFLOW__CORE__EXECUTOR="CeleryExecutor"
export AIRFLOW__CELERY__BROKER_URL="redis://localhost:6379/0"
export AIRFLOW__CELERY__RESULT_BACKEND="redis://localhost:6379/0"

# Airflow ì´ˆê¸°í™”
airflow db init
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

### 2. ì‹¤ë¬´ê¸‰ ETL íŒŒì´í”„ë¼ì¸

```python
# dags/advanced_etl_pipeline.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

# ì„¤ì •ê°’ ë¡œë“œ
BATCH_SIZE = int(Variable.get("batch_size", default_var=1000))
MAX_RETRIES = int(Variable.get("max_retries", default_var=3))

default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': MAX_RETRIES,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'catchup': False,
    'max_active_runs': 1,
}

dag = DAG(
    'advanced_etl_pipeline',
    default_args=default_args,
    description='ê³ ê¸‰ ETL íŒŒì´í”„ë¼ì¸ - ë°ì´í„° ì¶”ì¶œ, ë³€í™˜, ë¡œë“œ',
    schedule_interval='0 2 * * *',
    max_active_runs=1,
    tags=['etl', 'data-engineering', 'production'],
    doc_md=__doc__,
)

# ë°ì´í„° ì¶”ì¶œ ê·¸ë£¹
with TaskGroup("data_extraction", tooltip="ë°ì´í„° ì¶”ì¶œ") as extraction_group:
    
    # ì†ŒìŠ¤ íŒŒì¼ í™•ì¸
    check_source_files = FileSensor(
        task_id='check_source_files',
        filepath=[
            '/data/input/sales_data.csv',
            '/data/input/customer_data.csv',
            '/data/input/product_data.csv'
        ],
        poke_interval=30,
        timeout=300,
        mode='reschedule',
    )
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    validate_data_quality = PythonOperator(
        task_id='validate_data_quality',
        python_callable=validate_data_quality,
        op_kwargs={
            'source_files': [
                '/data/input/sales_data.csv',
                '/data/input/customer_data.csv',
                '/data/input/product_data.csv'
            ]
        },
        pool='data_validation_pool',
    )
    
    # ë°ì´í„° ì¶”ì¶œ
    extract_data = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data_from_sources,
        op_kwargs={'batch_size': BATCH_SIZE},
        pool='data_extraction_pool',
    )

# ë°ì´í„° ë³€í™˜ ê·¸ë£¹
with TaskGroup("data_transformation", tooltip="ë°ì´í„° ë³€í™˜") as transformation_group:
    
    # ë°ì´í„° ì •ì œ
    clean_data = PythonOperator(
        task_id='clean_data',
        python_callable=clean_and_validate_data,
        op_kwargs={'batch_size': BATCH_SIZE},
        pool='data_processing_pool',
    )
    
    # ë°ì´í„° ë³€í™˜
    transform_data = PythonOperator(
        task_id='transform_data',
        python_callable=transform_business_logic,
        op_kwargs={'batch_size': BATCH_SIZE},
        pool='data_processing_pool',
    )
    
    # ë°ì´í„° ì§‘ê³„
    aggregate_data = PythonOperator(
        task_id='aggregate_data',
        python_callable=create_aggregated_metrics,
        pool='data_processing_pool',
    )

# ë°ì´í„° ë¡œë“œ ê·¸ë£¹
with TaskGroup("data_loading", tooltip="ë°ì´í„° ë¡œë“œ") as loading_group:
    
    # ìŠ¤í…Œì´ì§• í…Œì´ë¸” ë¡œë“œ
    load_to_staging = PythonOperator(
        task_id='load_to_staging',
        python_callable=load_to_staging_tables,
        pool='data_loading_pool',
    )
    
    # ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë¡œë“œ
    load_to_warehouse = PythonOperator(
        task_id='load_to_warehouse',
        python_callable=load_to_data_warehouse,
        pool='data_loading_pool',
    )
    
    # ë°ì´í„° ê²€ì¦
    validate_loaded_data = PythonOperator(
        task_id='validate_loaded_data',
        python_callable=validate_loaded_data,
        trigger_rule='all_success',
    )

# ì•Œë¦¼ ë° ëª¨ë‹ˆí„°ë§
with TaskGroup("monitoring", tooltip="ëª¨ë‹ˆí„°ë§") as monitoring_group:
    
    # ì„±ê³µ ì•Œë¦¼
    success_notification = SlackWebhookOperator(
        task_id='success_notification',
        http_conn_id='slack_webhook',
        message="""
        âœ… ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
        
        ì‹¤í–‰ ì‹œê°„: {{ ds }}
        ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜: {ti.xcom_pull(task_ids='extract_data')}
        """,
        trigger_rule='all_success',
    )
    
    # ì‹¤íŒ¨ ì•Œë¦¼
    failure_notification = SlackWebhookOperator(
        task_id='failure_notification',
        http_conn_id='slack_webhook',
        message="""
        âŒ ETL íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨
        
        ì‹¤í–‰ ì‹œê°„: {{ ds }}
        ì‹¤íŒ¨í•œ Task: {{ ti.task_id }}
        ì—ëŸ¬ ë¡œê·¸: {{ ti.log_url }}
        """,
        trigger_rule='one_failed',
    )

# ì˜ì¡´ì„± ì„¤ì •
extraction_group >> transformation_group >> loading_group
[loading_group, monitoring_group]

# DAG ë¬¸ì„œí™”
dag.doc_md = """
# ê³ ê¸‰ ETL íŒŒì´í”„ë¼ì¸

ì´ DAGëŠ” ì‹¤ë¬´ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ì•Œë¦¼ ì‹œìŠ¤í…œ

## ì‹¤í–‰ ì£¼ê¸°
ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰

## ëª¨ë‹ˆí„°ë§
- Slack ì•Œë¦¼
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì—ëŸ¬ ì¶”ì 
"""
```

### 3. í•µì‹¬ í•¨ìˆ˜ êµ¬í˜„

```python
# plugins/etl_functions.py
import pandas as pd
import numpy as np
from datetime import datetime
from airflow.hooks.postgres_hook import PostgresHook
from airflow.models import Variable
import logging

logger = logging.getLogger(__name__)

def validate_data_quality(source_files, **context):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
    
    validation_results = {}
    
    for file_path in source_files:
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(file_path)
            
            # ê¸°ë³¸ ê²€ì¦
            validation_result = {
                'file_path': file_path,
                'row_count': len(df),
                'column_count': len(df.columns),
                'null_count': df.isnull().sum().sum(),
                'duplicate_count': df.duplicated().sum(),
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024),
            }
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            validation_result['data_types'] = df.dtypes.to_dict()
            
            # ì´ìƒì¹˜ ê²€ì¦ (IQR ë°©ë²•)
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            outliers = {}
            for col in numeric_columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers[col] = len(df[(df[col] < lower_bound) | (df[col] > upper_bound)])
            
            validation_result['outliers'] = outliers
            
            validation_results[file_path] = validation_result
            
            logger.info(f"ê²€ì¦ ì™„ë£Œ: {file_path} - {len(df)}í–‰")
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {file_path} - {str(e)}")
            raise e
    
    # ê²€ì¦ ê²°ê³¼ë¥¼ XComì— ì €ì¥
    context['task_instance'].xcom_push(key='validation_results', value=validation_results)
    
    return validation_results

def extract_data_from_sources(batch_size, **context):
    """ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    logger.info("ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    
    # ê²€ì¦ ê²°ê³¼ ë¡œë“œ
    validation_results = context['task_instance'].xcom_pull(
        task_ids='validate_data_quality',
        key='validation_results'
    )
    
    extracted_data = {}
    total_records = 0
    
    for file_path, validation in validation_results.items():
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = preprocess_data(df)
            
            extracted_data[file_path] = df
            total_records += len(df)
            
            logger.info(f"ì¶”ì¶œ ì™„ë£Œ: {file_path} - {len(df)}í–‰")
            
        except Exception as e:
            logger.error(f"ì¶”ì¶œ ì‹¤íŒ¨: {file_path} - {str(e)}")
            raise e
    
    # ì¶”ì¶œ ê²°ê³¼ë¥¼ XComì— ì €ì¥
    context['task_instance'].xcom_push(key='extracted_data', value=extracted_data)
    context['task_instance'].xcom_push(key='total_records', value=total_records)
    
    logger.info(f"ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ - ì´ {total_records}ê°œ ë ˆì½”ë“œ")
    
    return total_records

def clean_and_validate_data(batch_size, **context):
    """ë°ì´í„° ì •ì œ ë° ê²€ì¦"""
    logger.info("ë°ì´í„° ì •ì œ ì‹œì‘")
    
    # ì¶”ì¶œëœ ë°ì´í„° ë¡œë“œ
    extracted_data = context['task_instance'].xcom_pull(
        task_ids='extract_data',
        key='extracted_data'
    )
    
    cleaned_data = {}
    
    for file_path, df in extracted_data.items():
        try:
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            df_cleaned = handle_missing_values(df)
            
            # ì¤‘ë³µ ì œê±°
            df_cleaned = remove_duplicates(df_cleaned)
            
            # ì´ìƒì¹˜ ì²˜ë¦¬
            df_cleaned = handle_outliers(df_cleaned)
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            df_cleaned = convert_data_types(df_cleaned)
            
            cleaned_data[file_path] = df_cleaned
            
            logger.info(f"ì •ì œ ì™„ë£Œ: {file_path} - {len(df_cleaned)}í–‰")
            
        except Exception as e:
            logger.error(f"ì •ì œ ì‹¤íŒ¨: {file_path} - {str(e)}")
            raise e
    
    # ì •ì œ ê²°ê³¼ë¥¼ XComì— ì €ì¥
    context['task_instance'].xcom_push(key='cleaned_data', value=cleaned_data)
    
    return cleaned_data

def transform_business_logic(batch_size, **context):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©í•œ ë°ì´í„° ë³€í™˜"""
    logger.info("ë°ì´í„° ë³€í™˜ ì‹œì‘")
    
    # ì •ì œëœ ë°ì´í„° ë¡œë“œ
    cleaned_data = context['task_instance'].xcom_pull(
        task_ids='clean_data',
        key='cleaned_data'
    )
    
    transformed_data = {}
    
    for file_path, df in cleaned_data.items():
        try:
            # íŒŒì¼ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©
            if 'sales_data' in file_path:
                df_transformed = transform_sales_data(df)
            elif 'customer_data' in file_path:
                df_transformed = transform_customer_data(df)
            elif 'product_data' in file_path:
                df_transformed = transform_product_data(df)
            else:
                df_transformed = df
            
            transformed_data[file_path] = df_transformed
            
            logger.info(f"ë³€í™˜ ì™„ë£Œ: {file_path} - {len(df_transformed)}í–‰")
            
        except Exception as e:
            logger.error(f"ë³€í™˜ ì‹¤íŒ¨: {file_path} - {str(e)}")
            raise e
    
    # ë³€í™˜ ê²°ê³¼ë¥¼ XComì— ì €ì¥
    context['task_instance'].xcom_push(key='transformed_data', value=transformed_data)
    
    return transformed_data

def load_to_data_warehouse(**context):
    """ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ë¡œë“œ"""
    logger.info("ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë¡œë“œ ì‹œì‘")
    
    # ë³€í™˜ëœ ë°ì´í„° ë¡œë“œ
    transformed_data = context['task_instance'].xcom_pull(
        task_ids='transform_data',
        key='transformed_data'
    )
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    postgres_hook = PostgresHook(postgres_conn_id='data_warehouse_conn')
    
    for file_path, df in transformed_data.items():
        try:
            # í…Œì´ë¸”ëª… ê²°ì •
            table_name = determine_table_name(file_path)
            
            # ë°ì´í„° ë¡œë“œ
            postgres_hook.insert_rows(
                table=table_name,
                rows=df.values.tolist(),
                target_fields=df.columns.tolist(),
                replace=True
            )
            
            logger.info(f"ë¡œë“œ ì™„ë£Œ: {table_name} - {len(df)}í–‰")
            
        except Exception as e:
            logger.error(f"ë¡œë“œ ì‹¤íŒ¨: {file_path} - {str(e)}")
            raise e
    
    logger.info("ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë¡œë“œ ì™„ë£Œ")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    df.columns = df.columns.str.lower().str.replace(' ', '_')
    
    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    date_columns = df.select_dtypes(include=['object']).columns
    for col in date_columns:
        if 'date' in col.lower() or 'time' in col.lower():
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    return df

def handle_missing_values(df):
    """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
    # ìˆ«ìí˜• ì»¬ëŸ¼: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
    categorical_columns = df.select_dtypes(include=['object']).columns
    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])
    
    return df

def remove_duplicates(df):
    """ì¤‘ë³µ ì œê±°"""
    return df.drop_duplicates()

def handle_outliers(df):
    """ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ë°©ë²•)"""
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´
        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
    
    return df

def convert_data_types(df):
    """ë°ì´í„° íƒ€ì… ë³€í™˜"""
    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    date_columns = df.select_dtypes(include=['object']).columns
    for col in date_columns:
        if 'date' in col.lower() or 'time' in col.lower():
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ ì²˜ë¦¬
    numeric_columns = df.select_dtypes(include=['object']).columns
    for col in numeric_columns:
        if df[col].dtype == 'object':
            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
            if df[col].str.replace('.', '').str.replace('-', '').str.isnumeric().all():
                df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df

def determine_table_name(file_path):
    """íŒŒì¼ ê²½ë¡œì—ì„œ í…Œì´ë¸”ëª… ê²°ì •"""
    filename = os.path.basename(file_path)
    table_name = filename.replace('.csv', '').replace('_data', '')
    return f"staging_{table_name}"

# ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë³€í™˜ í•¨ìˆ˜ë“¤
def transform_sales_data(df):
    """ë§¤ì¶œ ë°ì´í„° ë³€í™˜"""
    # ë§¤ì¶œ ê¸ˆì•¡ ê³„ì‚°
    if 'quantity' in df.columns and 'unit_price' in df.columns:
        df['total_amount'] = df['quantity'] * df['unit_price']
    
    # í• ì¸ìœ¨ ì ìš©
    if 'discount_rate' in df.columns:
        df['final_amount'] = df['total_amount'] * (1 - df['discount_rate'] / 100)
    
    return df

def transform_customer_data(df):
    """ê³ ê° ë°ì´í„° ë³€í™˜"""
    # ê³ ê° ë“±ê¸‰ ê³„ì‚°
    if 'total_purchase' in df.columns:
        df['customer_grade'] = pd.cut(
            df['total_purchase'],
            bins=[0, 1000, 5000, 10000, float('inf')],
            labels=['Bronze', 'Silver', 'Gold', 'Platinum']
        )
    
    return df

def transform_product_data(df):
    """ìƒí’ˆ ë°ì´í„° ë³€í™˜"""
    # ì¹´í…Œê³ ë¦¬ ì •ê·œí™”
    if 'category' in df.columns:
        df['category'] = df['category'].str.upper().str.strip()
    
    return df
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Airflow ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´**
   - í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„
   - ë©”íƒ€ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
   - Executor ì¢…ë¥˜ì™€ íŠ¹ì§•

2. **DAG ìµœì í™” ì „ëµ**
   - íš¨ìœ¨ì ì¸ Task ì •ì˜
   - ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ìµœì í™”
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

3. **ê³ ê¸‰ ìŠ¤ì¼€ì¤„ë§ê³¼ íŠ¸ë¦¬ê±°**
   - ë™ì  ìŠ¤ì¼€ì¤„ë§
   - ì™¸ë¶€ íŠ¸ë¦¬ê±° í™œìš©
   - ìŠ¤ì¼€ì¤„ ìµœì í™”

4. **ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„ ì „ëµ**
   - ê³ ê¸‰ ì¬ì‹œë„ ë¡œì§
   - ì—ëŸ¬ ì•Œë¦¼ ì‹œìŠ¤í…œ
   - Circuit Breaker íŒ¨í„´

5. **ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ**
   - ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

6. **ì‹¤ë¬´ê¸‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
   - ì™„ì „í•œ ETL íŒŒì´í”„ë¼ì¸
   - ë°ì´í„° í’ˆì§ˆ ê²€ì¦
   - ì—ëŸ¬ ì²˜ë¦¬ ë° ì•Œë¦¼

### í•µì‹¬ ê°œë… ì •ë¦¬

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| **DAG ìµœì í™”** | ì„±ëŠ¥ê³¼ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± í–¥ìƒ | â­â­â­â­â­ |
| **ì—ëŸ¬ ì²˜ë¦¬** | ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ ìš´ì˜ | â­â­â­â­â­ |
| **ëª¨ë‹ˆí„°ë§** | ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì  | â­â­â­â­â­ |
| **ìŠ¤ì¼€ì¤„ë§** | ìœ ì—°í•œ ì‹¤í–‰ ì œì–´ | â­â­â­â­ |

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

1. **ì„±ëŠ¥ ìµœì í™”**: Pool, Queue, ë©”ëª¨ë¦¬ ê´€ë¦¬
2. **ì•ˆì •ì„±**: ì—ëŸ¬ ì²˜ë¦¬, ì¬ì‹œë„, Circuit Breaker
3. **ëª¨ë‹ˆí„°ë§**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ì•Œë¦¼, ëŒ€ì‹œë³´ë“œ
4. **í™•ì¥ì„±**: ë¶„ì‚° í™˜ê²½, ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

---

*ì´ ê°€ì´ë“œë¥¼ í†µí•´ Apache Airflowì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë§ˆìŠ¤í„°í•˜ê³ , ì‹¤ë¬´ì—ì„œ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!* ğŸš€

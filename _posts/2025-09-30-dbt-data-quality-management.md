---
layout: post
title: "dbtë¥¼ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ì „ ê°€ì´ë“œ - í˜„ëŒ€ì  ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬"
description: "dbtì™€ ì£¼ìš” ë°ì´í„° í”Œë«í¼ì„ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ì˜ ëª¨ë“  ê²ƒ. Snowflake, BigQuery, Redshiftì™€ í•¨ê»˜í•˜ëŠ” ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
excerpt: "dbtì™€ ì£¼ìš” ë°ì´í„° í”Œë«í¼ì„ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ì˜ ëª¨ë“  ê²ƒ. ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ì™„ì „í•œ ê°€ì´ë“œ"
category: data-quality
tags: [dbt, ë°ì´í„°í’ˆì§ˆ, DataQuality, Snowflake, BigQuery, Redshift, Databricks, dbtCloud, ë°ì´í„°íŒŒì´í”„ë¼ì¸]
series: modern-data-stack
series_order: 1
date: 2025-09-30
author: Data Droid
lang: ko
reading_time: 60ë¶„
difficulty: ì¤‘ê¸‰
---

# dbtë¥¼ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ì „ ê°€ì´ë“œ - í˜„ëŒ€ì  ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬

> **"ë°ì´í„°ëŠ” ìƒˆë¡œìš´ ì„ìœ ë‹¤"**ë¼ëŠ” ë§ì´ ìˆì§€ë§Œ, ì •ì œë˜ì§€ ì•Šì€ ì„ìœ ëŠ” ì•„ë¬´ ì“¸ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤. dbtëŠ” ë°”ë¡œ ê·¸ **ë°ì´í„° ì •ì œ ê³µì¥**ì˜ í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.

í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒì—ì„œ dbtëŠ” ë‹¨ìˆœí•œ ë³€í™˜ ë„êµ¬ë¥¼ ë„˜ì–´ **ë°ì´í„° í’ˆì§ˆì˜ ìˆ˜í˜¸ì** ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” dbtì™€ ì£¼ìš” ë°ì´í„° í”Œë«í¼ë“¤ì„ í™œìš©í•œ ì™„ì „í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ¯ ëª©ì°¨

- [dbtì™€ í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒ](#dbtì™€-í˜„ëŒ€ì -ë°ì´í„°-ìŠ¤íƒ)
- [dbt ìƒíƒœê³„ì™€ ì£¼ìš” í”Œë«í¼](#dbt-ìƒíƒœê³„ì™€-ì£¼ìš”-í”Œë«í¼)
- [ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬](#ë°ì´í„°-í’ˆì§ˆ-í…ŒìŠ¤íŠ¸-í”„ë ˆì„ì›Œí¬)
- [ì‹¤ë¬´ ë°ì´í„° í’ˆì§ˆ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•](#ì‹¤ë¬´-ë°ì´í„°-í’ˆì§ˆ-íŒŒì´í”„ë¼ì¸-êµ¬ì¶•)
- [ê³ ê¸‰ ë°ì´í„° í’ˆì§ˆ ì „ëµ](#ê³ ê¸‰-ë°ì´í„°-í’ˆì§ˆ-ì „ëµ)
- [ì‹¤ìŠµ: ì™„ì „í•œ dbt ë°ì´í„° í’ˆì§ˆ ì‹œìŠ¤í…œ](#ì‹¤ìŠµ-ì™„ì „í•œ-dbt-ë°ì´í„°-í’ˆì§ˆ-ì‹œìŠ¤í…œ)

---

## ğŸ—ï¸ dbtì™€ í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒ {#dbtì™€-í˜„ëŒ€ì -ë°ì´í„°-ìŠ¤íƒ}

### dbtë€ ë¬´ì—‡ì¸ê°€?

**dbt (Data Build Tool)**ëŠ” SQL ê¸°ë°˜ì˜ ë°ì´í„° ë³€í™˜ ë„êµ¬ë¡œ, í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.

| **íŠ¹ì§•** | **ì„¤ëª…** | **ì¥ì ** |
|-----------|----------|----------|
| **SQL ì¤‘ì‹¬** | Python/R ëŒ€ì‹  SQLë¡œ ë°ì´í„° ë³€í™˜ | ë¶„ì„ê°€ë„ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥ |
| **ë²„ì „ ê´€ë¦¬** | Gitê³¼ ì—°ë™ëœ ì½”ë“œ ê´€ë¦¬ | í˜‘ì—… ë° ë³€ê²½ ì¶”ì  ìš©ì´ |
| **í…ŒìŠ¤íŠ¸ ìë™í™”** | ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë‚´ì¥ | í’ˆì§ˆ ë³´ì¥ ë° ì‹ ë¢°ì„± í–¥ìƒ |
| **ë¬¸ì„œí™”** | ìë™ ìƒì„±ë˜ëŠ” ë°ì´í„° ë¬¸ì„œ | íŒ€ ê°„ ì†Œí†µ ê°œì„  |
| **ëª¨ë“ˆí™”** | ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤í¬ë¡œ | ê°œë°œ íš¨ìœ¨ì„± ì¦ëŒ€ |

### í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒì—ì„œì˜ dbt ì—­í• 

```
Raw Data Sources â†’ Data Warehouse â†’ dbt Layer â†’ Analytics Layer
                                      â†“
                              Data Quality Tests
                                      â†“
                              Documentation
```

### ë°ì´í„° í’ˆì§ˆì˜ ì¤‘ìš”ì„±

**ë‚˜ìœ ë°ì´í„°ì˜ ë¹„ìš©**ì€ ìƒìƒ ì´ìƒì…ë‹ˆë‹¤:

- **ì˜ì‚¬ê²°ì • ì˜¤ë¥˜**: ì˜ëª»ëœ ë°ì´í„°ë¡œ ì¸í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì†ì‹¤
- **ì‹ ë¢°ë„ í•˜ë½**: ë°ì´í„°ì— ëŒ€í•œ íŒ€ì˜ ì‹ ë¢° ìƒì‹¤
- **ê°œë°œ ì§€ì—°**: ë°ì´í„° ë¬¸ì œ í•´ê²°ì— ì†Œìš”ë˜ëŠ” ì‹œê°„
- **ê·œì • ì¤€ìˆ˜ ìœ„í—˜**: ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì´ìŠˆ

---

## ğŸŒ dbt ìƒíƒœê³„ì™€ ì£¼ìš” í”Œë«í¼ {#dbt-ìƒíƒœê³„ì™€-ì£¼ìš”-í”Œë«í¼}

### ì£¼ìš” ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ í”Œë«í¼

#### 1. **Snowflake**
- **íŠ¹ì§•**: í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ, ìë™ ìŠ¤ì¼€ì¼ë§
- **dbt í†µí•©**: ì™„ë²½í•œ ì§€ì›, ìµœì í™”ëœ ì»¤ë„¥í„°
- **ì¥ì **: ì„±ëŠ¥ ìš°ìˆ˜, ì‚¬ìš© í¸ì˜ì„±

#### 2. **Google BigQuery**
- **íŠ¹ì§•**: ì„œë²„ë¦¬ìŠ¤, í˜íƒ€ë°”ì´íŠ¸ ìŠ¤ì¼€ì¼
- **dbt í†µí•©**: ë„¤ì´í‹°ë¸Œ ì§€ì›, ë¹ ë¥¸ ì¿¼ë¦¬
- **ì¥ì **: ë¹„ìš© íš¨ìœ¨ì„±, ë¨¸ì‹ ëŸ¬ë‹ í†µí•©

#### 3. **Amazon Redshift**
- **íŠ¹ì§•**: í´ëŸ¬ìŠ¤í„° ê¸°ë°˜, ë†’ì€ ì„±ëŠ¥
- **dbt í†µí•©**: ì•ˆì •ì ì¸ ì§€ì›, ë‹¤ì–‘í•œ ì˜µì…˜
- **ì¥ì **: AWS ìƒíƒœê³„ í†µí•©, ì„±ìˆ™í•œ í”Œë«í¼

#### 4. **Databricks**
- **íŠ¹ì§•**: ë ˆì´í¬í•˜ìš°ìŠ¤, ë¨¸ì‹ ëŸ¬ë‹ í†µí•©
- **dbt í†µí•©**: Unity Catalog ì§€ì›
- **ì¥ì **: ë°ì´í„° ë ˆì´í¬ + ì›¨ì–´í•˜ìš°ìŠ¤ í†µí•©

### dbt ì‹¤í–‰ í™˜ê²½ ë¹„êµ

| **í™˜ê²½** | **íŠ¹ì§•** | **ì¥ì ** | **ë‹¨ì ** | **ì í•©í•œ ê²½ìš°** |
|-----------|----------|----------|----------|-----------------|
| **dbt Cloud** | í´ë¼ìš°ë“œ ê¸°ë°˜ SaaS | ì„¤ì • ê°„í¸, UI ì œê³µ | ë¹„ìš© ë°œìƒ | ì¤‘ì†Œê·œëª¨ íŒ€, ë¹ ë¥¸ ì‹œì‘ |
| **dbt Core** | ì˜¤í”ˆì†ŒìŠ¤, ë¡œì»¬ ì‹¤í–‰ | ë¬´ë£Œ, ì»¤ìŠ¤í„°ë§ˆì´ì§• | ì„¤ì • ë³µì¡ | ëŒ€ê·œëª¨ íŒ€, ê³ ë„í™” í•„ìš” |

---

## ğŸ§ª ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ {#ë°ì´í„°-í’ˆì§ˆ-í…ŒìŠ¤íŠ¸-í”„ë ˆì„ì›Œí¬}

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (Generic Tests)

#### 1. **not_null**: NULL ê°’ ê²€ì¦
```yaml
# models/schema.yml
models:
  - name: users
    columns:
      - name: user_id
        tests:
          - not_null
      - name: email
        tests:
          - not_null
```

#### 2. **unique**: ì¤‘ë³µ ê°’ ê²€ì¦
```yaml
models:
  - name: users
    columns:
      - name: user_id
        tests:
          - unique
          - not_null
```

#### 3. **accepted_values**: í—ˆìš© ê°’ ê²€ì¦
```yaml
models:
  - name: orders
    columns:
      - name: status
        tests:
          - accepted_values:
              values: ['pending', 'completed', 'cancelled']
```

#### 4. **relationships**: ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦
```yaml
models:
  - name: orders
    columns:
      - name: user_id
        tests:
          - relationships:
              to: ref('users')
              field: user_id
```

### ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ (Custom Tests)

#### 1. **ë‹¨ì¼ í…ŒìŠ¤íŠ¸ íŒŒì¼**
```sql
-- tests/assert_positive_amount.sql
select *
from ref('orders')
where amount <= 0
```

#### 2. **ë§¤í¬ë¡œ ê¸°ë°˜ í…ŒìŠ¤íŠ¸**
```sql
-- macros/test_positive_values.sql
test positive_values(model, column_name)
  select *
  from model
  where column_name <= 0
endtest
```

#### 3. **ê³ ê¸‰ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í…ŒìŠ¤íŠ¸**
```sql
-- tests/assert_revenue_consistency.sql
with daily_revenue as (
  select 
    date_trunc('day', created_at) as date,
    sum(amount) as total_revenue
  from ref('orders')
  where status = 'completed'
  group by 1
),
revenue_changes as (
  select 
    *,
    lag(total_revenue) over (order by date) as prev_revenue,
    abs(total_revenue - lag(total_revenue) over (order by date)) / 
    lag(total_revenue) over (order by date) as change_rate
  from daily_revenue
)
select *
from revenue_changes
where change_rate > 0.5  -- 50% ì´ìƒ ê¸‰ê²©í•œ ë³€í™”
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§

#### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
dbt test

# íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
dbt test --models users

# í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
dbt test --store-failures
```

---

## ğŸ­ ì‹¤ë¬´ ë°ì´í„° í’ˆì§ˆ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• {#ì‹¤ë¬´-ë°ì´í„°-í’ˆì§ˆ-íŒŒì´í”„ë¼ì¸-êµ¬ì¶•}

### 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„

```
dbt_project/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ profiles.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_users.sql
â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ int_user_metrics.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_users.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ fact_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚   â””â”€â”€ marketing/
â”‚   â”‚       â”œâ”€â”€ user_segments.sql
â”‚   â”‚       â””â”€â”€ schema.yml
â”‚   â””â”€â”€ data_quality/
â”‚       â”œâ”€â”€ dq_summary.sql
â”‚       â””â”€â”€ dq_alerts.sql
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ assert_positive_amounts.sql
â”‚   â””â”€â”€ assert_business_rules.sql
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ test_positive_values.sql
â”‚   â””â”€â”€ generate_schema_name.sql
â””â”€â”€ snapshots/
    â””â”€â”€ users_snapshot.sql
```

### 2. ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì •ì˜

#### í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ë¥˜

| **ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬** | **ì¸¡ì • í•­ëª©** | **ëª©í‘œ** | **ì„ê³„ê°’** |
|-------------------|---------------|----------|------------|
| **ì™„ì „ì„±** | NULL ê°’ ë¹„ìœ¨ | < 5% | 5% |
| **ì •í™•ì„±** | ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ìœ¨ | < 1% | 1% |
| **ì¼ê´€ì„±** | ì°¸ì¡° ë¬´ê²°ì„± ìœ„ë°˜ | 0% | 0% |
| **ì ì‹œì„±** | ë°ì´í„° ì§€ì—° ì‹œê°„ | < 1ì‹œê°„ | 1ì‹œê°„ |
| **ìœ íš¨ì„±** | í˜•ì‹ ì˜¤ë¥˜ ë¹„ìœ¨ | < 2% | 2% |

### 3. ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

#### ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
```sql
-- models/data_quality/dq_alerts.sql
with quality_violations as (
  select 
    'high_null_rate' as alert_type,
    'stg_users' as table_name,
    'user_id' as column_name,
    count(*) as violation_count,
    current_timestamp as detected_at
  from ref('stg_users')
  where user_id is null
  having count(*) > 100
  
  union all
  
  select 
    'invalid_email_format' as alert_type,
    'stg_users' as table_name,
    'email' as column_name,
    count(*) as violation_count,
    current_timestamp as detected_at
  from ref('stg_users')
  where email not like '%@%'
  having count(*) > 50
)

select *
from quality_violations
where violation_count > 0
```

---

## ğŸš€ ê³ ê¸‰ ë°ì´í„° í’ˆì§ˆ ì „ëµ {#ê³ ê¸‰-ë°ì´í„°-í’ˆì§ˆ-ì „ëµ}

### 1. ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€

#### í†µê³„ì  ë“œë¦¬í”„íŠ¸ ê°ì§€
```sql
-- models/data_quality/drift_detection.sql
with current_stats as (
  select 
    avg(amount) as mean_amount,
    stddev(amount) as std_amount,
    count(*) as record_count,
    current_date as measurement_date
  from ref('fact_orders')
  where created_at >= current_date - 7
),
historical_stats as (
  select 
    avg(amount) as mean_amount,
    stddev(amount) as std_amount,
    count(*) as record_count,
    current_date - 7 as measurement_date
  from ref('fact_orders')
  where created_at >= current_date - 14
    and created_at < current_date - 7
),
drift_analysis as (
  select 
    c.mean_amount as current_mean,
    h.mean_amount as historical_mean,
    abs(c.mean_amount - h.mean_amount) / h.mean_amount as mean_drift_ratio,
    c.std_amount as current_std,
    h.std_amount as historical_std,
    abs(c.std_amount - h.std_amount) / h.std_amount as std_drift_ratio,
    c.record_count as current_count,
    h.record_count as historical_count,
    abs(c.record_count - h.record_count) / h.record_count as count_drift_ratio
  from current_stats c
  cross join historical_stats h
)

select 
  *,
  case 
    when mean_drift_ratio > 0.2 or std_drift_ratio > 0.3 or count_drift_ratio > 0.5 
    then 'drift_detected'
    else 'normal'
  end as drift_status
from drift_analysis
```

### 2. ìë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬

#### CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©
```yaml
# .github/workflows/dbt-quality-check.yml
name: dbt Data Quality Check

on:
  pull_request:
    paths:
      - 'models/**'
      - 'tests/**'

jobs:
  quality-check:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    
    - name: Install dbt
      run: pip install dbt-snowflake
    
    - name: Run dbt tests
      run: |
        dbt deps
        dbt test --store-failures
```

### 3. íŒ€ í˜‘ì—…ê³¼ ê±°ë²„ë„ŒìŠ¤

#### ë°ì´í„° ê³„ì•½ (Data Contracts)
```yaml
# models/schema.yml
models:
  - name: users
    description: "User dimension table with complete user information"
    columns:
      - name: user_id
        description: "Unique identifier for each user"
        tests:
          - not_null
          - unique
        data_type: integer
      
      - name: email
        description: "User's email address"
        tests:
          - not_null
          - not_empty_string
        data_type: string
      
      - name: created_at
        description: "Timestamp when user was created"
        tests:
          - not_null
        data_type: timestamp
```

---

## ğŸ› ï¸ ì‹¤ìŠµ: ì™„ì „í•œ dbt ë°ì´í„° í’ˆì§ˆ ì‹œìŠ¤í…œ {#ì‹¤ìŠµ-ì™„ì „í•œ-dbt-ë°ì´í„°-í’ˆì§ˆ-ì‹œìŠ¤í…œ}

### 1. Snowflake + dbt Cloud ì„¤ì •

#### í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •
```yaml
# dbt_project.yml
name: 'ecommerce_data_quality'
version: '1.0.0'
config-version: 2

profile: 'snowflake_ecommerce'

model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
seed-paths: ["data"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

models:
  ecommerce_data_quality:
    staging:
      materialized: view
    intermediate:
      materialized: view
    marts:
      materialized: table
    data_quality:
      materialized: table

tests:
  store_failures: true
```

### 2. ì‹¤ë¬´ ë°ì´í„° ëª¨ë¸ë§ ì˜ˆì œ

#### ìŠ¤í…Œì´ì§• ëª¨ë¸
```sql
-- models/staging/stg_users.sql
select 
  user_id,
  email,
  first_name,
  last_name,
  phone,
  created_at,
  updated_at,
  is_active
from source('raw_data', 'users')
where created_at is not null
```

#### ì¤‘ê°„ ëª¨ë¸
```sql
-- models/intermediate/int_user_orders.sql
with user_orders as (
  select 
    u.user_id,
    u.email,
    u.created_at as user_created_at,
    count(o.order_id) as total_orders,
    sum(o.amount) as total_spent,
    avg(o.amount) as avg_order_value,
    max(o.created_at) as last_order_date
  from ref('stg_users') u
  left join ref('stg_orders') o on u.user_id = o.user_id
  group by 1, 2, 3
)

select 
  *,
  case 
    when total_spent > 1000 then 'high_value'
    when total_spent > 500 then 'medium_value'
    else 'low_value'
  end as customer_segment,
  
  case 
    when last_order_date >= current_date - 30 then 'active'
    when last_order_date >= current_date - 90 then 'at_risk'
    else 'inactive'
  end as customer_status
from user_orders
```

#### ë§ˆíŠ¸ ëª¨ë¸
```sql
-- models/marts/core/dim_users.sql
select 
  user_id,
  email,
  first_name,
  last_name,
  phone,
  user_created_at,
  total_orders,
  total_spent,
  avg_order_value,
  customer_segment,
  customer_status,
  current_timestamp as dbt_updated_at
from ref('int_user_orders')
```

### 3. í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

#### ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸
```yaml
# models/staging/schema.yml
version: 2

sources:
  - name: raw_data
    description: "Raw data from source systems"
    tables:
      - name: users
        description: "Raw user data"
        columns:
          - name: user_id
            tests:
              - not_null
              - unique
          - name: email
            tests:
              - not_null
              - not_empty_string
          - name: created_at
            tests:
              - not_null

models:
  - name: stg_users
    description: "Cleaned user data from staging layer"
    columns:
      - name: user_id
        description: "Unique user identifier"
        tests:
          - not_null
          - unique
      - name: email
        description: "User email address"
        tests:
          - not_null
          - not_empty_string
      - name: is_active
        description: "User active status"
        tests:
          - not_null
          - accepted_values:
              values: [true, false]
```

#### ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í…ŒìŠ¤íŠ¸
```sql
-- tests/assert_positive_order_amounts.sql
select *
from ref('stg_orders')
where amount <= 0
```

```sql
-- tests/assert_user_order_consistency.sql
with user_order_check as (
  select 
    u.user_id,
    count(distinct o.order_id) as order_count,
    sum(o.amount) as total_amount
  from ref('stg_users') u
  left join ref('stg_orders') o on u.user_id = o.user_id
  group by 1
)

select *
from user_order_check
where total_amount < 0
   or order_count < 0
```

### 4. ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

#### í’ˆì§ˆ ë©”íŠ¸ë¦­ ì§‘ê³„
```sql
-- models/data_quality/quality_metrics_summary.sql
with test_results as (
  select 
    node_name,
    status,
    execution_time,
    rows_affected,
    created_at
  from ref('test_results')
  where created_at >= current_date - 7
),

daily_metrics as (
  select 
    date_trunc('day', created_at) as date,
    count(*) as total_tests,
    sum(case when status = 'pass' then 1 else 0 end) as passed_tests,
    sum(case when status = 'fail' then 1 else 0 end) as failed_tests,
    avg(execution_time) as avg_execution_time
  from test_results
  group by 1
)

select 
  *,
  round(passed_tests * 100.0 / total_tests, 2) as pass_rate,
  case 
    when passed_tests * 100.0 / total_tests >= 95 then 'excellent'
    when passed_tests * 100.0 / total_tests >= 90 then 'good'
    when passed_tests * 100.0 / total_tests >= 80 then 'warning'
    else 'critical'
  end as quality_status
from daily_metrics
order by date desc
```

### 5. ìë™í™”ëœ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

#### ìš´ì˜ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/quality_monitor.py
import requests
import json
from datetime import datetime, timedelta

class DataQualityMonitor:
    def __init__(self, dbt_cloud_token, account_id):
        self.token = dbt_cloud_token
        self.account_id = account_id
        self.base_url = "https://cloud.getdbt.com/api/v2"
    
    def get_latest_run(self, job_id):
        """ìµœê·¼ dbt ì‹¤í–‰ ê²°ê³¼ ì¡°íšŒ"""
        headers = {
            'Authorization': f'Token {self.token}',
            'Content-Type': 'application/json'
        }
        
        response = requests.get(
            f"{self.base_url}/accounts/{self.account_id}/runs/",
            headers=headers,
            params={'job_definition_id': job_id, 'limit': 1}
        )
        
        if response.status_code == 200:
            runs = response.json()['data']
            if runs:
                return runs[0]
        return None
    
    def check_quality_status(self, job_id):
        """ë°ì´í„° í’ˆì§ˆ ìƒíƒœ í™•ì¸"""
        run = self.get_latest_run(job_id)
        
        if not run:
            return {'status': 'error', 'message': 'No recent runs found'}
        
        run_id = run['id']
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¡°íšŒ
        response = requests.get(
            f"{self.base_url}/accounts/{self.account_id}/runs/{run_id}/",
            headers={'Authorization': f'Token {self.token}'}
        )
        
        if response.status_code == 200:
            run_details = response.json()['data']
            
            return {
                'status': run_details['status'],
                'finished_at': run_details['finished_at'],
                'job_id': job_id,
                'run_id': run_id
            }
        
        return {'status': 'error', 'message': 'Failed to fetch run details'}
    
    def send_alert(self, message, severity='info'):
        """ì•Œë¦¼ ë°œì†¡ (Slack, Email ë“±)"""
        # Slack ì›¹í›… ë˜ëŠ” ì´ë©”ì¼ ë°œì†¡ ë¡œì§
        print(f"[{severity.upper()}] {message}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    monitor = DataQualityMonitor(
        dbt_cloud_token="your_token",
        account_id="your_account_id"
    )
    
    quality_status = monitor.check_quality_status("your_job_id")
    
    if quality_status['status'] == 'error':
        monitor.send_alert(
            f"Data quality check failed: {quality_status['message']}", 
            'critical'
        )
    else:
        monitor.send_alert(
            f"Data quality check completed: {quality_status['status']}", 
            'info'
        )
```

---

## ğŸ“Š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### í•µì‹¬ í¬ì¸íŠ¸

1. **dbtëŠ” ë°ì´í„° í’ˆì§ˆì˜ í•µì‹¬ ë„êµ¬**
   - SQL ê¸°ë°˜ì˜ ì ‘ê·¼ìœ¼ë¡œ ë¶„ì„ê°€ë„ ì‰½ê²Œ ì‚¬ìš©
   - ìë™í™”ëœ í…ŒìŠ¤íŠ¸ì™€ ë¬¸ì„œí™”ë¡œ ì‹ ë¢°ì„± í™•ë³´

2. **í”Œë«í¼ë³„ ìµœì í™” ì „ëµ**
   - Snowflake, BigQuery, Redshift ë“± ê° í”Œë«í¼ì˜ íŠ¹ì„± í™œìš©
   - dbt Cloud vs dbt Core ì„ íƒ ê¸°ì¤€

3. **ì²´ê³„ì ì¸ í’ˆì§ˆ ê´€ë¦¬**
   - ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¶€í„° ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ê¹Œì§€ ë‹¨ê³„ì  ì ‘ê·¼
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

4. **íŒ€ í˜‘ì—…ê³¼ ê±°ë²„ë„ŒìŠ¤**
   - ë°ì´í„° ê³„ì•½ì„ í†µí•œ ëª…í™•í•œ í’ˆì§ˆ ê¸°ì¤€
   - CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©ìœ¼ë¡œ ìë™í™”

### ë‹¤ìŒ ë‹¨ê³„

- **dbt íŒ¨í‚¤ì§€ í™œìš©**: dbt-utils, dbt-expectations ë“± í™•ì¥ ê¸°ëŠ¥
- **ê³ ê¸‰ ëª¨ë‹ˆí„°ë§**: Grafana, DataDog ë“± ì™¸ë¶€ ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì—°ë™
- **ë¨¸ì‹ ëŸ¬ë‹ í†µí•©**: ë°ì´í„° í’ˆì§ˆ ì´ìƒ íƒì§€ ML ëª¨ë¸ êµ¬ì¶•

---

> **"ì¢‹ì€ ë°ì´í„°ëŠ” ì¢‹ì€ ì˜ì‚¬ê²°ì •ì„ ë§Œë“ ë‹¤. dbtëŠ” ê·¸ ì¢‹ì€ ë°ì´í„°ë¥¼ ë³´ì¥í•˜ëŠ” ê°€ì¥ ê°•ë ¥í•œ ë„êµ¬ë‹¤."**

ì´ì œ ì—¬ëŸ¬ë¶„ë„ dbtë¥¼ í™œìš©í•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ëŒ€ì  ë°ì´í„° ìŠ¤íƒì—ì„œ ë°ì´í„° í’ˆì§ˆì€ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ì…ë‹ˆë‹¤. dbtì™€ í•¨ê»˜ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë³´ì¥í•˜ê³ , ë” ë‚˜ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì–´ë³´ì„¸ìš”!

---
layout: post
lang: ko
title: "Part 1: Apache Flink ê¸°ì´ˆì™€ í•µì‹¬ ê°œë… - ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ ì‹œì‘"
description: "Apache Flinkì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ í•µì‹¬ ê°œë…ì¸ DataStream API, ìƒíƒœ ê´€ë¦¬, ì‹œê°„ ì²˜ë¦¬ ë“±ì„ í•™ìŠµí•˜ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤."
date: 2025-09-15
author: Data Droid
category: data-engineering
tags: [Apache-Flink, DataStream-API, ìƒíƒœê´€ë¦¬, ì‹œê°„ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë°ì²˜ë¦¬, Python, PyFlink]
series: apache-flink-complete-guide
series_order: 1
reading_time: "35ë¶„"
difficulty: "ì¤‘ê¸‰"
---

# Part 1: Apache Flink ê¸°ì´ˆì™€ í•µì‹¬ ê°œë… - ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ ì‹œì‘

> Apache Flinkì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ í•µì‹¬ ê°œë…ì¸ DataStream API, ìƒíƒœ ê´€ë¦¬, ì‹œê°„ ì²˜ë¦¬ ë“±ì„ í•™ìŠµí•˜ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [Apache Flinkë€ ë¬´ì—‡ì¸ê°€?](#apache-flinkë€-ë¬´ì—‡ì¸ê°€)
2. [Flink ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê°œë…](#flink-ì•„í‚¤í…ì²˜ì™€-í•µì‹¬-ê°œë…)
3. [DataStream API ê¸°ì´ˆ](#datastream-api-ê¸°ì´ˆ)
4. [ì‹œê°„ ì²˜ë¦¬ì™€ ì›Œí„°ë§ˆí‚¹](#ì‹œê°„-ì²˜ë¦¬ì™€-ì›Œí„°ë§ˆí‚¹)
5. [ìƒíƒœ ê´€ë¦¬ ê¸°ì´ˆ](#ìƒíƒœ-ê´€ë¦¬-ê¸°ì´ˆ)
6. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ë¡œê·¸-ë¶„ì„)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸš€ Apache Flinkë€ ë¬´ì—‡ì¸ê°€?

### Flinkì˜ íƒ„ìƒ ë°°ê²½

Apache FlinkëŠ” 2009ë…„ ë…ì¼ì˜ TU Berlinì—ì„œ ì‹œì‘ëœ **Stratosphere í”„ë¡œì íŠ¸**ì—ì„œ ì¶œë°œí–ˆìŠµë‹ˆë‹¤. 2014ë…„ Apache Software Foundationì˜ Top-level í”„ë¡œì íŠ¸ê°€ ë˜ì—ˆìœ¼ë©°, **ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**ë¥¼ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

#### **í•µì‹¬ ì„¤ê³„ ì² í•™**

1. **True Streaming**: ë§ˆì´í¬ë¡œë°°ì¹˜ê°€ ì•„ë‹Œ ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
2. **Low Latency**: ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì§€ì—°ì‹œê°„ ë‹¬ì„±
3. **High Throughput**: ë†’ì€ ì²˜ë¦¬ëŸ‰ ìœ ì§€
4. **Exactly-Once**: ì •í™•íˆ í•œ ë²ˆ ì²˜ë¦¬ ë³´ì¥
5. **Fault Tolerance**: ê°•ë ¥í•œ ì¥ì•  ë³µêµ¬ ëŠ¥ë ¥

### Sparkì™€ì˜ ê·¼ë³¸ì  ì°¨ì´ì 

```python
# Spark: ë§ˆì´í¬ë¡œë°°ì¹˜ ë°©ì‹
from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext

spark = SparkSession.builder.appName("SparkStreaming").getOrCreate()
ssc = StreamingContext(spark.sparkContext, 5)  # 5ì´ˆ ë°°ì¹˜ ê°„ê²©

# Flink: ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
# ë°°ì¹˜ ê°„ê²© ì—†ìŒ - ì´ë²¤íŠ¸ ë„ì°© ì¦‰ì‹œ ì²˜ë¦¬
```

## ğŸ—ï¸ Flink ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê°œë…

### í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flink Cluster                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  JobManager (Master)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Dispatcher    â”‚  â”‚  ResourceManagerâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   JobMaster     â”‚  â”‚  CheckpointCoordâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TaskManager (Worker)                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚     Task 1      â”‚  â”‚     Task 2      â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚     Task 3      â”‚  â”‚     Task 4      â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### **1. JobManager**
- **ì—­í• **: ì‘ì—… ìŠ¤ì¼€ì¤„ë§ê³¼ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- **êµ¬ì„±ìš”ì†Œ**:
  - Dispatcher: ì‘ì—… ì œì¶œ ì¸í„°í˜ì´ìŠ¤
  - ResourceManager: ë¦¬ì†ŒìŠ¤ í• ë‹¹ ê´€ë¦¬
  - JobMaster: ê°œë³„ ì‘ì—… ê´€ë¦¬
  - CheckpointCoordinator: ì²´í¬í¬ì¸íŠ¸ ì¡°ì •

#### **2. TaskManager**
- **ì—­í• **: ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰
- **íŠ¹ì§•**:
  - Task Slotì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬
  - ìƒíƒœ ì €ì¥ì†Œ ì œê³µ
  - ë„¤íŠ¸ì›Œí¬ í†µì‹  ë‹´ë‹¹

### Flink í”„ë¡œê·¸ë¨ êµ¬ì¡°

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

# 1. ì‹¤í–‰ í™˜ê²½ ìƒì„±
env = StreamExecutionEnvironment.get_execution_environment()

# 2. ë°ì´í„° ì†ŒìŠ¤ ìƒì„±
data_stream = env.from_collection([1, 2, 3, 4, 5])

# 3. ë³€í™˜ ì—°ì‚° ì ìš©
result_stream = data_stream.map(lambda x: x * 2)

# 4. ë°ì´í„° ì‹±í¬ ì§€ì •
result_stream.print()

# 5. í”„ë¡œê·¸ë¨ ì‹¤í–‰
env.execute("Basic Flink Program")
```

## ğŸ“Š DataStream API ê¸°ì´ˆ

### ê¸°ë³¸ ë°ì´í„° íƒ€ì…

```python
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()

# ê¸°ë³¸ ë°ì´í„° íƒ€ì…ë“¤
basic_types = [
    "Hello", "World", 123, 45.67, True
]

data_stream = env.from_collection(basic_types)

# íƒ€ì… ì •ë³´ ëª…ì‹œì  ì§€ì •
typed_stream = data_stream.map(
    lambda x: str(x).upper(),
    output_type=Types.STRING()
)
```

### í•µì‹¬ ë³€í™˜ ì—°ì‚°

#### **1. Map ì—°ì‚°**
```python
# ë‹¨ì¼ ìš”ì†Œ ë³€í™˜
def multiply_by_two(x):
    return x * 2

data_stream = env.from_collection([1, 2, 3, 4, 5])
result = data_stream.map(multiply_by_two)
```

#### **2. Filter ì—°ì‚°**
```python
# ì¡°ê±´ì— ë”°ë¥¸ í•„í„°ë§
def is_even(x):
    return x % 2 == 0

numbers = env.from_collection([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
even_numbers = numbers.filter(is_even)
```

#### **3. FlatMap ì—°ì‚°**
```python
# 1:N ë³€í™˜
def split_words(sentence):
    return sentence.split()

text_stream = env.from_collection([
    "Hello World",
    "Apache Flink",
    "Stream Processing"
])

words = text_stream.flat_map(split_words)
```

#### **4. KeyBy ì—°ì‚°**
```python
from pyflink.common.typeinfo import Types

# í‚¤ë³„ ê·¸ë£¹í™”
class LogEntry:
    def __init__(self, user_id, action, timestamp):
        self.user_id = user_id
        self.action = action
        self.timestamp = timestamp

# ìƒ˜í”Œ ë¡œê·¸ ë°ì´í„°
log_entries = [
    LogEntry("user1", "login", "2025-01-01 10:00:00"),
    LogEntry("user2", "view", "2025-01-01 10:01:00"),
    LogEntry("user1", "logout", "2025-01-01 10:02:00")
]

log_stream = env.from_collection(log_entries)
keyed_stream = log_stream.key_by(lambda log: log.user_id)
```

### ìœˆë„ìš° ì—°ì‚°

#### **1. Tumbling Window**
```python
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common.time import Time

# ê³ ì • í¬ê¸° ìœˆë„ìš°
windowed_stream = keyed_stream.window(
    TumblingProcessingTimeWindows.of(Time.seconds(10))
).sum(lambda log: 1)  # ê° ìœˆë„ìš°ì—ì„œì˜ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
```

#### **2. Sliding Window**
```python
from pyflink.datastream.window import SlidingProcessingTimeWindows

# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (10ì´ˆ í¬ê¸°, 5ì´ˆ ìŠ¬ë¼ì´ë“œ)
sliding_window = keyed_stream.window(
    SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))
).sum(lambda log: 1)
```

#### **3. Session Window**
```python
from pyflink.datastream.window import ProcessingTimeSessionWindows

# ì„¸ì…˜ ìœˆë„ìš° (5ì´ˆ ë¹„í™œì„± ì‹œê°„)
session_window = keyed_stream.window(
    ProcessingTimeSessionWindows.with_gap(Time.seconds(5))
).sum(lambda log: 1)
```

## â° ì‹œê°„ ì²˜ë¦¬ì™€ ì›Œí„°ë§ˆí‚¹

### ì‹œê°„ì˜ ì¢…ë¥˜

#### **1. Event Time**
```python
from pyflink.common.time import Time
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.typeinfo import Types

env = StreamExecutionEnvironment.get_execution_environment()

# ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ ì„¤ì •
env.set_stream_time_characteristic(TimeCharacteristic.EventTime)

# íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì›Œí„°ë§ˆí¬ ì„¤ì •
class TimestampedEvent:
    def __init__(self, value, timestamp):
        self.value = value
        self.timestamp = timestamp

# ì›Œí„°ë§ˆí¬ ìƒì„±ì ì •ì˜
class TimestampExtractor:
    def extract_timestamp(self, element, previous_timestamp):
        return element.timestamp * 1000  # ë°€ë¦¬ì´ˆë¡œ ë³€í™˜

events = env.from_collection([
    TimestampedEvent("event1", 1640995200),  # 2022-01-01 00:00:00
    TimestampedEvent("event2", 1640995260),  # 2022-01-01 00:01:00
    TimestampedEvent("event3", 1640995320),  # 2022-01-01 00:02:00
])

# ì›Œí„°ë§ˆí¬ í• ë‹¹
watermarked_stream = events.assign_timestamps_and_watermarks(
    TimestampExtractor()
)
```

#### **2. Processing Time**
```python
# ì²˜ë¦¬ ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ ì„¤ì •
env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)

# ì²˜ë¦¬ ì‹œê°„ì€ ìë™ìœ¼ë¡œ í• ë‹¹ë¨
processing_time_stream = env.from_collection([1, 2, 3, 4, 5])
```

#### **3. Ingestion Time**
```python
# ìˆ˜ì§‘ ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ ì„¤ì •
env.set_stream_time_characteristic(TimeCharacteristic.IngestionTime)

# ìˆ˜ì§‘ ì‹œê°„ì€ ë°ì´í„°ê°€ Flinkì— ë„ì°©í•  ë•Œ í• ë‹¹ë¨
ingestion_time_stream = env.from_collection([1, 2, 3, 4, 5])
```

### ì›Œí„°ë§ˆí‚¹ ì „ëµ

#### **1. Ascending Timestamps**
```python
from pyflink.datastream.functions import AscendingTimestampExtractor

class AscendingWatermarkExtractor(AscendingTimestampExtractor):
    def extract_timestamp(self, element, previous_timestamp):
        return element.timestamp * 1000

ascending_watermark = events.assign_timestamps_and_watermarks(
    AscendingWatermarkExtractor()
)
```

#### **2. Bounded Out-of-Orderness**
```python
from pyflink.datastream.functions import BoundedOutOfOrdernessTimestampExtractor
from pyflink.common.time import Time

class BoundedWatermarkExtractor(BoundedOutOfOrdernessTimestampExtractor):
    def __init__(self):
        super().__init__(Time.seconds(10))  # 10ì´ˆ ì§€ì—° í—ˆìš©
    
    def extract_timestamp(self, element, previous_timestamp):
        return element.timestamp * 1000

bounded_watermark = events.assign_timestamps_and_watermarks(
    BoundedWatermarkExtractor()
)
```

## ğŸ—ƒï¸ ìƒíƒœ ê´€ë¦¬ ê¸°ì´ˆ

### ìƒíƒœì˜ ì¢…ë¥˜

#### **1. ValueState**
```python
from pyflink.common.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types
from pyflink.datastream import KeyedProcessFunction

class CounterFunction(KeyedProcessFunction):
    def __init__(self):
        self.count_state = None
    
    def open(self, runtime_context):
        # ValueState ì´ˆê¸°í™”
        self.count_state = runtime_context.get_state(
            ValueStateDescriptor("count", Types.INT())
        )
    
    def process_element(self, value, ctx):
        # í˜„ì¬ ìƒíƒœ ê°’ ê°€ì ¸ì˜¤ê¸°
        current_count = self.count_state.value() or 0
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        new_count = current_count + 1
        self.count_state.update(new_count)
        
        # ê²°ê³¼ ì¶œë ¥
        ctx.collect(f"Key: {value}, Count: {new_count}")

# ì‚¬ìš© ì˜ˆì œ
counter_stream = keyed_stream.process(CounterFunction())
```

#### **2. ListState**
```python
from pyflink.common.state import ListStateDescriptor

class ListCollector(KeyedProcessFunction):
    def __init__(self):
        self.list_state = None
    
    def open(self, runtime_context):
        self.list_state = runtime_context.get_list_state(
            ListStateDescriptor("items", Types.STRING())
        )
    
    def process_element(self, value, ctx):
        # ë¦¬ìŠ¤íŠ¸ì— ê°’ ì¶”ê°€
        self.list_state.add(value)
        
        # ë¦¬ìŠ¤íŠ¸ ë‚´ìš© ìˆ˜ì§‘
        items = []
        for item in self.list_state.get():
            items.append(item)
        
        ctx.collect(f"Collected items: {items}")

list_stream = keyed_stream.process(ListCollector())
```

#### **3. MapState**
```python
from pyflink.common.state import MapStateDescriptor

class MapAggregator(KeyedProcessFunction):
    def __init__(self):
        self.map_state = None
    
    def open(self, runtime_context):
        self.map_state = runtime_context.get_map_state(
            MapStateDescriptor("counts", Types.STRING(), Types.INT())
        )
    
    def process_element(self, value, ctx):
        # ë§µì—ì„œ í˜„ì¬ ì¹´ìš´íŠ¸ ê°€ì ¸ì˜¤ê¸°
        current_count = self.map_state.get(value) or 0
        
        # ì¹´ìš´íŠ¸ ì¦ê°€
        self.map_state.put(value, current_count + 1)
        
        # ê²°ê³¼ ì¶œë ¥
        ctx.collect(f"Value: {value}, Count: {current_count + 1}")

map_stream = keyed_stream.process(MapAggregator())
```

### ì²´í¬í¬ì¸íŒ…

```python
from pyflink.common import Configuration
from pyflink.common.checkpointing import CheckpointingMode

# ì²´í¬í¬ì¸íŒ… ì„¤ì •
env.get_checkpoint_config().enable_checkpointing(1000)  # 1ì´ˆë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)

# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
env.get_checkpoint_config().set_checkpoint_storage_dir("file:///tmp/flink-checkpoints")

# ìµœëŒ€ ë™ì‹œ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
env.get_checkpoint_config().set_max_concurrent_checkpoints(1)

# ìµœì†Œ ì²´í¬í¬ì¸íŠ¸ ê°„ê²©
env.get_checkpoint_config().set_min_pause_between_checkpoints(500)
```

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„

### í”„ë¡œì íŠ¸ ê°œìš”

ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘
- ì‚¬ìš©ìë³„ í™œë™ ë¶„ì„
- ì´ìƒ íŒ¨í„´ íƒì§€
- ì‹¤ì‹œê°„ ì•Œë¦¼

### 1. ë¡œê·¸ ë°ì´í„° ëª¨ë¸ë§

```python
from dataclasses import dataclass
from typing import Optional
import json

@dataclass
class LogEvent:
    user_id: str
    session_id: str
    action: str
    timestamp: int
    ip_address: str
    user_agent: str
    metadata: Optional[dict] = None
    
    def to_json(self):
        return json.dumps({
            'user_id': self.user_id,
            'session_id': self.session_id,
            'action': self.action,
            'timestamp': self.timestamp,
            'ip_address': self.ip_address,
            'user_agent': self.user_agent,
            'metadata': self.metadata or {}
        })
    
    @classmethod
    def from_json(cls, json_str):
        data = json.loads(json_str)
        return cls(**data)
```

### 2. ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.typeinfo import Types
from pyflink.common.state import ValueStateDescriptor, ListStateDescriptor
from pyflink.common.time import Time
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.datastream.window import TumblingEventTimeWindows
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types

class LogAnalyzer:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.setup_environment()
    
    def setup_environment(self):
        # ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        self.env.get_checkpoint_config().enable_checkpointing(1000)
        
        # ì´ë²¤íŠ¸ ì‹œê°„ ì„¤ì •
        self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
    
    def create_kafka_source(self, topic, bootstrap_servers):
        """Kafka ì†ŒìŠ¤ ìƒì„±"""
        kafka_properties = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': 'log-analyzer-group'
        }
        
        kafka_source = FlinkKafkaConsumer(
            topic,
            SimpleStringSchema(),
            kafka_properties
        )
        
        return self.env.add_source(kafka_source)
    
    def parse_log_events(self, log_stream):
        """ë¡œê·¸ ì´ë²¤íŠ¸ íŒŒì‹±"""
        def parse_log(log_str):
            try:
                return LogEvent.from_json(log_str)
            except Exception as e:
                print(f"Failed to parse log: {log_str}, Error: {e}")
                return None
        
        return log_stream.map(parse_log, output_type=Types.PICKLED_BYTE_ARRAY()) \
                        .filter(lambda x: x is not None)
    
    def detect_anomalies(self, log_stream):
        """ì´ìƒ íŒ¨í„´ íƒì§€"""
        class AnomalyDetector(KeyedProcessFunction):
            def __init__(self):
                self.failed_login_count = None
                self.last_login_time = None
                self.suspicious_actions = None
            
            def open(self, runtime_context):
                self.failed_login_count = runtime_context.get_state(
                    ValueStateDescriptor("failed_logins", Types.INT())
                )
                self.last_login_time = runtime_context.get_state(
                    ValueStateDescriptor("last_login", Types.LONG())
                )
                self.suspicious_actions = runtime_context.get_list_state(
                    ListStateDescriptor("suspicious", Types.STRING())
                )
            
            def process_element(self, log_event, ctx):
                current_time = log_event.timestamp
                
                # ì‹¤íŒ¨í•œ ë¡œê·¸ì¸ ì‹œë„ ì¶”ì 
                if log_event.action == "login_failed":
                    current_count = self.failed_login_count.value() or 0
                    self.failed_login_count.update(current_count + 1)
                    
                    # 5ë¶„ ë‚´ 5íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
                    if current_count >= 4:  # 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ 4ë©´ 5ë²ˆì§¸
                        ctx.collect({
                            'type': 'security_alert',
                            'user_id': log_event.user_id,
                            'message': f'Multiple failed login attempts: {current_count + 1}',
                            'timestamp': current_time
                        })
                        self.failed_login_count.clear()
                
                # ì„±ê³µí•œ ë¡œê·¸ì¸ ì‹œ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
                elif log_event.action == "login_success":
                    self.failed_login_count.clear()
                    self.last_login_time.update(current_time)
                
                # ë¹„ì •ìƒì ì¸ ì‹œê°„ëŒ€ í™œë™ ê°ì§€
                elif log_event.action in ["view", "purchase", "download"]:
                    last_login = self.last_login_time.value()
                    if last_login and (current_time - last_login) > 86400:  # 24ì‹œê°„
                        ctx.collect({
                            'type': 'anomaly_alert',
                            'user_id': log_event.user_id,
                            'message': f'Activity after long inactivity: {log_event.action}',
                            'timestamp': current_time
                        })
        
        # ì‚¬ìš©ìë³„ë¡œ í‚¤ ì§€ì •í•˜ê³  ì´ìƒ íƒì§€
        return log_stream.key_by(lambda log: log.user_id) \
                        .process(AnomalyDetector())
    
    def calculate_user_metrics(self, log_stream):
        """ì‚¬ìš©ìë³„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        class UserMetricsCalculator(KeyedProcessFunction):
            def __init__(self):
                self.action_counts = None
                self.session_duration = None
                self.last_action_time = None
            
            def open(self, runtime_context):
                self.action_counts = runtime_context.get_map_state(
                    MapStateDescriptor("action_counts", Types.STRING(), Types.INT())
                )
                self.session_duration = runtime_context.get_state(
                    ValueStateDescriptor("session_duration", Types.LONG())
                )
                self.last_action_time = runtime_context.get_state(
                    ValueStateDescriptor("last_action_time", Types.LONG())
                )
            
            def process_element(self, log_event, ctx):
                current_time = log_event.timestamp
                
                # ì•¡ì…˜ë³„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                current_count = self.action_counts.get(log_event.action) or 0
                self.action_counts.put(log_event.action, current_count + 1)
                
                # ì„¸ì…˜ ì§€ì† ì‹œê°„ ê³„ì‚°
                last_time = self.last_action_time.value()
                if last_time:
                    duration = current_time - last_time
                    current_duration = self.session_duration.value() or 0
                    self.session_duration.update(current_duration + duration)
                
                self.last_action_time.update(current_time)
                
                # 1ë¶„ë§ˆë‹¤ ë©”íŠ¸ë¦­ ì¶œë ¥
                if current_time % 60 == 0:
                    metrics = {
                        'user_id': log_event.user_id,
                        'timestamp': current_time,
                        'session_duration': self.session_duration.value() or 0,
                        'action_counts': dict(self.action_counts.items())
                    }
                    ctx.collect(metrics)
        
        return log_stream.key_by(lambda log: log.user_id) \
                        .process(UserMetricsCalculator())
    
    def run_analysis(self):
        """ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        # Kafkaì—ì„œ ë¡œê·¸ ì½ê¸°
        log_stream = self.create_kafka_source("user-logs", "localhost:9092")
        
        # ë¡œê·¸ íŒŒì‹±
        parsed_logs = self.parse_log_events(log_stream)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì›Œí„°ë§ˆí¬ í• ë‹¹
        watermarked_logs = parsed_logs.assign_timestamps_and_watermarks(
            TimestampExtractor()
        )
        
        # ì´ìƒ íƒì§€
        anomalies = self.detect_anomalies(watermarked_logs)
        
        # ì‚¬ìš©ì ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self.calculate_user_metrics(watermarked_logs)
        
        # ê²°ê³¼ ì¶œë ¥
        anomalies.print("Security Alerts")
        metrics.print("User Metrics")
        
        # ì‹¤í–‰
        self.env.execute("Real-time Log Analysis")

# ì›Œí„°ë§ˆí¬ ì¶”ì¶œê¸°
class TimestampExtractor(BoundedOutOfOrdernessTimestampExtractor):
    def __init__(self):
        super().__init__(Time.seconds(10))  # 10ì´ˆ ì§€ì—° í—ˆìš©
    
    def extract_timestamp(self, element, previous_timestamp):
        return element.timestamp * 1000

if __name__ == "__main__":
    analyzer = LogAnalyzer()
    analyzer.run_analysis()
```

### 3. ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°
class LogGenerator:
    def __init__(self):
        import random
        import time
        self.users = ["user1", "user2", "user3", "user4", "user5"]
        self.actions = ["login", "logout", "view", "purchase", "download", "login_failed"]
        self.ip_addresses = ["192.168.1.1", "192.168.1.2", "10.0.0.1", "10.0.0.2"]
    
    def generate_log(self):
        import random
        import time
        
        return LogEvent(
            user_id=random.choice(self.users),
            session_id=f"session_{random.randint(1000, 9999)}",
            action=random.choice(self.actions),
            timestamp=int(time.time()),
            ip_address=random.choice(self.ip_addresses),
            user_agent="Mozilla/5.0 (Test Browser)"
        ).to_json()

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def test_log_analysis():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    generator = LogGenerator()
    test_logs = [generator.generate_log() for _ in range(100)]
    
    log_stream = env.from_collection(test_logs)
    
    # ë¡œê·¸ íŒŒì‹±
    parsed_logs = log_stream.map(
        lambda x: LogEvent.from_json(x),
        output_type=Types.PICKLED_BYTE_ARRAY()
    ).filter(lambda x: x is not None)
    
    # ê°„ë‹¨í•œ í†µê³„ ê³„ì‚°
    user_actions = parsed_logs.map(
        lambda log: (log.user_id, log.action, 1)
    ).key_by(lambda x: x[0]) \
     .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) \
     .sum(2)
    
    user_actions.print()
    
    env.execute("Log Analysis Test")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    test_log_analysis()
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Apache Flink ì†Œê°œ**
   - Flinkì˜ íƒ„ìƒ ë°°ê²½ê³¼ ì„¤ê³„ ì² í•™
   - Sparkì™€ì˜ ê·¼ë³¸ì  ì°¨ì´ì 
   - ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ ê°œë…

2. **Flink ì•„í‚¤í…ì²˜**
   - JobManagerì™€ TaskManagerì˜ ì—­í• 
   - í´ëŸ¬ìŠ¤í„° êµ¬ì„±ê³¼ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
   - í”„ë¡œê·¸ë¨ ì‹¤í–‰ êµ¬ì¡°

3. **DataStream API**
   - ê¸°ë³¸ ë°ì´í„° íƒ€ì…ê³¼ ë³€í™˜ ì—°ì‚°
   - Map, Filter, FlatMap, KeyBy ì—°ì‚°
   - ìœˆë„ìš° ì—°ì‚° (Tumbling, Sliding, Session)

4. **ì‹œê°„ ì²˜ë¦¬**
   - Event Time, Processing Time, Ingestion Time
   - ì›Œí„°ë§ˆí‚¹ ì „ëµê³¼ ì§€ì—° ë°ì´í„° ì²˜ë¦¬
   - íƒ€ì„ìŠ¤íƒ¬í”„ í• ë‹¹ ë°©ë²•

5. **ìƒíƒœ ê´€ë¦¬**
   - ValueState, ListState, MapState
   - ì²´í¬í¬ì¸íŒ…ê³¼ ì¥ì•  ë³µêµ¬
   - ìƒíƒœ ê¸°ë°˜ ì²˜ë¦¬ ë¡œì§

6. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ
   - ì´ìƒ íŒ¨í„´ íƒì§€
   - ì‚¬ìš©ì ë©”íŠ¸ë¦­ ê³„ì‚°

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **DataStream API** | ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ | â­â­â­â­â­ |
| **ìƒíƒœ ê´€ë¦¬** | ìƒíƒœ ê¸°ë°˜ ì²˜ë¦¬ | â­â­â­â­â­ |
| **ì‹œê°„ ì²˜ë¦¬** | ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ | â­â­â­â­â­ |
| **ìœˆë„ìš° ì—°ì‚°** | ì‹œê°„ ê¸°ë°˜ ì§‘ê³„ | â­â­â­â­ |
| **ì²´í¬í¬ì¸íŒ…** | ì¥ì•  ë³µêµ¬ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 2: ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ íŒ¨í„´
- ì²´í¬í¬ì¸íŒ…ê³¼ ì„¸ì´ë¸Œí¬ì¸íŠ¸ ì‹¬í™”
- ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬ ì „ëµ
- ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 2: ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬](/data-engineering/2025/09/16/apache-flink-advanced-streaming.html)

---

*ì´ì œ Flinkì˜ ê¸°ì´ˆë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.* ğŸš€

---
layout: post
lang: ko
title: "Part 4: Apache Flink í”„ë¡œë•ì…˜ ë°°í¬ì™€ ì„±ëŠ¥ ìµœì í™” - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìš´ì˜ì˜ ì™„ì„±"
description: "Apache Flinkë¥¼ Kubernetesì—ì„œ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ê³ , ì„±ëŠ¥ì„ ìµœì í™”í•˜ë©°, ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµì„ êµ¬í˜„í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-09-18
author: Data Droid
category: data-engineering
tags: [Apache-Flink, Kubernetes, í”„ë¡œë•ì…˜ë°°í¬, ì„±ëŠ¥ìµœì í™”, ëª¨ë‹ˆí„°ë§, ì¥ì• ë³µêµ¬, DevOps, CI/CD]
series: apache-flink-complete-guide
series_order: 4
reading_time: "50ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 4: Apache Flink í”„ë¡œë•ì…˜ ë°°í¬ì™€ ì„±ëŠ¥ ìµœì í™” - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìš´ì˜ì˜ ì™„ì„±

> Apache Flinkë¥¼ Kubernetesì—ì„œ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ê³ , ì„±ëŠ¥ì„ ìµœì í™”í•˜ë©°, ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµì„ êµ¬í˜„í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [Kubernetesë¥¼ í™œìš©í•œ Flink í´ëŸ¬ìŠ¤í„° ë°°í¬](#kubernetesë¥¼-í™œìš©í•œ-flink-í´ëŸ¬ìŠ¤í„°-ë°°í¬)
2. [ì„±ëŠ¥ íŠœë‹ê³¼ ìµœì í™”](#ì„±ëŠ¥-íŠœë‹ê³¼-ìµœì í™”)
3. [ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ](#ëª¨ë‹ˆí„°ë§ê³¼-ì•Œë¦¼-ì‹œìŠ¤í…œ)
4. [ì¥ì•  ë³µêµ¬ì™€ ìš´ì˜ ì „ëµ](#ì¥ì• -ë³µêµ¬ì™€-ìš´ì˜-ì „ëµ)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ Flink í”Œë«í¼](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì—”í„°í”„ë¼ì´ì¦ˆê¸‰-flink-í”Œë«í¼)
6. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸš€ Kubernetesë¥¼ í™œìš©í•œ Flink í´ëŸ¬ìŠ¤í„° ë°°í¬

### Flink on Kubernetes ì•„í‚¤í…ì²˜

Kubernetesì—ì„œ Flinkë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **ìë™ ìŠ¤ì¼€ì¼ë§**: ì›Œí¬ë¡œë“œì— ë”°ë¼ ìë™ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì¡°ì •
- **ê³ ê°€ìš©ì„±**: Pod ì¥ì•  ì‹œ ìë™ ë³µêµ¬
- **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ì™€ CPU ì‚¬ìš©
- **ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬**: ë‚´ì¥ëœ ë„¤íŠ¸ì›Œí‚¹ê³¼ ë¡œë“œ ë°¸ëŸ°ì‹±

### 1. Flink Operator ì„¤ì¹˜

```yaml
# flink-operator-install.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: flink-operator
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-operator
  namespace: flink-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flink-operator
  template:
    metadata:
      labels:
        app: flink-operator
    spec:
      serviceAccountName: flink-operator
      containers:
      - name: flink-operator
        image: apache/flink-kubernetes-operator:1.7.0
        ports:
        - containerPort: 8080
        env:
        - name: OPERATOR_NAME
          value: "flink-operator"
        - name: WATCH_NAMESPACE
          value: ""
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flink-operator
  namespace: flink-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: flink-operator
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: flink-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flink-operator
subjects:
- kind: ServiceAccount
  name: flink-operator
  namespace: flink-operator
```

### 2. Flink Application ë°°í¬

```yaml
# flink-application.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: real-time-analytics
  namespace: flink-production
spec:
  image: flink:1.18-scala_2.12-java11
  flinkVersion: "1.18"
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "4"
    parallelism.default: "4"
    state.backend: "rocksdb"
    state.checkpoints.dir: "s3://flink-checkpoints/checkpoints"
    state.savepoints.dir: "s3://flink-savepoints/savepoints"
    execution.checkpointing.interval: "60s"
    execution.checkpointing.mode: "EXACTLY_ONCE"
    execution.checkpointing.timeout: "600s"
    execution.checkpointing.min-pause: "60s"
    execution.checkpointing.max-concurrent-checkpoints: "1"
    restart-strategy: "fixed-delay"
    restart-strategy.fixed-delay.attempts: "3"
    restart-strategy.fixed-delay.delay: "10s"
    high-availability: "kubernetes"
    high-availability.storageDir: "s3://flink-ha/ha"
  serviceAccount: flink-service-account
  jobManager:
    replicas: 1
    resource:
      memory: "2048m"
      cpu: "1000m"
    podTemplate:
      spec:
        containers:
        - name: flink-main-container
          env:
          - name: JAVA_OPTS
            value: "-Xmx1536m"
          resources:
            requests:
              memory: "2048m"
              cpu: "1000m"
            limits:
              memory: "2048m"
              cpu: "1000m"
  taskManager:
    replicas: 3
    resource:
      memory: "4096m"
      cpu: "2000m"
    podTemplate:
      spec:
        containers:
        - name: flink-main-container
          env:
          - name: JAVA_OPTS
            value: "-Xmx3072m"
          resources:
            requests:
              memory: "4096m"
              cpu: "2000m"
            limits:
              memory: "4096m"
              cpu: "2000m"
  job:
    jarURI: "s3://flink-jobs/real-time-analytics.jar"
    parallelism: 8
    upgradeMode: "stateless"
    allowNonRestoredState: false
```

### 3. ê³ ê°€ìš©ì„± ì„¤ì •

```yaml
# flink-ha-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-ha-config
  namespace: flink-production
data:
  flink-conf.yaml: |
    # ê³ ê°€ìš©ì„± ì„¤ì •
    high-availability: kubernetes
    high-availability.cluster-id: flink-cluster
    high-availability.storageDir: s3://flink-ha/ha
    
    # ZooKeeper ì„¤ì • (ì„ íƒì‚¬í•­)
    # high-availability.zookeeper.quorum: zookeeper-service:2181
    # high-availability.zookeeper.path.root: /flink
    
    # Kubernetes ë¦¬ë” ì„ íƒê¸°
    kubernetes.operator.leader.election.lease-duration: 15s
    kubernetes.operator.leader.election.renew-deadline: 10s
    kubernetes.operator.leader.election.retry-period: 2s
    
    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    state.backend: rocksdb
    state.backend.incremental: true
    state.checkpoints.dir: s3://flink-checkpoints/checkpoints
    state.savepoints.dir: s3://flink-savepoints/savepoints
    execution.checkpointing.interval: 60s
    execution.checkpointing.mode: EXACTLY_ONCE
    execution.checkpointing.timeout: 600s
    execution.checkpointing.min-pause: 60s
    execution.checkpointing.max-concurrent-checkpoints: 1
    
    # ì¬ì‹œì‘ ì „ëµ
    restart-strategy: fixed-delay
    restart-strategy.fixed-delay.attempts: 3
    restart-strategy.fixed-delay.delay: 10s
    
    # ë©”íŠ¸ë¦­ ì„¤ì •
    metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
    metrics.reporter.prom.port: 9249
    metrics.system-resource: true
    metrics.system-resource-probing-interval: 5000
```

### 4. ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •

```yaml
# flink-autoscaling.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: auto-scaling-flink-app
  namespace: flink-production
spec:
  flinkConfiguration:
    # ìë™ ìŠ¤ì¼€ì¼ë§ í™œì„±í™”
    kubernetes.operator.auto-scaling.enabled: "true"
    kubernetes.operator.auto-scaling.metrics.window: "60s"
    kubernetes.operator.auto-scaling.metrics.delay: "30s"
    kubernetes.operator.auto-scaling.scale-up.grace-period: "60s"
    kubernetes.operator.auto-scaling.scale-down.grace-period: "300s"
    
    # ìŠ¤ì¼€ì¼ë§ ì„ê³„ê°’
    kubernetes.operator.auto-scaling.target.utilization: "0.8"
    kubernetes.operator.auto-scaling.min.replicas: "2"
    kubernetes.operator.auto-scaling.max.replicas: "10"
    
    # ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­
    kubernetes.operator.auto-scaling.metrics.jvm.cpu: "true"
    kubernetes.operator.auto-scaling.metrics.jvm.memory: "true"
    kubernetes.operator.auto-scaling.metrics.operator.num-records-in: "true"
    kubernetes.operator.auto-scaling.metrics.operator.num-records-out: "true"
  taskManager:
    replicas: 2
    # HPA ì„¤ì •
    horizontalPodAutoscaler:
      minReplicas: 2
      maxReplicas: 10
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 80
      - type: Resource
        resource:
          name: memory
          target:
            type: Utilization
            averageUtilization: 80
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 10
            periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 100
            periodSeconds: 60
```

## âš¡ ì„±ëŠ¥ íŠœë‹ê³¼ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```python
class FlinkMemoryOptimizer:
    def __init__(self):
        self.memory_configs = {}
    
    def generate_optimized_config(self, workload_type, data_volume):
        """ì›Œí¬ë¡œë“œ íƒ€ì…ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”"""
        
        if workload_type == "streaming":
            return self.get_streaming_memory_config(data_volume)
        elif workload_type == "batch":
            return self.get_batch_memory_config(data_volume)
        elif workload_type == "machine_learning":
            return self.get_ml_memory_config(data_volume)
        else:
            return self.get_default_memory_config()
    
    def get_streaming_memory_config(self, data_volume):
        """ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬ë¡œë“œìš© ë©”ëª¨ë¦¬ ì„¤ì •"""
        if data_volume == "low":  # < 1GB/min
            return {
                "taskmanager.memory.process.size": "2g",
                "taskmanager.memory.managed.size": "512m",
                "taskmanager.memory.jvm-metaspace.size": "256m",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.2"
            }
        elif data_volume == "medium":  # 1-10GB/min
            return {
                "taskmanager.memory.process.size": "4g",
                "taskmanager.memory.managed.size": "1g",
                "taskmanager.memory.jvm-metaspace.size": "512m",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.2"
            }
        else:  # > 10GB/min
            return {
                "taskmanager.memory.process.size": "8g",
                "taskmanager.memory.managed.size": "2g",
                "taskmanager.memory.jvm-metaspace.size": "1g",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.2"
            }
    
    def get_batch_memory_config(self, data_volume):
        """ë°°ì¹˜ ì›Œí¬ë¡œë“œìš© ë©”ëª¨ë¦¬ ì„¤ì •"""
        if data_volume == "small":  # < 100GB
            return {
                "taskmanager.memory.process.size": "4g",
                "taskmanager.memory.managed.size": "2g",
                "taskmanager.memory.jvm-metaspace.size": "512m",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.1"
            }
        elif data_volume == "large":  # 100GB - 1TB
            return {
                "taskmanager.memory.process.size": "8g",
                "taskmanager.memory.managed.size": "4g",
                "taskmanager.memory.jvm-metaspace.size": "1g",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.1"
            }
        else:  # > 1TB
            return {
                "taskmanager.memory.process.size": "16g",
                "taskmanager.memory.managed.size": "8g",
                "taskmanager.memory.jvm-metaspace.size": "2g",
                "taskmanager.memory.jvm-overhead.fraction": "0.1",
                "taskmanager.memory.network.fraction": "0.1"
            }
    
    def get_ml_memory_config(self, data_volume):
        """ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬ë¡œë“œìš© ë©”ëª¨ë¦¬ ì„¤ì •"""
        return {
            "taskmanager.memory.process.size": "12g",
            "taskmanager.memory.managed.size": "6g",
            "taskmanager.memory.jvm-metaspace.size": "1g",
            "taskmanager.memory.jvm-overhead.fraction": "0.15",
            "taskmanager.memory.network.fraction": "0.1"
        }
    
    def get_default_memory_config(self):
        """ê¸°ë³¸ ë©”ëª¨ë¦¬ ì„¤ì •"""
        return {
            "taskmanager.memory.process.size": "4g",
            "taskmanager.memory.managed.size": "1g",
            "taskmanager.memory.jvm-metaspace.size": "512m",
            "taskmanager.memory.jvm-overhead.fraction": "0.1",
            "taskmanager.memory.network.fraction": "0.2"
        }
```

### 2. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

```python
class NetworkOptimizer:
    def __init__(self):
        self.network_configs = {}
    
    def optimize_network_performance(self, cluster_size, data_throughput):
        """ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìµœì í™”"""
        
        config = {
            # ë„¤íŠ¸ì›Œí¬ ë²„í¼ ì„¤ì •
            "taskmanager.network.memory.fraction": "0.2",
            "taskmanager.network.memory.min": "128mb",
            "taskmanager.network.memory.max": "1gb",
            
            # ë„¤íŠ¸ì›Œí¬ ìŠ¤ë ˆë“œ ì„¤ì •
            "taskmanager.network.netty.num-arenas": str(min(cluster_size, 4)),
            "taskmanager.network.netty.server.numThreads": "4",
            "taskmanager.network.netty.client.numThreads": "4",
            
            # ì—°ê²° ì„¤ì •
            "taskmanager.network.netty.server.backlog": "0",
            "taskmanager.network.netty.client.connectTimeoutSec": "10",
            
            # ë²„í¼ ì„¤ì •
            "taskmanager.network.memory.buffers-per-channel": "2",
            "taskmanager.network.memory.floating-buffers-per-gate": "8",
        }
        
        # ë°ì´í„° ì²˜ë¦¬ëŸ‰ì— ë”°ë¥¸ ì¶”ê°€ ìµœì í™”
        if data_throughput > 1000:  # MB/s
            config.update({
                "taskmanager.network.memory.fraction": "0.3",
                "taskmanager.network.memory.max": "2gb",
                "taskmanager.network.memory.buffers-per-channel": "4",
                "taskmanager.network.memory.floating-buffers-per-gate": "16",
            })
        
        return config
    
    def optimize_shuffle_performance(self, data_size, parallelism):
        """Shuffle ì„±ëŠ¥ ìµœì í™”"""
        
        # ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
        optimal_partitions = min(max(data_size // (100 * 1024 * 1024), 1), 1000)
        
        config = {
            "taskmanager.numberOfTaskSlots": str(min(parallelism, 4)),
            "parallelism.default": str(parallelism),
            
            # Shuffle ìµœì í™”
            "taskmanager.memory.segment-size": "32kb",
            "taskmanager.network.sort-shuffle.min-parallelism": "1",
            "taskmanager.network.sort-shuffle.min-buffers": "64",
            
            # ì••ì¶• ì„¤ì •
            "taskmanager.network.compression.enabled": "true",
            "taskmanager.network.compression.codec": "lz4",
        }
        
        return config
```

### 3. ìƒíƒœ ë°±ì—”ë“œ ìµœì í™”

```python
class StateBackendOptimizer:
    def __init__(self):
        self.rocksdb_configs = {}
    
    def optimize_rocksdb_backend(self, state_size, access_pattern):
        """RocksDB ë°±ì—”ë“œ ìµœì í™”"""
        
        if access_pattern == "read_heavy":
            return self.get_read_optimized_config(state_size)
        elif access_pattern == "write_heavy":
            return self.get_write_optimized_config(state_size)
        elif access_pattern == "mixed":
            return self.get_balanced_config(state_size)
        else:
            return self.get_default_config()
    
    def get_read_optimized_config(self, state_size):
        """ì½ê¸° ìµœì í™” ì„¤ì •"""
        block_cache_size = min(state_size // 4, 512 * 1024 * 1024)  # 512MB max
        
        return {
            # ë¸”ë¡ ìºì‹œ ìµœì í™”
            "state.backend.rocksdb.block.cache-size": f"{block_cache_size}b",
            "state.backend.rocksdb.block.cache-size-per-column-family": f"{block_cache_size // 4}b",
            
            # ë¸”ë¡ í¬ê¸° ìµœì í™”
            "state.backend.rocksdb.block.blocksize": "16kb",
            "state.backend.rocksdb.block.bloom-filter-bits-per-key": "10",
            
            # ì••ì¶• ì„¤ì •
            "state.backend.rocksdb.compaction.level0-file-num-compaction-trigger": "4",
            "state.backend.rocksdb.compaction.level0-slowdown-writes-trigger": "20",
            "state.backend.rocksdb.compaction.level0-stop-writes-trigger": "36",
            
            # ì½ê¸° ìµœì í™”
            "state.backend.rocksdb.readoptions.async-io": "true",
            "state.backend.rocksdb.readoptions.cache-index-and-filter-blocks": "true",
        }
    
    def get_write_optimized_config(self, state_size):
        """ì“°ê¸° ìµœì í™” ì„¤ì •"""
        write_buffer_size = min(state_size // 8, 128 * 1024 * 1024)  # 128MB max
        
        return {
            # ì“°ê¸° ë²„í¼ ìµœì í™”
            "state.backend.rocksdb.write-buffer-size": f"{write_buffer_size}b",
            "state.backend.rocksdb.max-write-buffer-number": "4",
            "state.backend.rocksdb.min-write-buffer-number-to-merge": "2",
            
            # WAL ì„¤ì •
            "state.backend.rocksdb.wal.enabled": "true",
            "state.backend.rocksdb.wal.sync": "false",
            
            # ì••ì¶• ì„¤ì •
            "state.backend.rocksdb.compaction.level0-file-num-compaction-trigger": "2",
            "state.backend.rocksdb.compaction.level0-slowdown-writes-trigger": "8",
            "state.backend.rocksdb.compaction.level0-stop-writes-trigger": "12",
            
            # ì“°ê¸° ìµœì í™”
            "state.backend.rocksdb.writeoptions.sync": "false",
            "state.backend.rocksdb.writeoptions.disable-wal": "false",
        }
    
    def get_balanced_config(self, state_size):
        """ê· í˜• ì¡íŒ ì„¤ì •"""
        return {
            # ë¸”ë¡ ìºì‹œ
            "state.backend.rocksdb.block.cache-size": f"{min(state_size // 8, 256 * 1024 * 1024)}b",
            
            # ì“°ê¸° ë²„í¼
            "state.backend.rocksdb.write-buffer-size": f"{min(state_size // 16, 64 * 1024 * 1024)}b",
            "state.backend.rocksdb.max-write-buffer-number": "3",
            
            # ì••ì¶• ì„¤ì •
            "state.backend.rocksdb.compaction.level0-file-num-compaction-trigger": "4",
            "state.backend.rocksdb.compaction.level0-slowdown-writes-trigger": "20",
            "state.backend.rocksdb.compaction.level0-stop-writes-trigger": "36",
            
            # ì¼ë°˜ ì„¤ì •
            "state.backend.rocksdb.block.blocksize": "16kb",
            "state.backend.rocksdb.wal.enabled": "true",
        }
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ

### 1. Prometheus ë©”íŠ¸ë¦­ ì„¤ì •

```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "flink_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'flink-jobmanager'
        static_configs:
          - targets: ['flink-jobmanager:9249']
        scrape_interval: 10s
        metrics_path: /metrics
        
      - job_name: 'flink-taskmanager'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - flink-production
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: flink-taskmanager
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            action: keep
            regex: metrics
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: instance
        scrape_interval: 10s
        metrics_path: /metrics
        
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
```

### 2. Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
{
  "dashboard": {
    "title": "Apache Flink Production Dashboard",
    "panels": [
      {
        "id": 1,
        "title": "Job Status",
        "type": "stat",
        "targets": [
          {
            "expr": "flink_jobmanager_numRunningJobs",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 5},
                {"color": "red", "value": 10}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Records Processed Per Second",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(flink_taskmanager_job_task_operator_numRecordsInPerSecond[5m])) by (job_name)",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Records/sec",
            "min": 0
          }
        ]
      },
      {
        "id": 3,
        "title": "Checkpoint Duration",
        "type": "timeseries",
        "targets": [
          {
            "expr": "flink_jobmanager_job_lastCheckpointDuration",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Duration (ms)",
            "min": 0
          }
        ]
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "flink_taskmanager_Status_JVM_Memory_Heap_Used",
            "refId": "A"
          },
          {
            "expr": "flink_taskmanager_Status_JVM_Memory_Heap_Max",
            "refId": "B"
          }
        ],
        "yAxes": [
          {
            "label": "Memory (bytes)",
            "min": 0
          }
        ]
      },
      {
        "id": 5,
        "title": "Backpressure",
        "type": "timeseries",
        "targets": [
          {
            "expr": "flink_taskmanager_job_task_backPressuredTimeMsPerSecond",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Backpressure (ms/sec)",
            "min": 0
          }
        ]
      }
    ]
  }
}
```

### 3. ì•Œë¦¼ ê·œì¹™ ì„¤ì •

```yaml
# flink-alert-rules.yml
groups:
  - name: flink-alerts
    rules:
      - alert: FlinkJobFailed
        expr: flink_jobmanager_job_status == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Flink job has failed"
          description: "Flink job has been in failed state for more than 1 minute"
      
      - alert: HighCheckpointDuration
        expr: flink_jobmanager_job_lastCheckpointDuration > 60000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High checkpoint duration detected"
          description: "Checkpoint duration is above the threshold"
      
      - alert: HighBackpressure
        expr: flink_taskmanager_job_task_backPressuredTimeMsPerSecond > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High backpressure detected"
          description: "Task is experiencing high backpressure"
      
      - alert: HighMemoryUsage
        expr: (flink_taskmanager_Status_JVM_Memory_Heap_Used / flink_taskmanager_Status_JVM_Memory_Heap_Max) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "TaskManager memory usage is above threshold"
      
      - alert: LowThroughput
        expr: rate(flink_taskmanager_job_task_operator_numRecordsInPerSecond[5m]) < 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low throughput detected"
          description: "Job throughput is below normal threshold"
```

## ğŸ”§ ì¥ì•  ë³µêµ¬ì™€ ìš´ì˜ ì „ëµ

### 1. ìë™ ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ

```python
class FlinkDisasterRecovery:
    def __init__(self, kubernetes_client, flink_client):
        self.k8s_client = kubernetes_client
        self.flink_client = flink_client
        self.recovery_strategies = {}
    
    def setup_disaster_recovery(self):
        """ì¬í•´ ë³µêµ¬ ì‹œìŠ¤í…œ ì„¤ì •"""
        
        # 1. ì²´í¬í¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§
        self.setup_checkpoint_monitoring()
        
        # 2. ìë™ ì¬ì‹œì‘ ì„¤ì •
        self.setup_auto_restart()
        
        # 3. ë°±ì—… ë° ë³µì› ì‹œìŠ¤í…œ
        self.setup_backup_system()
        
        # 4. ì¥ì•  ì „íŒŒ ë°©ì§€
        self.setup_circuit_breaker()
    
    def setup_checkpoint_monitoring(self):
        """ì²´í¬í¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        checkpoint_config = {
            "execution.checkpointing.interval": "60s",
            "execution.checkpointing.mode": "EXACTLY_ONCE",
            "execution.checkpointing.timeout": "600s",
            "execution.checkpointing.min-pause": "60s",
            "execution.checkpointing.max-concurrent-checkpoints": "1",
            "execution.checkpointing.unaligned": "true",
            "execution.checkpointing.alignment-timeout": "0s",
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ
            "state.backend": "rocksdb",
            "state.checkpoints.dir": "s3://flink-checkpoints/checkpoints",
            "state.savepoints.dir": "s3://flink-savepoints/savepoints",
            
            # ê³ ê°€ìš©ì„±
            "high-availability": "kubernetes",
            "high-availability.storageDir": "s3://flink-ha/ha",
            "high-availability.cluster-id": "flink-cluster"
        }
        
        return checkpoint_config
    
    def setup_auto_restart(self):
        """ìë™ ì¬ì‹œì‘ ì„¤ì •"""
        restart_config = {
            "restart-strategy": "exponential-delay",
            "restart-strategy.exponential-delay.initial-backoff": "10s",
            "restart-strategy.exponential-delay.max-backoff": "2min",
            "restart-strategy.exponential-delay.backoff-multiplier": "2.0",
            "restart-strategy.exponential-delay.reset-backoff-threshold": "10min",
            "restart-strategy.exponential-delay.jitter-factor": "0.1"
        }
        
        return restart_config
    
    def setup_backup_system(self):
        """ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •"""
        backup_config = {
            # ìë™ ì„¸ì´ë¸Œí¬ì¸íŠ¸
            "execution.savepoint.interval": "1h",
            "execution.savepoint.trigger": "periodic",
            
            # ë°±ì—… ì €ì¥ì†Œ
            "state.backend.incremental": "true",
            "state.backend.rocksdb.checkpoint.transfer.thread.num": "4",
            
            # ì••ì¶• ì„¤ì •
            "state.backend.rocksdb.compression.type": "LZ4",
            "state.backend.rocksdb.compaction.level.max-size-level-base": "256mb"
        }
        
        return backup_config
    
    def setup_circuit_breaker(self):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ ì„¤ì •"""
        circuit_breaker_config = {
            # ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
            "taskmanager.network.netty.client.connectTimeoutSec": "30",
            "taskmanager.network.netty.client.requestTimeoutSec": "60",
            
            # ì¬ì‹œë„ ì„¤ì •
            "taskmanager.network.retries": "3",
            "taskmanager.network.retry-delay": "10s",
            
            # ë°±í”„ë ˆì…” ì„¤ì •
            "taskmanager.network.memory.fraction": "0.2",
            "taskmanager.network.memory.max": "1gb"
        }
        
        return circuit_breaker_config
    
    def execute_disaster_recovery(self, failure_type, job_name):
        """ì¬í•´ ë³µêµ¬ ì‹¤í–‰"""
        
        if failure_type == "job_failure":
            return self.recover_from_job_failure(job_name)
        elif failure_type == "cluster_failure":
            return self.recover_from_cluster_failure(job_name)
        elif failure_type == "data_corruption":
            return self.recover_from_data_corruption(job_name)
        else:
            return self.recover_from_unknown_failure(job_name)
    
    def recover_from_job_failure(self, job_name):
        """ì‘ì—… ì‹¤íŒ¨ì—ì„œ ë³µêµ¬"""
        recovery_steps = [
            "1. ì‹¤íŒ¨í•œ ì‘ì—… ì¤‘ì§€",
            "2. ìµœì‹  ì²´í¬í¬ì¸íŠ¸ í™•ì¸",
            "3. ìƒíƒœ ë°±ì—”ë“œ ê²€ì¦",
            "4. ì‘ì—… ì¬ì‹œì‘",
            "5. ë³µêµ¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§"
        ]
        
        # ì‹¤ì œ ë³µêµ¬ ë¡œì§ êµ¬í˜„
        try:
            # 1. ì‘ì—… ì¤‘ì§€
            self.flink_client.cancel_job(job_name)
            
            # 2. ìµœì‹  ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            latest_checkpoint = self.get_latest_checkpoint(job_name)
            
            # 3. ìƒíƒœ ê²€ì¦
            if self.validate_checkpoint(latest_checkpoint):
                # 4. ì‘ì—… ì¬ì‹œì‘
                self.flink_client.restart_job(job_name, latest_checkpoint)
                return {"status": "success", "message": "Job recovered successfully"}
            else:
                # 5. ì„¸ì´ë¸Œí¬ì¸íŠ¸ì—ì„œ ë³µêµ¬ ì‹œë„
                latest_savepoint = self.get_latest_savepoint(job_name)
                if latest_savepoint:
                    self.flink_client.restart_job(job_name, latest_savepoint)
                    return {"status": "success", "message": "Job recovered from savepoint"}
                else:
                    return {"status": "failed", "message": "No valid checkpoint or savepoint found"}
                    
        except Exception as e:
            return {"status": "error", "message": f"Recovery failed: {str(e)}"}
    
    def recover_from_cluster_failure(self, job_name):
        """í´ëŸ¬ìŠ¤í„° ì‹¤íŒ¨ì—ì„œ ë³µêµ¬"""
        recovery_steps = [
            "1. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸",
            "2. Kubernetes ë¦¬ì†ŒìŠ¤ ì¬ì‹œì‘",
            "3. Flink í´ëŸ¬ìŠ¤í„° ì¬ë°°í¬",
            "4. ì‘ì—… ë³µêµ¬",
            "5. ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦"
        ]
        
        try:
            # 1. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
            cluster_status = self.check_cluster_status()
            
            if cluster_status["healthy"]:
                return self.recover_from_job_failure(job_name)
            else:
                # 2. í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘
                self.restart_flink_cluster()
                
                # 3. ì‘ì—… ë³µêµ¬
                return self.recover_from_job_failure(job_name)
                
        except Exception as e:
            return {"status": "error", "message": f"Cluster recovery failed: {str(e)}"}
    
    def get_latest_checkpoint(self, job_name):
        """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ"""
        checkpoints = self.flink_client.list_checkpoints(job_name)
        if checkpoints:
            return max(checkpoints, key=lambda x: x["timestamp"])
        return None
    
    def get_latest_savepoint(self, job_name):
        """ìµœì‹  ì„¸ì´ë¸Œí¬ì¸íŠ¸ ì¡°íšŒ"""
        savepoints = self.flink_client.list_savepoints(job_name)
        if savepoints:
            return max(savepoints, key=lambda x: x["timestamp"])
        return None
    
    def validate_checkpoint(self, checkpoint):
        """ì²´í¬í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
        if not checkpoint:
            return False
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        # ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ê²€ì¦
        # ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì‚¬
        
        return True
    
    def check_cluster_status(self):
        """í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
        try:
            # Kubernetes í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
            pods = self.k8s_client.list_pods_for_all_namespaces(
                label_selector="app=flink"
            )
            
            healthy_pods = 0
            total_pods = len(pods.items)
            
            for pod in pods.items:
                if pod.status.phase == "Running":
                    healthy_pods += 1
            
            health_ratio = healthy_pods / total_pods if total_pods > 0 else 0
            
            return {
                "healthy": health_ratio > 0.8,
                "health_ratio": health_ratio,
                "total_pods": total_pods,
                "healthy_pods": healthy_pods
            }
            
        except Exception as e:
            return {"healthy": False, "error": str(e)}
    
    def restart_flink_cluster(self):
        """Flink í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘"""
        try:
            # JobManager ì¬ì‹œì‘
            self.k8s_client.delete_namespaced_deployment(
                name="flink-jobmanager",
                namespace="flink-production"
            )
            
            # TaskManager ì¬ì‹œì‘
            self.k8s_client.delete_namespaced_deployment(
                name="flink-taskmanager",
                namespace="flink-production"
            )
            
            # ì¬ì‹œì‘ ëŒ€ê¸°
            import time
            time.sleep(30)
            
            return {"status": "success", "message": "Cluster restarted successfully"}
            
        except Exception as e:
            return {"status": "error", "message": f"Cluster restart failed: {str(e)}"}
```

### 2. ìš´ì˜ ìë™í™” ì‹œìŠ¤í…œ

```python
class FlinkOperationsAutomation:
    def __init__(self):
        self.operation_templates = {}
        self.monitoring_thresholds = {}
    
    def setup_operational_automation(self):
        """ìš´ì˜ ìë™í™” ì‹œìŠ¤í…œ ì„¤ì •"""
        
        # 1. ìë™ ìŠ¤ì¼€ì¼ë§
        self.setup_auto_scaling()
        
        # 2. ìë™ ë°±ì—…
        self.setup_auto_backup()
        
        # 3. ìë™ ì—…ë°ì´íŠ¸
        self.setup_auto_update()
        
        # 4. ìë™ ëª¨ë‹ˆí„°ë§
        self.setup_auto_monitoring()
    
    def setup_auto_scaling(self):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •"""
        scaling_config = {
            "kubernetes.operator.auto-scaling.enabled": "true",
            "kubernetes.operator.auto-scaling.metrics.window": "60s",
            "kubernetes.operator.auto-scaling.metrics.delay": "30s",
            "kubernetes.operator.auto-scaling.scale-up.grace-period": "60s",
            "kubernetes.operator.auto-scaling.scale-down.grace-period": "300s",
            "kubernetes.operator.auto-scaling.target.utilization": "0.8",
            "kubernetes.operator.auto-scaling.min.replicas": "2",
            "kubernetes.operator.auto-scaling.max.replicas": "10"
        }
        
        return scaling_config
    
    def setup_auto_backup(self):
        """ìë™ ë°±ì—… ì„¤ì •"""
        backup_schedule = {
            "checkpoint_interval": "1h",
            "savepoint_interval": "24h",
            "retention_policy": "7d",
            "backup_storage": "s3://flink-backups/",
            "compression": "gzip",
            "encryption": "aes256"
        }
        
        return backup_schedule
    
    def setup_auto_update(self):
        """ìë™ ì—…ë°ì´íŠ¸ ì„¤ì •"""
        update_config = {
            "update_strategy": "rolling",
            "update_window": "02:00-06:00",
            "health_check_timeout": "300s",
            "rollback_threshold": "10%",
            "canary_percentage": "10%"
        }
        
        return update_config
    
    def setup_auto_monitoring(self):
        """ìë™ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        monitoring_config = {
            "metrics_collection_interval": "10s",
            "alert_evaluation_interval": "30s",
            "retention_period": "30d",
            "anomaly_detection": "enabled",
            "performance_baseline": "auto"
        }
        
        return monitoring_config

class FlinkPerformanceOptimizer:
    def __init__(self):
        self.optimization_rules = {}
    
    def optimize_for_workload(self, workload_type, metrics):
        """ì›Œí¬ë¡œë“œ íƒ€ì…ì— ë”°ë¥¸ ì„±ëŠ¥ ìµœì í™”"""
        
        if workload_type == "low_latency":
            return self.optimize_for_low_latency(metrics)
        elif workload_type == "high_throughput":
            return self.optimize_for_high_throughput(metrics)
        elif workload_type == "memory_intensive":
            return self.optimize_for_memory_intensive(metrics)
        else:
            return self.optimize_for_balanced(metrics)
    
    def optimize_for_low_latency(self, metrics):
        """ì €ì§€ì—° ìµœì í™”"""
        optimizations = {
            "execution.buffer-timeout": "1ms",
            "execution.checkpointing.interval": "10s",
            "execution.checkpointing.timeout": "30s",
            "taskmanager.network.memory.fraction": "0.1",
            "taskmanager.network.memory.min": "64mb",
            "taskmanager.network.memory.max": "256mb",
            "taskmanager.network.netty.server.backlog": "0",
            "taskmanager.network.netty.client.connectTimeoutSec": "5"
        }
        
        return optimizations
    
    def optimize_for_high_throughput(self, metrics):
        """ê³ ì²˜ë¦¬ëŸ‰ ìµœì í™”"""
        optimizations = {
            "execution.buffer-timeout": "100ms",
            "execution.checkpointing.interval": "300s",
            "execution.checkpointing.timeout": "600s",
            "taskmanager.network.memory.fraction": "0.3",
            "taskmanager.network.memory.min": "512mb",
            "taskmanager.network.memory.max": "2gb",
            "taskmanager.network.memory.buffers-per-channel": "4",
            "taskmanager.network.memory.floating-buffers-per-gate": "16"
        }
        
        return optimizations
    
    def optimize_for_memory_intensive(self, metrics):
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ìµœì í™”"""
        optimizations = {
            "taskmanager.memory.managed.size": "4g",
            "taskmanager.memory.process.size": "8g",
            "state.backend.rocksdb.block.cache-size": "2gb",
            "state.backend.rocksdb.write-buffer-size": "256mb",
            "state.backend.rocksdb.max-write-buffer-number": "4",
            "state.backend.rocksdb.compaction.level0-file-num-compaction-trigger": "2"
        }
        
        return optimizations
```

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ Flink í”Œë«í¼

### í”„ë¡œì íŠ¸ ê°œìš”

ëŒ€ê·œëª¨ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ Apache Flinkë¥¼ ìš´ì˜í•˜ê¸° ìœ„í•œ ì™„ì „í•œ í”Œë«í¼ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ í”Œë«í¼ì€ ìë™í™”, ëª¨ë‹ˆí„°ë§, ì¥ì•  ë³µêµ¬, ì„±ëŠ¥ ìµœì í™”ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.

### 1. í”Œë«í¼ ì•„í‚¤í…ì²˜

```python
class EnterpriseFlinkPlatform:
    def __init__(self):
        self.components = {
            "deployment": FlinkDeploymentManager(),
            "monitoring": FlinkMonitoringSystem(),
            "automation": FlinkOperationsAutomation(),
            "recovery": FlinkDisasterRecovery(),
            "optimization": FlinkPerformanceOptimizer()
        }
    
    def deploy_enterprise_platform(self):
        """ì—”í„°í”„ë¼ì´ì¦ˆ í”Œë«í¼ ë°°í¬"""
        
        deployment_steps = [
            "1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° RBAC ì„¤ì •",
            "2. Flink Operator ì„¤ì¹˜",
            "3. ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬",
            "4. Flink í´ëŸ¬ìŠ¤í„° ë°°í¬",
            "5. ìë™í™” ê·œì¹™ ì„¤ì •",
            "6. ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì„±",
            "7. ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •"
        ]
        
        for step in deployment_steps:
            print(f"Executing: {step}")
            self.execute_deployment_step(step)
    
    def execute_deployment_step(self, step):
        """ë°°í¬ ë‹¨ê³„ ì‹¤í–‰"""
        if "ë„¤ì„ìŠ¤í˜ì´ìŠ¤" in step:
            self.create_namespaces_and_rbac()
        elif "Operator" in step:
            self.install_flink_operator()
        elif "ëª¨ë‹ˆí„°ë§" in step:
            self.deploy_monitoring_stack()
        elif "í´ëŸ¬ìŠ¤í„°" in step:
            self.deploy_flink_cluster()
        elif "ìë™í™”" in step:
            self.setup_automation_rules()
        elif "ì•Œë¦¼" in step:
            self.configure_alerting()
        elif "ë°±ì—…" in step:
            self.setup_backup_system()
    
    def create_namespaces_and_rbac(self):
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° RBAC ìƒì„±"""
        namespaces = [
            "flink-production",
            "flink-staging",
            "monitoring",
            "logging"
        ]
        
        for namespace in namespaces:
            self.create_namespace(namespace)
            self.create_rbac_for_namespace(namespace)
    
    def install_flink_operator(self):
        """Flink Operator ì„¤ì¹˜"""
        operator_manifest = """
        apiVersion: v1
        kind: Namespace
        metadata:
          name: flink-operator
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: flink-operator
          namespace: flink-operator
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: flink-operator
          template:
            metadata:
              labels:
                app: flink-operator
            spec:
              serviceAccountName: flink-operator
              containers:
              - name: flink-operator
                image: apache/flink-kubernetes-operator:1.7.0
                ports:
                - containerPort: 8080
                env:
                - name: OPERATOR_NAME
                  value: "flink-operator"
                - name: WATCH_NAMESPACE
                  value: ""
                resources:
                  requests:
                    memory: "256Mi"
                    cpu: "100m"
                  limits:
                    memory: "512Mi"
                    cpu: "500m"
        """
        
        self.apply_manifest(operator_manifest)
    
    def deploy_monitoring_stack(self):
        """ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬"""
        monitoring_components = [
            "prometheus",
            "grafana",
            "alertmanager",
            "jaeger",
            "elasticsearch",
            "kibana"
        ]
        
        for component in monitoring_components:
            self.deploy_monitoring_component(component)
    
    def deploy_flink_cluster(self):
        """Flink í´ëŸ¬ìŠ¤í„° ë°°í¬"""
        cluster_configs = {
            "production": self.get_production_cluster_config(),
            "staging": self.get_staging_cluster_config(),
            "development": self.get_development_cluster_config()
        }
        
        for env, config in cluster_configs.items():
            self.deploy_cluster_for_environment(env, config)
    
    def get_production_cluster_config(self):
        """í”„ë¡œë•ì…˜ í´ëŸ¬ìŠ¤í„° ì„¤ì •"""
        return {
            "flinkConfiguration": {
                "taskmanager.numberOfTaskSlots": "8",
                "parallelism.default": "16",
                "state.backend": "rocksdb",
                "state.checkpoints.dir": "s3://prod-flink-checkpoints/checkpoints",
                "state.savepoints.dir": "s3://prod-flink-savepoints/savepoints",
                "execution.checkpointing.interval": "60s",
                "execution.checkpointing.mode": "EXACTLY_ONCE",
                "high-availability": "kubernetes",
                "high-availability.storageDir": "s3://prod-flink-ha/ha",
                "restart-strategy": "exponential-delay",
                "restart-strategy.exponential-delay.initial-backoff": "10s",
                "restart-strategy.exponential-delay.max-backoff": "2min",
                "metrics.reporter.prom.class": "org.apache.flink.metrics.prometheus.PrometheusReporter",
                "metrics.reporter.prom.port": "9249"
            },
            "jobManager": {
                "replicas": 2,
                "resource": {
                    "memory": "4g",
                    "cpu": "2000m"
                }
            },
            "taskManager": {
                "replicas": 5,
                "resource": {
                    "memory": "8g",
                    "cpu": "4000m"
                }
            }
        }
    
    def get_staging_cluster_config(self):
        """ìŠ¤í…Œì´ì§• í´ëŸ¬ìŠ¤í„° ì„¤ì •"""
        return {
            "flinkConfiguration": {
                "taskmanager.numberOfTaskSlots": "4",
                "parallelism.default": "8",
                "state.backend": "rocksdb",
                "state.checkpoints.dir": "s3://staging-flink-checkpoints/checkpoints",
                "execution.checkpointing.interval": "120s",
                "execution.checkpointing.mode": "AT_LEAST_ONCE",
                "restart-strategy": "fixed-delay",
                "restart-strategy.fixed-delay.attempts": "3",
                "restart-strategy.fixed-delay.delay": "10s"
            },
            "jobManager": {
                "replicas": 1,
                "resource": {
                    "memory": "2g",
                    "cpu": "1000m"
                }
            },
            "taskManager": {
                "replicas": 2,
                "resource": {
                    "memory": "4g",
                    "cpu": "2000m"
                }
            }
        }
    
    def get_development_cluster_config(self):
        """ê°œë°œ í´ëŸ¬ìŠ¤í„° ì„¤ì •"""
        return {
            "flinkConfiguration": {
                "taskmanager.numberOfTaskSlots": "2",
                "parallelism.default": "2",
                "state.backend": "memory",
                "execution.checkpointing.interval": "300s",
                "execution.checkpointing.mode": "AT_LEAST_ONCE",
                "restart-strategy": "fixed-delay",
                "restart-strategy.fixed-delay.attempts": "1",
                "restart-strategy.fixed-delay.delay": "5s"
            },
            "jobManager": {
                "replicas": 1,
                "resource": {
                    "memory": "1g",
                    "cpu": "500m"
                }
            },
            "taskManager": {
                "replicas": 1,
                "resource": {
                    "memory": "2g",
                    "cpu": "1000m"
                }
            }
        }
    
    def setup_automation_rules(self):
        """ìë™í™” ê·œì¹™ ì„¤ì •"""
        automation_rules = {
            "auto_scaling": {
                "enabled": True,
                "min_replicas": 2,
                "max_replicas": 10,
                "target_cpu": 80,
                "target_memory": 80,
                "scale_up_cooldown": "60s",
                "scale_down_cooldown": "300s"
            },
            "auto_backup": {
                "enabled": True,
                "checkpoint_interval": "1h",
                "savepoint_interval": "24h",
                "retention_days": 7,
                "backup_storage": "s3://flink-backups/"
            },
            "auto_update": {
                "enabled": False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìˆ˜ë™ ìŠ¹ì¸ í•„ìš”
                "update_window": "02:00-06:00",
                "canary_percentage": 10,
                "health_check_timeout": "300s"
            },
            "auto_recovery": {
                "enabled": True,
                "max_restart_attempts": 3,
                "restart_delay": "10s",
                "escalation_timeout": "300s"
            }
        }
        
        for rule_type, config in automation_rules.items():
            self.configure_automation_rule(rule_type, config)
    
    def configure_alerting(self):
        """ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì„±"""
        alert_rules = {
            "critical": [
                "FlinkJobFailed",
                "ClusterDown",
                "DataLoss",
                "CheckpointFailure"
            ],
            "warning": [
                "HighCheckpointDuration",
                "HighBackpressure",
                "HighMemoryUsage",
                "LowThroughput"
            ],
            "info": [
                "JobStarted",
                "JobCompleted",
                "ScalingEvent",
                "BackupCompleted"
            ]
        }
        
        notification_channels = {
            "email": {
                "enabled": True,
                "recipients": ["admin@company.com", "oncall@company.com"],
                "severity": ["critical", "warning"]
            },
            "slack": {
                "enabled": True,
                "webhook_url": "https://hooks.slack.com/services/...",
                "channels": ["#flink-alerts", "#data-team"],
                "severity": ["critical", "warning"]
            },
            "pagerduty": {
                "enabled": True,
                "integration_key": "integration_key_here",
                "severity": ["critical"]
            }
        }
        
        self.configure_alert_rules(alert_rules)
        self.configure_notification_channels(notification_channels)
    
    def setup_backup_system(self):
        """ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •"""
        backup_config = {
            "checkpoints": {
                "enabled": True,
                "interval": "1h",
                "retention": "7d",
                "storage": "s3://flink-checkpoints/",
                "compression": "gzip"
            },
            "savepoints": {
                "enabled": True,
                "interval": "24h",
                "retention": "30d",
                "storage": "s3://flink-savepoints/",
                "compression": "gzip"
            },
            "configurations": {
                "enabled": True,
                "interval": "1d",
                "retention": "90d",
                "storage": "s3://flink-configs/",
                "include": ["flink-conf.yaml", "log4j.properties"]
            },
            "state_backend": {
                "enabled": True,
                "interval": "1h",
                "retention": "7d",
                "storage": "s3://flink-state/",
                "encryption": "aes256"
            }
        }
        
        self.configure_backup_system(backup_config)
    
    def create_comprehensive_dashboard(self):
        """ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        dashboard_config = {
            "title": "Enterprise Flink Platform Dashboard",
            "panels": [
                {
                    "title": "Cluster Overview",
                    "type": "stat",
                    "metrics": [
                        "flink_jobmanager_numRunningJobs",
                        "flink_jobmanager_numCompletedJobs",
                        "flink_jobmanager_numFailedJobs"
                    ]
                },
                {
                    "title": "Performance Metrics",
                    "type": "timeseries",
                    "metrics": [
                        "flink_taskmanager_job_task_operator_numRecordsInPerSecond",
                        "flink_taskmanager_job_task_operator_numRecordsOutPerSecond",
                        "flink_taskmanager_job_task_operator_latency"
                    ]
                },
                {
                    "title": "Resource Usage",
                    "type": "timeseries",
                    "metrics": [
                        "flink_taskmanager_Status_JVM_Memory_Heap_Used",
                        "flink_taskmanager_Status_JVM_CPU_Load",
                        "flink_taskmanager_Status_JVM_Threads_Count"
                    ]
                },
                {
                    "title": "Checkpoint Health",
                    "type": "timeseries",
                    "metrics": [
                        "flink_jobmanager_job_lastCheckpointDuration",
                        "flink_jobmanager_job_lastCheckpointSize",
                        "flink_jobmanager_job_lastCheckpointAlignmentBuffered"
                    ]
                },
                {
                    "title": "Backpressure",
                    "type": "heatmap",
                    "metrics": [
                        "flink_taskmanager_job_task_backPressuredTimeMsPerSecond"
                    ]
                }
            ]
        }
        
        return dashboard_config

# í”Œë«í¼ ì‹¤í–‰ ì˜ˆì œ
def deploy_enterprise_flink_platform():
    """ì—”í„°í”„ë¼ì´ì¦ˆ Flink í”Œë«í¼ ë°°í¬"""
    platform = EnterpriseFlinkPlatform()
    
    print("ğŸš€ Starting Enterprise Flink Platform Deployment...")
    
    # 1. í”Œë«í¼ ë°°í¬
    platform.deploy_enterprise_platform()
    
    # 2. ëŒ€ì‹œë³´ë“œ ìƒì„±
    dashboard = platform.create_comprehensive_dashboard()
    
    # 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    platform.run_integration_tests()
    
    print("âœ… Enterprise Flink Platform deployed successfully!")
    
    return platform

if __name__ == "__main__":
    platform = deploy_enterprise_flink_platform()
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Kubernetesë¥¼ í™œìš©í•œ Flink í´ëŸ¬ìŠ¤í„° ë°°í¬**
   - Flink Operator ì„¤ì¹˜ì™€ ì„¤ì •
   - ê³ ê°€ìš©ì„± í´ëŸ¬ìŠ¤í„° êµ¬ì„±
   - ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •
   - í™˜ê²½ë³„ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬

2. **ì„±ëŠ¥ íŠœë‹ê³¼ ìµœì í™”**
   - ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ
   - ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìµœì í™”
   - ìƒíƒœ ë°±ì—”ë“œ ìµœì í™”
   - ì›Œí¬ë¡œë“œë³„ ì„±ëŠ¥ íŠœë‹

3. **ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ**
   - Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±
   - ì•Œë¦¼ ê·œì¹™ ì„¤ì •
   - ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

4. **ì¥ì•  ë³µêµ¬ì™€ ìš´ì˜ ì „ëµ**
   - ìë™ ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ
   - ìš´ì˜ ìë™í™”
   - ë°±ì—… ë° ë³µì› ì „ëµ
   - ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´

5. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ Flink í”Œë«í¼
   - ì™„ì „ ìë™í™”ëœ ìš´ì˜ í™˜ê²½
   - ë‹¤ì¤‘ í™˜ê²½ ê´€ë¦¬
   - ì¢…í•© ëŒ€ì‹œë³´ë“œ ì‹œìŠ¤í…œ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **Kubernetes** | ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | â­â­â­â­â­ |
| **Flink Operator** | Kubernetes ë„¤ì´í‹°ë¸Œ ê´€ë¦¬ | â­â­â­â­â­ |
| **Prometheus/Grafana** | ëª¨ë‹ˆí„°ë§ê³¼ ì‹œê°í™” | â­â­â­â­â­ |
| **ìë™í™” ì‹œìŠ¤í…œ** | ìš´ì˜ íš¨ìœ¨ì„± | â­â­â­â­ |
| **ì¥ì•  ë³µêµ¬** | ì‹œìŠ¤í…œ ì•ˆì •ì„± | â­â­â­â­ |

### Apache Flink ì‹œë¦¬ì¦ˆ ì™„ì„±!

ì¶•í•˜í•©ë‹ˆë‹¤! Apache Flink ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆë¥¼ ëª¨ë‘ ì™„ì£¼í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‰

#### **ì‹œë¦¬ì¦ˆ ì „ì²´ ìš”ì•½:**

- âœ… **Part 1**: Apache Flink ê¸°ì´ˆì™€ í•µì‹¬ ê°œë…
- âœ… **Part 2**: ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬
- âœ… **Part 3**: ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEP
- âœ… **Part 4**: í”„ë¡œë•ì…˜ ë°°í¬ì™€ ì„±ëŠ¥ ìµœì í™”

#### **ìŠµë“í•œ í•µì‹¬ ì—­ëŸ‰:**

1. **ê¸°ìˆ ì  ì—­ëŸ‰**
   - Apache Flinkì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ ì´í•´
   - DataStream APIë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì²˜ë¦¬ êµ¬í˜„
   - ìƒíƒœ ê´€ë¦¬ì™€ ì²´í¬í¬ì¸íŒ… í™œìš©
   - ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ (CEP) êµ¬í˜„
   - í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ì™€ ìš´ì˜

2. **ì‹¤ë¬´ ì—­ëŸ‰**
   - ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
   - ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ ì§€ì—°ì‹œê°„ ë‹¬ì„±
   - ì¥ì•  ë³µêµ¬ì™€ ìš´ì˜ ìë™í™”
   - ì„±ëŠ¥ ìµœì í™”ì™€ ëª¨ë‹ˆí„°ë§

3. **ì—”í„°í”„ë¼ì´ì¦ˆ ì—­ëŸ‰**
   - Kubernetes ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
   - ìë™í™”ëœ ìš´ì˜ ì‹œìŠ¤í…œ êµ¬ì¶•
   - ì¢…í•© ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ
   - ì¥ì•  ë³µêµ¬ì™€ ë°±ì—… ì „ëµ

ì´ì œ Apache Flinkë¥¼ í™œìš©í•˜ì—¬ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì§€ì‹ê³¼ ê²½í—˜ì„ ê°–ì¶”ì…¨ìŠµë‹ˆë‹¤! ğŸš€

---

**ì‹œë¦¬ì¦ˆ ì™„ì„±**: [Apache Flink ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ](/data-engineering/2025/09/14/apache-flink-series-overview.html)

---

*Apache Flinkì˜ ì„¸ê³„ì—ì„œ ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ í˜ì„ ê²½í—˜í•˜ì„¸ìš”!* ğŸ¯

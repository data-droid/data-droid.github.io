---
layout: post
lang: ko
title: "Apache Kafka ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ: í”„ë¡œë“€ì„œë¶€í„° ì»¨ìŠˆë¨¸ê¹Œì§€"
description: "ëŒ€ìš©ëŸ‰ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” Apache Kafkaì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ë¬´ í™œìš© ë°©ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤."
date: 2025-09-09
author: Data Droid
category: data-engineering
tags: [Apache-Kafka, ì‹¤ì‹œê°„ìŠ¤íŠ¸ë¦¬ë°, ë°ì´í„°íŒŒì´í”„ë¼ì¸, ë©”ì‹œì§€í, ì´ë²¤íŠ¸ë“œë¦¬ë¸, ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤]
reading_time: "30ë¶„"
difficulty: "ê³ ê¸‰"
---

# Apache Kafka ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ: í”„ë¡œë“€ì„œë¶€í„° ì»¨ìŠˆë¨¸ê¹Œì§€

> ëŒ€ìš©ëŸ‰ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” Apache Kafkaì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ë¬´ í™œìš© ë°©ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Kafka ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´](#kafka-ì•„í‚¤í…ì²˜-ì‹¬í™”-ì´í•´)
2. [í”„ë¡œë“€ì„œ ìµœì í™” ì „ëµ](#í”„ë¡œë“€ì„œ-ìµœì í™”-ì „ëµ)
3. [ì»¨ìŠˆë¨¸ ê·¸ë£¹ê³¼ íŒŒí‹°ì…”ë‹](#ì»¨ìŠˆë¨¸-ê·¸ë£¹ê³¼-íŒŒí‹°ì…”ë‹)
4. [ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ KStreams](#ìŠ¤íŠ¸ë¦¬ë°-ì²˜ë¦¬ì™€-kstreams)
5. [ëª¨ë‹ˆí„°ë§ê³¼ ìš´ì˜ ê´€ë¦¬](#ëª¨ë‹ˆí„°ë§ê³¼-ìš´ì˜-ê´€ë¦¬)
6. [ì‹¤ìŠµ: ì‹¤ë¬´ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ êµ¬ì¶•](#ì‹¤ìŠµ-ì‹¤ë¬´ê¸‰-ìŠ¤íŠ¸ë¦¬ë°-ì‹œìŠ¤í…œ-êµ¬ì¶•)
7. [ì„±ëŠ¥ íŠœë‹ê³¼ í™•ì¥ì„±](#ì„±ëŠ¥-íŠœë‹ê³¼-í™•ì¥ì„±)
8. [ë³´ì•ˆê³¼ ê¶Œí•œ ê´€ë¦¬](#ë³´ì•ˆê³¼-ê¶Œí•œ-ê´€ë¦¬)
9. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ— ï¸ Kafka ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´ {#kafka-ì•„í‚¤í…ì²˜-ì‹¬í™”-ì´í•´}

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„

Apache KafkaëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

#### **1. Broker**
- **ì—­í• **: ë©”ì‹œì§€ ì €ì¥ ë° ì „ë‹¬ì˜ í•µì‹¬ ì—”ì§„
- **ë™ì‘ ì›ë¦¬**: íŒŒí‹°ì…˜ë³„ë¡œ ë©”ì‹œì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì €ì¥
- **ì„±ëŠ¥ ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬, ì••ì¶•, ì¸ë±ì‹±

#### **2. Topic & Partition**
- **Topic**: ë©”ì‹œì§€ì˜ ë…¼ë¦¬ì  ë¶„ë¥˜
- **Partition**: Topicì˜ ë¬¼ë¦¬ì  ë¶„í•  ë‹¨ìœ„
- **Replication**: ë°ì´í„° ì•ˆì •ì„± ë³´ì¥

#### **3. Producer**
- **ì—­í• **: ë©”ì‹œì§€ ìƒì‚° ë° ì „ì†¡
- **íŠ¹ì§•**: ë¹„ë™ê¸° ì „ì†¡, ë°°ì¹˜ ì²˜ë¦¬, ì••ì¶•
- **ì„±ëŠ¥**: Throughputê³¼ Latencyì˜ ê· í˜•

#### **4. Consumer**
- **ì—­í• **: ë©”ì‹œì§€ ì†Œë¹„ ë° ì²˜ë¦¬
- **Consumer Group**: ë³‘ë ¬ ì²˜ë¦¬ ë° ë¶€í•˜ ë¶„ì‚°
- **Offset Management**: ë©”ì‹œì§€ ì²˜ë¦¬ ìƒíƒœ ì¶”ì 

### ë©”ì‹œì§€ ì „ë‹¬ ë³´ì¥ (Delivery Guarantees)

```yaml
# ë©”ì‹œì§€ ì „ë‹¬ ë³´ì¥ ë ˆë²¨
at-least-once:  # ìµœì†Œ í•œ ë²ˆ ì „ë‹¬ (ì¤‘ë³µ ê°€ëŠ¥)
exactly-once:   # ì •í™•íˆ í•œ ë²ˆ ì „ë‹¬ (ì¤‘ë³µ ì—†ìŒ)
at-most-once:   # ìµœëŒ€ í•œ ë²ˆ ì „ë‹¬ (ì†ì‹¤ ê°€ëŠ¥)
```

## âš¡ í”„ë¡œë“€ì„œ ìµœì í™” ì „ëµ {#í”„ë¡œë“€ì„œ-ìµœì í™”-ì „ëµ}

### 1. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```java
// Producer ì„¤ì • ìµœì í™”
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
props.put("batch.size", 16384);                    // ë°°ì¹˜ í¬ê¸° (16KB)
props.put("linger.ms", 5);                         // ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„ (5ms)
props.put("compression.type", "snappy");           // ì••ì¶• íƒ€ì…
props.put("acks", "all");                          // ëª¨ë“  ë³µì œë³¸ í™•ì¸
props.put("retries", 3);                           // ì¬ì‹œë„ íšŸìˆ˜
props.put("retry.backoff.ms", 100);               // ì¬ì‹œë„ ê°„ê²©

// ê³ ì„±ëŠ¥ ì„¤ì •
props.put("buffer.memory", 33554432);              // ë²„í¼ ë©”ëª¨ë¦¬ (32MB)
props.put("max.in.flight.requests.per.connection", 5); // ë™ì‹œ ìš”ì²­ ìˆ˜
props.put("enable.idempotence", true);             // ë©±ë“±ì„± ë³´ì¥

KafkaProducer<String, String> producer = new KafkaProducer<>(props);
```

### 2. íŒŒí‹°ì…”ë‹ ì „ëµ

```java
// ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ êµ¬í˜„
public class CustomPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, 
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        
        // í‚¤ ê¸°ë°˜ íŒŒí‹°ì…”ë‹
        if (key != null) {
            return Math.abs(key.hashCode()) % numPartitions;
        }
        
        // ë¼ìš´ë“œ ë¡œë¹ˆ íŒŒí‹°ì…”ë‹
        return ThreadLocalRandom.current().nextInt(numPartitions);
    }
    
    @Override
    public void close() {}
    
    @Override
    public void configure(Map<String, ?> configs) {}
}

// íŒŒí‹°ì…”ë„ˆ ì„¤ì •
props.put("partitioner.class", "com.example.CustomPartitioner");
```

### 3. ì••ì¶•ê³¼ ì••ì¶•

```java
// ì••ì¶• ì„¤ì • ë¹„êµ
public class CompressionComparison {
    
    // Snappy ì••ì¶• (ë¹ ë¥¸ ì••ì¶•/í•´ì œ)
    public void setupSnappyCompression() {
        props.put("compression.type", "snappy");
        // ì¥ì : ë‚®ì€ CPU ì‚¬ìš©ëŸ‰, ë¹ ë¥¸ ì²˜ë¦¬
        // ë‹¨ì : ì••ì¶•ë¥ ì´ ë‚®ìŒ
    }
    
    // Gzip ì••ì¶• (ë†’ì€ ì••ì¶•ë¥ )
    public void setupGzipCompression() {
        props.put("compression.type", "gzip");
        // ì¥ì : ë†’ì€ ì••ì¶•ë¥ , ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ˆì•½
        // ë‹¨ì : ë†’ì€ CPU ì‚¬ìš©ëŸ‰
    }
    
    // LZ4 ì••ì¶• (ê· í˜•ì¡íŒ ì„±ëŠ¥)
    public void setupLZ4Compression() {
        props.put("compression.type", "lz4");
        // ì¥ì : ë¹ ë¥¸ ì••ì¶•/í•´ì œ, ì ë‹¹í•œ ì••ì¶•ë¥ 
        // ë‹¨ì : Snappyë³´ë‹¤ ì•½ê°„ ëŠë¦¼
    }
}
```

## ğŸ”„ ì»¨ìŠˆë¨¸ ê·¸ë£¹ê³¼ íŒŒí‹°ì…”ë‹ {#ì»¨ìŠˆë¨¸-ê·¸ë£¹ê³¼-íŒŒí‹°ì…”ë‹}

### 1. ì»¨ìŠˆë¨¸ ê·¸ë£¹ ê´€ë¦¬

```java
// ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì„¤ì •
Properties consumerProps = new Properties();
consumerProps.put("bootstrap.servers", "localhost:9092");
consumerProps.put("group.id", "my-consumer-group");
consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

// ì˜¤í”„ì…‹ ê´€ë¦¬ ì„¤ì •
consumerProps.put("auto.offset.reset", "earliest");  // earliest, latest, none
consumerProps.put("enable.auto.commit", false);      // ìˆ˜ë™ ì˜¤í”„ì…‹ ì»¤ë°‹
consumerProps.put("max.poll.records", 500);          // í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜
consumerProps.put("session.timeout.ms", 30000);      // ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
consumerProps.put("heartbeat.interval.ms", 3000);    // í•˜íŠ¸ë¹„íŠ¸ ê°„ê²©

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
```

### 2. ìˆ˜ë™ ì˜¤í”„ì…‹ ê´€ë¦¬

```java
// ìˆ˜ë™ ì˜¤í”„ì…‹ ì»¤ë°‹ ì˜ˆì œ
public class ManualOffsetCommit {
    private KafkaConsumer<String, String> consumer;
    private Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
    
    public void processMessages() {
        consumer.subscribe(Arrays.asList("my-topic"));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, String> record : records) {
                    // ë©”ì‹œì§€ ì²˜ë¦¬
                    processMessage(record);
                    
                    // ì˜¤í”„ì…‹ ì €ì¥ (ë‹¤ìŒì— ì»¤ë°‹í•  ì˜¤í”„ì…‹)
                    TopicPartition partition = new TopicPartition(record.topic(), record.partition());
                    OffsetAndMetadata offset = new OffsetAndMetadata(record.offset() + 1);
                    offsets.put(partition, offset);
                }
                
                // ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜¤í”„ì…‹ ì»¤ë°‹
                if (!offsets.isEmpty()) {
                    consumer.commitSync(offsets);
                    offsets.clear();
                }
            }
        } catch (Exception e) {
            // ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤í”„ì…‹ ë¡¤ë°±
            consumer.seekToBeginning(consumer.assignment());
        }
    }
    
    private void processMessage(ConsumerRecord<String, String> record) {
        // ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§
        System.out.printf("Topic: %s, Partition: %d, Offset: %d, Key: %s, Value: %s%n",
                record.topic(), record.partition(), record.offset(), 
                record.key(), record.value());
    }
}
```

### 3. ë¦¬ë°¸ëŸ°ì‹± ì²˜ë¦¬

```java
// ë¦¬ë°¸ëŸ°ì‹± ë¦¬ìŠ¤ë„ˆ êµ¬í˜„
public class RebalanceListener implements ConsumerRebalanceListener {
    private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
    private KafkaConsumer<String, String> consumer;
    
    public RebalanceListener(KafkaConsumer<String, String> consumer) {
        this.consumer = consumer;
    }
    
    @Override
    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
        // íŒŒí‹°ì…˜ í•´ì œ ì „ í˜„ì¬ ì˜¤í”„ì…‹ ì»¤ë°‹
        System.out.println("Partitions revoked: " + partitions);
        consumer.commitSync(currentOffsets);
        currentOffsets.clear();
    }
    
    @Override
    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
        // íŒŒí‹°ì…˜ í• ë‹¹ í›„ ì˜¤í”„ì…‹ ë³µì›
        System.out.println("Partitions assigned: " + partitions);
        for (TopicPartition partition : partitions) {
            consumer.seekToBeginning(Arrays.asList(partition));
        }
    }
    
    public void addOffset(TopicPartition partition, long offset) {
        currentOffsets.put(partition, new OffsetAndMetadata(offset));
    }
}
```

## ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ KStreams

### 1. KStreams ê¸°ë³¸ ì„¤ì •

```java
// KStreams ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
Properties streamsProps = new Properties();
streamsProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-streams-app");
streamsProps.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
streamsProps.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
streamsProps.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

// ìƒíƒœ ì €ì¥ì†Œ ì„¤ì •
streamsProps.put(StreamsConfig.STATE_DIR_CONFIG, "/tmp/kafka-streams");
streamsProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);

// ì„±ëŠ¥ ìµœì í™” ì„¤ì •
streamsProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 4);
streamsProps.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024); // 10MB

StreamsBuilder builder = new StreamsBuilder();
```

### 2. ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì˜ˆì œ

```java
// ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ ë° ì§‘ê³„
public class StreamProcessingExample {
    
    public void setupStreamProcessing() {
        StreamsBuilder builder = new StreamsBuilder();
        
        // ì†ŒìŠ¤ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        KStream<String, String> sourceStream = builder.stream("input-topic");
        
        // ë°ì´í„° ë³€í™˜
        KStream<String, String> transformedStream = sourceStream
            .filter((key, value) -> value != null && !value.isEmpty())
            .mapValues(value -> value.toUpperCase())
            .map((key, value) -> KeyValue.pair(key, "PROCESSED: " + value));
        
        // ìœˆë„ìš° ì§‘ê³„
        KTable<Windowed<String>, Long> windowedCounts = sourceStream
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count();
        
        // ê²°ê³¼ë¥¼ ì¶œë ¥ í† í”½ìœ¼ë¡œ ì „ì†¡
        transformedStream.to("output-topic");
        windowedCounts.toStream().to("aggregated-topic");
        
        // ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        KafkaStreams streams = new KafkaStreams(builder.build(), streamsProps);
        streams.start();
        
        // ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

### 3. ìƒíƒœ ì €ì¥ì†Œ í™œìš©

```java
// ìƒíƒœ ì €ì¥ì†Œë¥¼ í™œìš©í•œ ì§‘ê³„
public class StatefulStreamProcessing {
    
    public void setupStatefulProcessing() {
        StreamsBuilder builder = new StreamsBuilder();
        
        // ìƒíƒœ ì €ì¥ì†Œ ìƒì„±
        StoreBuilder<KeyValueStore<String, Long>> storeBuilder = Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore("user-counts"),
            Serdes.String(),
            Serdes.Long()
        );
        builder.addStateStore(storeBuilder);
        
        // ìƒíƒœ ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•œ ì§‘ê³„
        KStream<String, String> sourceStream = builder.stream("user-events");
        
        sourceStream
            .groupByKey()
            .aggregate(
                () -> 0L,
                (key, value, aggregate) -> aggregate + 1,
                Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("user-counts")
                    .withKeySerde(Serdes.String())
                    .withValueSerde(Serdes.Long())
            )
            .toStream()
            .to("user-counts-topic");
    }
}
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ê³¼ ìš´ì˜ ê´€ë¦¬ {#ëª¨ë‹ˆí„°ë§ê³¼-ìš´ì˜-ê´€ë¦¬}

### 1. JMX ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```java
// JMX ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
public class KafkaMetricsMonitor {
    
    public void setupJMXMonitoring() {
        // JMX ì„¤ì •
        System.setProperty("com.sun.management.jmxremote", "true");
        System.setProperty("com.sun.management.jmxremote.port", "9999");
        System.setProperty("com.sun.management.jmxremote.authenticate", "false");
        System.setProperty("com.sun.management.jmxremote.ssl", "false");
        
        // ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();
        
        // Producer ë©”íŠ¸ë¦­
        ObjectName producerMetrics = new ObjectName("kafka.producer:type=producer-metrics,client-id=my-producer");
        ObjectName consumerMetrics = new ObjectName("kafka.consumer:type=consumer-metrics,client-id=my-consumer");
        
        // ë©”íŠ¸ë¦­ ê°’ ì¡°íšŒ
        Double recordSendRate = (Double) mBeanServer.getAttribute(producerMetrics, "record-send-rate");
        Double recordSendTotal = (Double) mBeanServer.getAttribute(producerMetrics, "record-send-total");
        
        System.out.println("Record Send Rate: " + recordSendRate);
        System.out.println("Record Send Total: " + recordSendTotal);
    }
}
```

### 2. ì¹´í”„ì¹´ ê´€ë¦¬ ë„êµ¬

```bash
#!/bin/bash
# Kafka ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

# í† í”½ ìƒì„±
kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --partitions 3 \
  --replication-factor 2

# í† í”½ ëª©ë¡ ì¡°íšŒ
kafka-topics.sh --list --bootstrap-server localhost:9092

# í† í”½ ìƒì„¸ ì •ë³´
kafka-topics.sh --describe \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒíƒœ í™•ì¸
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-consumer-group --describe

# ì˜¤í”„ì…‹ ë¦¬ì…‹
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-consumer-group --reset-offsets \
  --to-earliest --topic my-topic --execute

# ë©”ì‹œì§€ ì „ì†¡ í…ŒìŠ¤íŠ¸
kafka-console-producer.sh --bootstrap-server localhost:9092 \
  --topic my-topic

# ë©”ì‹œì§€ ìˆ˜ì‹  í…ŒìŠ¤íŠ¸
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic my-topic --from-beginning
```

### 3. ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

```java
// Kafka ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ ì‹œìŠ¤í…œ
public class KafkaAlertSystem {
    
    public void setupAlerting() {
        // ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìŠ¤ë ˆë“œ
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        
        scheduler.scheduleAtFixedRate(() -> {
            try {
                // ì§€ì—° ì‹œê°„ ëª¨ë‹ˆí„°ë§
                double consumerLag = getConsumerLag();
                if (consumerLag > 1000) {
                    sendAlert("High consumer lag detected: " + consumerLag);
                }
                
                // ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§
                double throughput = getThroughput();
                if (throughput < 100) {
                    sendAlert("Low throughput detected: " + throughput);
                }
                
                // ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§
                double errorRate = getErrorRate();
                if (errorRate > 0.01) {
                    sendAlert("High error rate detected: " + errorRate);
                }
                
            } catch (Exception e) {
                System.err.println("Error in monitoring: " + e.getMessage());
            }
        }, 0, 30, TimeUnit.SECONDS);
    }
    
    private void sendAlert(String message) {
        // Slack, ì´ë©”ì¼, SMS ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
        System.out.println("ALERT: " + message);
        // ì‹¤ì œ ì•Œë¦¼ êµ¬í˜„
    }
}
```

## ğŸ›  ï¸ ì‹¤ìŠµ: ì‹¤ë¬´ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ êµ¬ì¶• {#ì‹¤ìŠµ-ì‹¤ë¬´ê¸‰-ìŠ¤íŠ¸ë¦¬ë°-ì‹œìŠ¤í…œ-êµ¬ì¶•}

### 1. í™˜ê²½ ì„¤ì •

```bash
# Docker Composeë¡œ Kafka í´ëŸ¬ìŠ¤í„° êµ¬ì„±
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
```

### 2. ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```java
// ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ
public class LogProcessingSystem {
    
    public static void main(String[] args) {
        // Producer ì„¤ì •
        Properties producerProps = new Properties();
        producerProps.put("bootstrap.servers", "localhost:9092");
        producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        producerProps.put("compression.type", "snappy");
        producerProps.put("batch.size", 16384);
        producerProps.put("linger.ms", 5);
        
        // Consumer ì„¤ì •
        Properties consumerProps = new Properties();
        consumerProps.put("bootstrap.servers", "localhost:9092");
        consumerProps.put("group.id", "log-processor");
        consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        consumerProps.put("auto.offset.reset", "earliest");
        consumerProps.put("enable.auto.commit", false);
        
        // ë¡œê·¸ ìˆ˜ì§‘ ìŠ¤ë ˆë“œ
        Thread logCollector = new Thread(() -> {
            try (KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps)) {
                while (true) {
                    // ë¡œê·¸ íŒŒì¼ì—ì„œ ì½ê¸°
                    String logLine = readLogLine();
                    if (logLine != null) {
                        ProducerRecord<String, String> record = new ProducerRecord<>("raw-logs", logLine);
                        producer.send(record, (metadata, exception) -> {
                            if (exception != null) {
                                System.err.println("Error sending log: " + exception.getMessage());
                            }
                        });
                    }
                    Thread.sleep(100); // 100ms ê°„ê²©
                }
            } catch (Exception e) {
                System.err.println("Error in log collector: " + e.getMessage());
            }
        });
        
        // ë¡œê·¸ ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        Thread logProcessor = new Thread(() -> {
            try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps)) {
                consumer.subscribe(Arrays.asList("raw-logs"));
                
                while (true) {
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                    
                    for (ConsumerRecord<String, String> record : records) {
                        // ë¡œê·¸ íŒŒì‹± ë° ì²˜ë¦¬
                        LogEntry logEntry = parseLog(record.value());
                        if (logEntry != null) {
                            processLogEntry(logEntry);
                        }
                    }
                    
                    // ì˜¤í”„ì…‹ ì»¤ë°‹
                    consumer.commitSync();
                }
            } catch (Exception e) {
                System.err.println("Error in log processor: " + e.getMessage());
            }
        });
        
        // ìŠ¤ë ˆë“œ ì‹œì‘
        logCollector.start();
        logProcessor.start();
        
        // ì¢…ë£Œ ì‹œê·¸ë„ ëŒ€ê¸°
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            logCollector.interrupt();
            logProcessor.interrupt();
        }));
    }
    
    private static String readLogLine() {
        // ì‹¤ì œ ë¡œê·¸ íŒŒì¼ì—ì„œ ì½ê¸° êµ¬í˜„
        return "2025-09-09 10:30:45 INFO User login successful user_id=12345";
    }
    
    private static LogEntry parseLog(String logLine) {
        // ë¡œê·¸ íŒŒì‹± ë¡œì§ êµ¬í˜„
        String[] parts = logLine.split(" ");
        if (parts.length >= 4) {
            return new LogEntry(parts[0] + " " + parts[1], parts[2], parts[3]);
        }
        return null;
    }
    
    private static void processLogEntry(LogEntry logEntry) {
        // ë¡œê·¸ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
        System.out.println("Processing log: " + logEntry);
    }
    
    static class LogEntry {
        String timestamp;
        String level;
        String message;
        
        LogEntry(String timestamp, String level, String message) {
            this.timestamp = timestamp;
            this.level = level;
            this.message = message;
        }
        
        @Override
        public String toString() {
            return String.format("[%s] %s: %s", timestamp, level, message);
        }
    }
}
```

### 3. ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

```java
// ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì˜ˆì œ
public class EventDrivenMicroservice {
    
    public static void main(String[] args) {
        // ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤
        EventProcessor eventProcessor = new EventProcessor();
        eventProcessor.start();
        
        // ì£¼ë¬¸ ì„œë¹„ìŠ¤
        OrderService orderService = new OrderService();
        orderService.start();
        
        // ì¬ê³  ì„œë¹„ìŠ¤
        InventoryService inventoryService = new InventoryService();
        inventoryService.start();
    }
    
    // ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸°
    static class EventProcessor {
        private KafkaConsumer<String, String> consumer;
        
        public void start() {
            Properties props = new Properties();
            props.put("bootstrap.servers", "localhost:9092");
            props.put("group.id", "event-processor");
            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            
            consumer = new KafkaConsumer<>(props);
            consumer.subscribe(Arrays.asList("orders", "inventory", "payments"));
            
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, String> record : records) {
                    processEvent(record.topic(), record.key(), record.value());
                }
            }
        }
        
        private void processEvent(String topic, String key, String value) {
            switch (topic) {
                case "orders":
                    handleOrderEvent(key, value);
                    break;
                case "inventory":
                    handleInventoryEvent(key, value);
                    break;
                case "payments":
                    handlePaymentEvent(key, value);
                    break;
            }
        }
        
        private void handleOrderEvent(String key, String value) {
            System.out.println("Processing order event: " + key + " -> " + value);
            // ì£¼ë¬¸ ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        }
        
        private void handleInventoryEvent(String key, String value) {
            System.out.println("Processing inventory event: " + key + " -> " + value);
            // ì¬ê³  ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        }
        
        private void handlePaymentEvent(String key, String value) {
            System.out.println("Processing payment event: " + key + " -> " + value);
            // ê²°ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        }
    }
}
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Kafka ì•„í‚¤í…ì²˜ ì‹¬í™” ì´í•´**
   - í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„
   - ë©”ì‹œì§€ ì „ë‹¬ ë³´ì¥ ë ˆë²¨
   - íŒŒí‹°ì…”ë‹ê³¼ ë³µì œ ì „ëµ

2. **í”„ë¡œë“€ì„œ ìµœì í™” ì „ëµ**
   - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
   - íŒŒí‹°ì…”ë‹ ì „ëµ
   - ì••ì¶•ê³¼ ì••ì¶•

3. **ì»¨ìŠˆë¨¸ ê·¸ë£¹ê³¼ íŒŒí‹°ì…”ë‹**
   - ì»¨ìŠˆë¨¸ ê·¸ë£¹ ê´€ë¦¬
   - ìˆ˜ë™ ì˜¤í”„ì…‹ ê´€ë¦¬
   - ë¦¬ë°¸ëŸ°ì‹± ì²˜ë¦¬

4. **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ KStreams**
   - KStreams ê¸°ë³¸ ì„¤ì •
   - ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì˜ˆì œ
   - ìƒíƒœ ì €ì¥ì†Œ í™œìš©

5. **ëª¨ë‹ˆí„°ë§ê³¼ ìš´ì˜ ê´€ë¦¬**
   - JMX ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - ì¹´í”„ì¹´ ê´€ë¦¬ ë„êµ¬
   - ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

6. **ì‹¤ë¬´ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ êµ¬ì¶•**
   - ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ
   - ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
   - Docker í™˜ê²½ êµ¬ì„±

### í•µì‹¬ ê°œë… ì •ë¦¬

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| **í”„ë¡œë“€ì„œ ìµœì í™”** | ì„±ëŠ¥ê³¼ ì•ˆì •ì„± í–¥ìƒ | â­â­â­â­â­ |
| **ì»¨ìŠˆë¨¸ ê·¸ë£¹** | ë³‘ë ¬ ì²˜ë¦¬ ë° ë¶€í•˜ ë¶„ì‚° | â­â­â­â­â­ |
| **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬** | ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ | â­â­â­â­â­ |
| **ëª¨ë‹ˆí„°ë§** | ì‹œìŠ¤í…œ ìƒíƒœ ì¶”ì  | â­â­â­â­ |

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

1. **ì„±ëŠ¥ ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬, ì••ì¶•, íŒŒí‹°ì…”ë‹
2. **ì•ˆì •ì„±**: ë³µì œ, ì˜¤í”„ì…‹ ê´€ë¦¬, ì—ëŸ¬ ì²˜ë¦¬
3. **ëª¨ë‹ˆí„°ë§**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ì•Œë¦¼, ë¡œê·¸ ë¶„ì„
4. **í™•ì¥ì„±**: í´ëŸ¬ìŠ¤í„° êµ¬ì„±, ë¶€í•˜ ë¶„ì‚°

---

*ì´ ê°€ì´ë“œë¥¼ í†µí•´ Apache Kafkaì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë§ˆìŠ¤í„°í•˜ê³ , ì‹¤ë¬´ì—ì„œ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!* ğŸš€

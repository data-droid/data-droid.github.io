---
layout: post
title: "Part 1: Time Series Database ê¸°ì´ˆì™€ ì•„í‚¤í…ì²˜ - ì‹œê³„ì—´ ë°ì´í„°ì˜ í•µì‹¬ ì´í•´"
date: 2025-09-28 10:00:00 +0900
category: data-engineering
tags: [Time-Series-Database, TDB, InfluxDB, TimescaleDB, ì‹œê³„ì—´ë°ì´í„°, IoT, ëª¨ë‹ˆí„°ë§, ì‹¤ì‹œê°„ë¶„ì„]
author: Data Droid
lang: ko
series: time-series-database-mastery
series_order: 1
reading_time: "45ë¶„"
difficulty: "ì¤‘ê¸‰"
excerpt: "ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ë¶€í„° ì£¼ìš” TDB ì†”ë£¨ì…˜ ë¹„êµ, ì•„í‚¤í…ì²˜ ì´í•´, ê·¸ë¦¬ê³  ì‹¤ë¬´ í”„ë¡œì íŠ¸ê¹Œì§€ Time Series Databaseì˜ ëª¨ë“  ê¸°ì´ˆë¥¼ ì™„ì „íˆ ì •ë³µí•©ë‹ˆë‹¤."
---

# Part 1: Time Series Database ê¸°ì´ˆì™€ ì•„í‚¤í…ì²˜ - ì‹œê³„ì—´ ë°ì´í„°ì˜ í•µì‹¬ ì´í•´

> ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ë¶€í„° ì£¼ìš” TDB ì†”ë£¨ì…˜ ë¹„êµ, ì•„í‚¤í…ì²˜ ì´í•´, ê·¸ë¦¬ê³  ì‹¤ë¬´ í”„ë¡œì íŠ¸ê¹Œì§€ Time Series Databaseì˜ ëª¨ë“  ê¸°ì´ˆë¥¼ ì™„ì „íˆ ì •ë³µí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [ì‹œê³„ì—´ ë°ì´í„°ì™€ TDB ê°œìš”](#ì‹œê³„ì—´-ë°ì´í„°ì™€-tdb-ê°œìš”)
2. [ì£¼ìš” TDB ì†”ë£¨ì…˜ ë¹„êµ ë¶„ì„](#ì£¼ìš”-tdb-ì†”ë£¨ì…˜-ë¹„êµ-ë¶„ì„)
3. [TDB ì•„í‚¤í…ì²˜ì™€ ì €ì¥ ë°©ì‹](#tdb-ì•„í‚¤í…ì²˜ì™€-ì €ì¥-ë°©ì‹)
4. [TDB ì„±ëŠ¥ íŠ¹ì„±ê³¼ ìµœì í™” ì›ë¦¬](#tdb-ì„±ëŠ¥-íŠ¹ì„±ê³¼-ìµœì í™”-ì›ë¦¬)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: IoT ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-iot-ì„¼ì„œ-ë°ì´í„°-ìˆ˜ì§‘-ì‹œìŠ¤í…œ)
6. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

---

## ğŸ• ì‹œê³„ì—´ ë°ì´í„°ì™€ TDB ê°œìš” {#ì‹œê³„ì—´-ë°ì´í„°ì™€-tdb-ê°œìš”}

### ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±

ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ìˆœì„œëŒ€ë¡œ ê¸°ë¡ëœ ë°ì´í„°ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ê³ ìœ í•œ íŠ¹ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

| íŠ¹ì„± | ì„¤ëª… | ì˜ˆì‹œ | ì˜í–¥ |
|------|------|------|------|
| **ì‹œê°„ ê¸°ë°˜ ì •ë ¬** | ë°ì´í„°ê°€ ì‹œê°„ ìˆœì„œë¡œ ìƒì„±ë¨ | ì„¼ì„œ ë°ì´í„°, ë¡œê·¸, ì§€í‘œ | ìˆœì°¨ì  ì ‘ê·¼ ìµœì í™” ê°€ëŠ¥ |
| **ê³ ë¹ˆë„ ìƒì„±** | ì§§ì€ ê°„ê²©ìœ¼ë¡œ ëŒ€ëŸ‰ ìƒì„± | IoT ì„¼ì„œ (ì´ˆë‹¹ ìˆ˜ì²œ ê°œ), ì›¹ ë¡œê·¸ | ë†’ì€ ì“°ê¸° ì²˜ë¦¬ëŸ‰ í•„ìš” |
| **ë¶ˆë³€ì„±** | í•œë²ˆ ê¸°ë¡ëœ ë°ì´í„°ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŒ | ì„¼ì„œ ì¸¡ì •ê°’, ê±°ë˜ ê¸°ë¡ | ì½ê¸° ì „ìš© ìµœì í™” ê°€ëŠ¥ |
| **ì••ì¶• ê°€ëŠ¥ì„±** | ì—°ì†ëœ ê°’ë“¤ì´ ìœ ì‚¬í•œ íŒ¨í„´ | ì˜¨ë„, ì••ë ¥, CPU ì‚¬ìš©ë¥  | íš¨ìœ¨ì  ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì ìš© |
| **ë³´ì¡´ ì •ì±…** | ì˜¤ë˜ëœ ë°ì´í„°ëŠ” ì‚­ì œ ë˜ëŠ” ì•„ì¹´ì´ë¸Œ | ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ ìœ ì§€ | ìë™í™”ëœ ë°ì´í„° ë¼ì´í”„ì‚¬ì´í´ |

### ì „í†µì  ë°ì´í„°ë² ì´ìŠ¤ì˜ í•œê³„

| ë¬¸ì œì  | ì„¤ëª… | ì‹œê³„ì—´ ë°ì´í„°ì—ì„œì˜ ì˜í–¥ |
|--------|------|-------------------------|
| **ì¸ë±ìŠ¤ ì˜¤ë²„í—¤ë“œ** | ì‹œê°„ë³„ ì¸ë±ìŠ¤ ê´€ë¦¬ ë¹„ìš© | ì´ˆë‹¹ ìˆ˜ë§Œ ê±´ ì“°ê¸° ì‹œ ì„±ëŠ¥ ì €í•˜ |
| **ì €ì¥ ê³µê°„ ë¹„íš¨ìœ¨** | í–‰ ê¸°ë°˜ ì €ì¥ ë°©ì‹ | ì••ì¶•ë¥  ë‚®ìŒ, ì €ì¥ ë¹„ìš© ì¦ê°€ |
| **ì¿¼ë¦¬ ë³µì¡ì„±** | ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬ì˜ ë³µì¡í•œ êµ¬ë¬¸ | ê°œë°œ ìƒì‚°ì„± ì €í•˜ |
| **í™•ì¥ì„± í•œê³„** | ë‹¨ì¼ ë…¸ë“œ ì¤‘ì‹¬ ì„¤ê³„ | ëŒ€ìš©ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ í•œê³„ |

### TDBì˜ í•µì‹¬ ì¥ì 

| íŠ¹ì„± | TDB | ì „í†µì  DB | ê°œì„  íš¨ê³¼ |
|------|-----|-----------|-----------|
| **ì“°ê¸° ìµœì í™”** | ì‹œê°„ ê¸°ë°˜ ìˆœì°¨ ì“°ê¸° | ëœë¤ ì“°ê¸° íŒ¨í„´ | 10-100x ì²˜ë¦¬ëŸ‰ í–¥ìƒ |
| **ì••ì¶• íš¨ìœ¨** | 10:1 ~ 100:1 | 2:1 ~ 3:1 | 5-30x ì €ì¥ ê³µê°„ ì ˆì•½ |
| **ì¿¼ë¦¬ ì„±ëŠ¥** | ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬ ìµœì í™” | ë³µì¡í•œ SQL í•„ìš” | 10-100x ì‘ë‹µ ì†ë„ í–¥ìƒ |
| **ì €ì¥ íš¨ìœ¨** | ì—´ ê¸°ë°˜ ì••ì¶• ì €ì¥ | í–‰ ê¸°ë°˜ ì €ì¥ | ë†’ì€ ì••ì¶•ë¥  ë‹¬ì„± |
| **ë°ì´í„° ìƒëª…ì£¼ê¸°** | ìë™í™”ëœ ë³´ì¡´ ì •ì±… | ìˆ˜ë™ ê´€ë¦¬ í•„ìš” | ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ |

#### ì„±ëŠ¥ ë¹„êµ ë¶„ì„

| ë©”íŠ¸ë¦­ | ì „í†µì  DB | TDB | ê°œì„ ìœ¨ |
|--------|-----------|-----|--------|
| **ì“°ê¸° ì²˜ë¦¬ëŸ‰** | 1K-10K writes/sec | 100K-1M writes/sec | **10-100x** |
| **ì••ì¶•ë¥ ** | 2:1 ~ 3:1 | 10:1 ~ 100:1 | **5-30x** |
| **ì¿¼ë¦¬ ì‘ë‹µì‹œê°„** | ë³µì¡í•œ SQL, ëŠë¦° ì‘ë‹µ | ê°„ë‹¨í•œ ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬, ë¹ ë¥¸ ì‘ë‹µ | **10-100x** |
| **ì €ì¥ ë¹„ìš©** | ë†’ì€ ì €ì¥ ë¹„ìš© | ì••ì¶•ìœ¼ë¡œ ë¹„ìš© ì ˆì•½ | **50-90% ì ˆì•½** |
| **ìš´ì˜ ë³µì¡ë„** | ìˆ˜ë™ ê´€ë¦¬ í•„ìš” | ìë™í™”ëœ ê´€ë¦¬ | **ìš´ì˜ íš¨ìœ¨ì„± ëŒ€í­ í–¥ìƒ** |

---

## ğŸ” ì£¼ìš” TDB ì†”ë£¨ì…˜ ë¹„êµ ë¶„ì„ {#ì£¼ìš”-tdb-ì†”ë£¨ì…˜-ë¹„êµ-ë¶„ì„}

### TDB ì†”ë£¨ì…˜ ë¶„ë¥˜

| ë¶„ë¥˜ | ì†”ë£¨ì…˜ | íŠ¹ì§• | ì‚¬ìš© ì‚¬ë¡€ |
|------|--------|------|-----------|
| **ì „ìš© TDB** | InfluxDB, TimescaleDB | ì‹œê³„ì—´ ì „ìš© ì„¤ê³„ | IoT, ëª¨ë‹ˆí„°ë§, ê¸ˆìœµ ë°ì´í„° |
| **í™•ì¥í˜• DB** | ClickHouse, Apache Druid | ë¶„ì„ DB + ì‹œê³„ì—´ | ëŒ€ìš©ëŸ‰ ë¶„ì„, ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ |
| **ë©”íŠ¸ë¦­ ì¤‘ì‹¬** | Prometheus, VictoriaMetrics | ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì €ì¥ | ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§, ì•Œë¦¼ |
| **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤** | AWS Timestream, Azure Time Series | ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ | í´ë¼ìš°ë“œ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ |

### ìƒì„¸ ì†”ë£¨ì…˜ ë¹„êµ

#### 1. InfluxDB

| í•­ëª© | InfluxDB 1.x | InfluxDB 2.x | íŠ¹ì§• |
|------|--------------|--------------|------|
| **ë¼ì´ì„¼ìŠ¤** | ì˜¤í”ˆì†ŒìŠ¤ + ìƒìš© | ì˜¤í”ˆì†ŒìŠ¤ + ìƒìš© | ìƒìš© ë²„ì „ì€ í´ëŸ¬ìŠ¤í„°ë§ ì§€ì› |
| **ë°ì´í„° ëª¨ë¸** | Line Protocol | Line Protocol | íƒœê·¸ + í•„ë“œ êµ¬ì¡° |
| **ì••ì¶•** | TSM (Time Structured Merge) | TSM | ì—´ ê¸°ë°˜ ì••ì¶• |
| **ì¿¼ë¦¬ ì–¸ì–´** | InfluxQL | Flux | FluxëŠ” ë” ê°•ë ¥í•œ ë°ì´í„° ì²˜ë¦¬ |
| **í´ëŸ¬ìŠ¤í„°ë§** | ìƒìš© ë²„ì „ë§Œ | ìƒìš© ë²„ì „ë§Œ | ìˆ˜í‰ í™•ì¥ ì§€ì› |

```python
class InfluxDBExample:
    def __init__(self):
        self.client = InfluxDBClient(
            host='localhost',
            port=8086,
            username='admin',
            password='password'
        )
    
    def write_sensor_data(self, sensor_data):
        """ì„¼ì„œ ë°ì´í„° ì“°ê¸° ì˜ˆì‹œ"""
        
        # Line Protocol í˜•ì‹
        line_protocol = [
            {
                "measurement": "sensor_data",
                "tags": {
                    "sensor_id": sensor_data['sensor_id'],
                    "location": sensor_data['location'],
                    "type": sensor_data['type']
                },
                "fields": {
                    "temperature": sensor_data['temperature'],
                    "humidity": sensor_data['humidity'],
                    "pressure": sensor_data['pressure']
                },
                "time": sensor_data['timestamp']
            }
        ]
        
        return self.client.write_points(line_protocol)
    
    def query_time_range(self, start_time, end_time):
        """ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬ ì˜ˆì‹œ"""
        
        query = f"""
        SELECT mean(temperature), mean(humidity)
        FROM sensor_data
        WHERE time >= '{start_time}' AND time <= '{end_time}'
        GROUP BY time(1h), location
        """
        
        return self.client.query(query)
```

#### 2. TimescaleDB

| í•­ëª© | TimescaleDB | íŠ¹ì§• |
|------|-------------|------|
| **ê¸°ë°˜** | PostgreSQL í™•ì¥ | SQL í˜¸í™˜ì„± ìœ ì§€ |
| **í•˜ì´í¼í…Œì´ë¸”** | ìë™ íŒŒí‹°ì…”ë‹ | ì‹œê°„ ê¸°ë°˜ ìë™ ë¶„í•  |
| **ì••ì¶•** | ì—´ ê¸°ë°˜ ì••ì¶• | PostgreSQL ì••ì¶• í™œìš© |
| **ì¿¼ë¦¬** | í‘œì¤€ SQL + ì‹œê³„ì—´ í•¨ìˆ˜ | ê¸°ì¡´ SQL ì§€ì‹ í™œìš© ê°€ëŠ¥ |
| **í™•ì¥ì„±** | PostgreSQL í´ëŸ¬ìŠ¤í„°ë§ | ìˆ˜í‰/ìˆ˜ì§ í™•ì¥ |

```sql
-- TimescaleDB ì‚¬ìš© ì˜ˆì‹œ
-- 1. í•˜ì´í¼í…Œì´ë¸” ìƒì„±
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,
    sensor_id INTEGER NOT NULL,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    location TEXT
);

SELECT create_hypertable('sensor_data', 'time');

-- 2. ë°ì´í„° ì‚½ì…
INSERT INTO sensor_data VALUES 
    (NOW(), 1, 25.5, 60.0, 'room1'),
    (NOW(), 2, 26.0, 58.0, 'room2');

-- 3. ì‹œê³„ì—´ ì¿¼ë¦¬
SELECT 
    time_bucket('1 hour', time) AS hour,
    location,
    avg(temperature) as avg_temp,
    max(temperature) as max_temp
FROM sensor_data 
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY hour, location
ORDER BY hour;
```

#### 3. Prometheus

| í•­ëª© | Prometheus | íŠ¹ì§• |
|------|------------|------|
| **ëª©ì ** | ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì €ì¥/ì¿¼ë¦¬ | ëª¨ë‹ˆí„°ë§ ì „ìš© |
| **ë°ì´í„° ëª¨ë¸** | ë©”íŠ¸ë¦­ + ë¼ë²¨ | ì‹œê³„ì—´ + ë©”íƒ€ë°ì´í„° |
| **ì €ì¥** | ë¡œì»¬ SSD ìµœì í™” | ë‹¨ì¼ ë…¸ë“œ ì €ì¥ |
| **ì¿¼ë¦¬** | PromQL | ì‹œê³„ì—´ ì¿¼ë¦¬ ì „ìš© |
| **í™•ì¥ì„±** | Federation, Remote Storage | ë¶„ì‚° ì•„í‚¤í…ì²˜ |

```yaml
# Prometheus ì„¤ì • ì˜ˆì‹œ
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']
    scrape_interval: 5s
    metrics_path: /metrics

  - job_name: 'application-metrics'
    static_configs:
      - targets: ['app:8080']
    scrape_interval: 10s
```

### ì†”ë£¨ì…˜ ì„ íƒ ê°€ì´ë“œ

| ì‚¬ìš© ì‚¬ë¡€ | ì¶”ì²œ ì†”ë£¨ì…˜ | ì´ìœ  |
|-----------|-------------|------|
| **IoT ì„¼ì„œ ë°ì´í„°** | InfluxDB, TimescaleDB | ë†’ì€ ì“°ê¸° ì²˜ë¦¬ëŸ‰, ì••ì¶• íš¨ìœ¨ |
| **ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§** | Prometheus | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìµœì í™”, ì•Œë¦¼ í†µí•© |
| **ê¸ˆìœµ ë°ì´í„°** | TimescaleDB | ACID ë³´ì¥, SQL í˜¸í™˜ì„± |
| **ëŒ€ìš©ëŸ‰ ë¶„ì„** | ClickHouse, Druid | ì—´ ê¸°ë°˜ ì €ì¥, ë¹ ë¥¸ ì§‘ê³„ |
| **í´ë¼ìš°ë“œ ê¸°ë°˜** | AWS Timestream, Azure TS | ê´€ë¦¬í˜• ì„œë¹„ìŠ¤, ìë™ í™•ì¥ |

---

## ğŸ—ï¸ TDB ì•„í‚¤í…ì²˜ì™€ ì €ì¥ ë°©ì‹ {#tdb-ì•„í‚¤í…ì²˜ì™€-ì €ì¥-ë°©ì‹}

### TDB ì•„í‚¤í…ì²˜ íŒ¨í„´

#### 1. ë‹¨ì¼ ë…¸ë“œ ì•„í‚¤í…ì²˜

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | íŠ¹ì§• |
|----------|------|------|
| **Ingestion Layer** | ë°ì´í„° ìˆ˜ì§‘ | HTTP API, ë©”ì‹œì§€ í ì—°ê²° |
| **Storage Engine** | ë°ì´í„° ì €ì¥ | ì••ì¶•, ì¸ë±ì‹±, WAL |
| **Query Engine** | ì¿¼ë¦¬ ì²˜ë¦¬ | ì‹œê°„ ë²”ìœ„ ìµœì í™” |
| **Retention Manager** | ë°ì´í„° ìƒëª…ì£¼ê¸° | ìë™ ì‚­ì œ/ì•„ì¹´ì´ë¸Œ |

#### ë‹¨ì¼ ë…¸ë“œ TDB êµ¬ì„±ìš”ì†Œ

| ê³„ì¸µ | êµ¬ì„±ìš”ì†Œ | ì„¤ëª… |
|------|----------|------|
| **Ingestion** | HTTP API | REST API for data ingestion |
| | Message Queue | Kafka, RabbitMQ integration |
| | Batch Processing | Bulk data import |
| **Storage** | WAL | Write-Ahead Logging |
| | Compression | Columnar compression |
| | Indexing | Time-based indexing |
| **Query** | Optimization | Time range query optimization |
| | Aggregation | Built-in aggregation functions |
| | Caching | Query result caching |
    
#### ë°ì´í„° ì²˜ë¦¬ íë¦„

| ë‹¨ê³„ | ì²˜ë¦¬ ê³¼ì • | ì„¤ëª… | ìµœì í™” ìš”ì†Œ |
|------|-----------|------|-------------|
| **1ë‹¨ê³„** | ë°ì´í„° ìˆ˜ì‹  (Ingestion Layer) | HTTP API, ë©”ì‹œì§€ íë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘ | ë°°ì¹˜ ì²˜ë¦¬, ì—°ê²° í’€ë§ |
| **2ë‹¨ê³„** | í˜•ì‹ ê²€ì¦ ë° ì „ì²˜ë¦¬ | ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬, í˜•ì‹ ë³€í™˜ | ë¹ ë¥¸ ê²€ì¦ ì•Œê³ ë¦¬ì¦˜ |
| **3ë‹¨ê³„** | WALì— ì“°ê¸° ë¡œê·¸ ê¸°ë¡ | Write-Ahead Loggingìœ¼ë¡œ ë‚´êµ¬ì„± ë³´ì¥ | ìˆœì°¨ ì“°ê¸° ìµœì í™” |
| **4ë‹¨ê³„** | ë©”ëª¨ë¦¬ ë²„í¼ì— ì„ì‹œ ì €ì¥ | ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ë©”ëª¨ë¦¬ ìºì‹± | ë²„í¼ í¬ê¸° ìµœì í™” |
| **5ë‹¨ê³„** | ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë””ìŠ¤í¬ì— í”ŒëŸ¬ì‹œ | íš¨ìœ¨ì ì¸ ë””ìŠ¤í¬ I/O | ë°°ì¹˜ í¬ê¸° ì¡°ì • |
| **6ë‹¨ê³„** | ì••ì¶• ë° ì¸ë±ì‹± ìˆ˜í–‰ | ì €ì¥ ê³µê°„ ì ˆì•½ ë° ì¿¼ë¦¬ ìµœì í™” | ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì„ íƒ |
| **7ë‹¨ê³„** | ì¿¼ë¦¬ ì—”ì§„ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥ | ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ | ì¸ë±ìŠ¤ ìµœì í™” |


#### 2. ë¶„ì‚° ì•„í‚¤í…ì²˜

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | íŠ¹ì§• |
|----------|------|------|
| **Load Balancer** | ìš”ì²­ ë¶„ì‚° | ì—¬ëŸ¬ ë…¸ë“œì— ë¶€í•˜ ë¶„ì‚° |
| **Coordinator** | í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ | ë©”íƒ€ë°ì´í„°, ë¼ìš°íŒ… |
| **Storage Nodes** | ë°ì´í„° ì €ì¥ | ìƒ¤ë”©ëœ ë°ì´í„° ì €ì¥ |
| **Query Coordinator** | ë¶„ì‚° ì¿¼ë¦¬ | ì—¬ëŸ¬ ë…¸ë“œ ê²°ê³¼ ë³‘í•© |

### ì €ì¥ ë°©ì‹ ë¹„êµ

#### 1. í–‰ ê¸°ë°˜ vs ì—´ ê¸°ë°˜ ì €ì¥

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|------|-----------|
| **í–‰ ê¸°ë°˜** | ë‹¨ì¼ ë ˆì½”ë“œ ì ‘ê·¼ ë¹ ë¦„ | ì••ì¶•ë¥  ë‚®ìŒ, ì§‘ê³„ ëŠë¦¼ | OLTP, íŠ¸ëœì­ì…˜ |
| **ì—´ ê¸°ë°˜** | ì••ì¶•ë¥  ë†’ìŒ, ì§‘ê³„ ë¹ ë¦„ | ë‹¨ì¼ ë ˆì½”ë“œ ì ‘ê·¼ ëŠë¦¼ | OLAP, ë¶„ì„, ì‹œê³„ì—´ |

#### ì €ì¥ êµ¬ì¡° ë¹„êµ

| ì €ì¥ ë°©ì‹ | ë°ì´í„° êµ¬ì¡° | ì••ì¶•ë¥  | ì¿¼ë¦¬ ì„±ëŠ¥ | íŠ¹ì§• |
|-----------|-------------|--------|-----------|------|
| **í–‰ ê¸°ë°˜** | `[timestamp1, sensor_id1, temp1, humidity1]`<br>`[timestamp2, sensor_id2, temp2, humidity2]`<br>`[timestamp3, sensor_id3, temp3, humidity3]` | 2:1 ~ 5:1 | ë‹¨ì¼ ë ˆì½”ë“œ: ë¹ ë¦„<br>ì§‘ê³„: ëŠë¦¼ | ê° í–‰ì´ ì—°ì† ì €ì¥ |
| **ì—´ ê¸°ë°˜** | `timestamps: [timestamp1, timestamp2, timestamp3]`<br>`sensor_ids: [sensor_id1, sensor_id2, sensor_id3]`<br>`temperatures: [temp1, temp2, temp3]`<br>`humidities: [humidity1, humidity2, humidity3]` | 10:1 ~ 100:1 | ë‹¨ì¼ ë ˆì½”ë“œ: ëŠë¦¼<br>ì§‘ê³„: ë¹ ë¦„ | ê° ì»¬ëŸ¼ì´ ì—°ì† ì €ì¥ |

#### 2. ì••ì¶• ì•Œê³ ë¦¬ì¦˜

| ì•Œê³ ë¦¬ì¦˜ | ì••ì¶•ë¥  | ì†ë„ | ìš©ë„ |
|----------|--------|------|------|
| **LZ4** | ì¤‘ê°„ | ë§¤ìš° ë¹ ë¦„ | ì‹¤ì‹œê°„ ì••ì¶• |
| **ZSTD** | ë†’ìŒ | ë¹ ë¦„ | ê· í˜•ì¡íŒ ì„±ëŠ¥ |
| **GZIP** | ë†’ìŒ | ëŠë¦¼ | ë†’ì€ ì••ì¶•ë¥  í•„ìš” |
| **Delta Compression** | ë§¤ìš° ë†’ìŒ | ë¹ ë¦„ | ì‹œê³„ì—´ ì „ìš© |

#### ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | ì›ë¦¬ | ì˜ˆì‹œ | ì••ì¶•ë¥  | ì†ë„ | ìµœì  ìš©ë„ |
|----------|------|------|--------|------|-----------|
| **Delta Compression** | ì—°ì†ëœ ê°’ì˜ ì°¨ì´ë§Œ ì €ì¥ | ì›ë³¸: [100, 102, 101, 103, 102]<br>ì••ì¶•: [100, +2, -1, +2, -1] | 50:1 ~ 1000:1 | ë¹ ë¦„ | ì‹œê³„ì—´ ë°ì´í„° |
| **LZ4** | ì¤‘ë³µ íŒ¨í„´ ì••ì¶• | ì¼ë°˜ì ì¸ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ | 3:1 ~ 10:1 | ë§¤ìš° ë¹ ë¦„ | ì‹¤ì‹œê°„ ì••ì¶• |
    
#### ì••ì¶• íš¨ìœ¨ì„± ë¶„ì„

| ë°ì´í„° íŠ¹ì„± | ì••ì¶•ë¥  ì˜í–¥ | ì„¤ëª… |
|-------------|-------------|------|
| **ë³€ë™ì„± (Volatility)** | ë‚®ì„ìˆ˜ë¡ ì••ì¶•ë¥  í–¥ìƒ | ì—°ì†ëœ ê°’ì´ ìœ ì‚¬í•˜ë©´ ì••ì¶• íš¨ìœ¨ ì¦ê°€ |
| **íŒ¨í„´ (Pattern)** | ë°˜ë³µ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì••ì¶•ë¥  í–¥ìƒ | ì£¼ê¸°ì  ë˜ëŠ” ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ |
| **ì •ë°€ë„ (Precision)** | ì •ë°€ë„ê°€ ë‚®ì„ìˆ˜ë¡ ì••ì¶•ë¥  í–¥ìƒ | ì†Œìˆ˜ì  ìë¦¿ìˆ˜ê°€ ì ìœ¼ë©´ ì••ì¶• íš¨ìœ¨ ì¦ê°€ |

#### ì¶”ì²œ ì••ì¶• ì•Œê³ ë¦¬ì¦˜

| ë°ì´í„° íŠ¹ì„± | ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ | ì´ìœ  |
|-------------|---------------|------|
| **ë‚®ì€ ë³€ë™ì„±** | Delta Compression | ì—°ì† ê°’ì˜ ì°¨ì´ê°€ ì‘ì•„ ì••ì¶•ë¥  ìµœê³  |
| **ë†’ì€ ë³€ë™ì„±** | LZ4 or ZSTD | ì¼ë°˜ì ì¸ ì••ì¶•ìœ¼ë¡œ ì ì ˆí•œ íš¨ìœ¨ |
| **ì‹¤ì‹œê°„ ì²˜ë¦¬** | LZ4 | ë¹ ë¥¸ ì••ì¶•/í•´ì œ ì†ë„ |
| **ì €ì¥ ìµœì í™”** | ZSTD or GZIP | ë†’ì€ ì••ì¶•ë¥ ë¡œ ì €ì¥ ê³µê°„ ì ˆì•½ |

---

## âš¡ TDB ì„±ëŠ¥ íŠ¹ì„±ê³¼ ìµœì í™” ì›ë¦¬ {#tdb-ì„±ëŠ¥-íŠ¹ì„±ê³¼-ìµœì í™”-ì›ë¦¬}

### ì“°ê¸° ì„±ëŠ¥ ìµœì í™”

#### 1. ë°°ì¹˜ ì“°ê¸° (Batch Writing)

| ë°©ì‹ | ì„¤ëª… | ì„±ëŠ¥ í–¥ìƒ | êµ¬í˜„ ì˜ˆì‹œ |
|------|------|-----------|-----------|
| **ë©”ëª¨ë¦¬ ë²„í¼** | ì„ì‹œ ë©”ëª¨ë¦¬ì— ë°°ì¹˜ ì €ì¥ | 10-100x | WAL + ë©”ëª¨ë¦¬ í |
| **ì••ì¶• ë°°ì¹˜** | ì—¬ëŸ¬ í¬ì¸íŠ¸ ì••ì¶• í›„ ì €ì¥ | 5-20x | ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì ìš© |
| **ì¸ë±ìŠ¤ ì§€ì—°** | ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ | 3-10x | ë°°ì¹˜ ì¸ë±ì‹± |

#### ìµœì í™”ëœ ì“°ê¸° ì²˜ë¦¬ ì „ëµ

| ìµœì í™” ë°©ì‹ | ì„¤ëª… | ì¥ì  | ì„±ëŠ¥ í–¥ìƒ |
|-------------|------|------|-----------|
| **ë©”ëª¨ë¦¬ ë²„í¼ë§** | ë©”ëª¨ë¦¬ ë²„í¼ì— ë°°ì¹˜ ì €ì¥ | ë””ìŠ¤í¬ I/O íšŸìˆ˜ ê°ì†Œ, ì••ì¶• íš¨ìœ¨ì„± í–¥ìƒ, ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ìµœì í™” | 10-100x |
| **ì••ì¶• ë°°ì¹­** | ì—¬ëŸ¬ í¬ì¸íŠ¸ë¥¼ í•¨ê»˜ ì••ì¶• | ì••ì¶•ë¥  í–¥ìƒ, ì••ì¶• ì˜¤ë²„í—¤ë“œ ê°ì†Œ, ì €ì¥ ê³µê°„ ì ˆì•½ | 5-20x |
| **ì¸ë±ìŠ¤ ì§€ì—°** | ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì§€ì—° | ì“°ê¸° ì§€ì—°ì‹œê°„ ê°ì†Œ, ì¸ë±ìŠ¤ ì¡°ê°í™” ë°©ì§€, ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„± | 3-10x |


#### 2. ì••ì¶• ìµœì í™”

#### ì‹œê³„ì—´ íŠ¹í™” ì••ì¶• ì „ëµ

| ì••ì¶• ê¸°ë²• | ì„¤ëª… | ì ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|-----------|------|---------------|
| **Delta Encoding** | ì—°ì† ê°’ì˜ ì°¨ì´ ì €ì¥ | ì²œì²œíˆ ë³€í™”í•˜ëŠ” ì„¼ì„œ ë°ì´í„° |
| **Run Length Encoding** | ì—°ì†ëœ ë™ì¼ ê°’ ì••ì¶• | ìƒìˆ˜ ê°’ì´ ë§ì€ ë°ì´í„° |
| **Dictionary Compression** | ë°˜ë³µë˜ëŠ” ê°’ ì‚¬ì „ ì••ì¶• | ë°˜ë³µ íŒ¨í„´ì´ ìˆëŠ” ë°ì´í„° |
    
#### ì••ì¶• íš¨ê³¼ ë¶„ì„

| ë°ì´í„° íŒ¨í„´ | ìµœì  ì••ì¶• ê¸°ë²• | íš¨ê³¼ |
|-------------|---------------|------|
| **ìƒìˆ˜ ê°’ (Constant Values)** | RLE (Run Length Encoding) | ì—°ì†ëœ ë™ì¼ ê°’ ì••ì¶• |
| **ì„ í˜• íŠ¸ë Œë“œ (Linear Trends)** | Delta Encoding | ì—°ì† ê°’ì˜ ì°¨ì´ ì €ì¥ |
| **ë°˜ë³µ íŒ¨í„´ (Repetitive Patterns)** | Dictionary Compression | ë°˜ë³µë˜ëŠ” ê°’ ì‚¬ì „ ì••ì¶• |
| **ëœë¤ ê°’ (Random Values)** | ì¼ë°˜ ì••ì¶• (LZ4, ZSTD) | ì¼ë°˜ì ì¸ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ |

#### ì‹¤ì œ ë°ì´í„° íƒ€ì…ë³„ ì••ì¶•ë¥ 

| ë°ì´í„° íƒ€ì… | ì••ì¶•ë¥  | íŠ¹ì§• |
|-------------|--------|------|
| **ì˜¨ë„ ì„¼ì„œ** | 20:1 ~ 50:1 | ì²œì²œíˆ ë³€í™”, ë†’ì€ ì••ì¶•ë¥  |
| **CPU ì‚¬ìš©ë¥ ** | 10:1 ~ 30:1 | ì¤‘ê°„ ë³€ë™ì„±, ì ë‹¹í•œ ì••ì¶•ë¥  |
| **ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½** | 5:1 ~ 15:1 | ë†’ì€ ë³€ë™ì„±, ë‚®ì€ ì••ì¶•ë¥  |
| **ì—ëŸ¬ ë¡œê·¸** | 2:1 ~ 5:1 | ëœë¤ì„± ë†’ìŒ, ë‚®ì€ ì••ì¶•ë¥  |


### ì½ê¸° ì„±ëŠ¥ ìµœì í™”

#### 1. ì¸ë±ì‹± ì „ëµ

| ì¸ë±ìŠ¤ íƒ€ì… | ì„¤ëª… | ì„±ëŠ¥ íŠ¹ì„± | ì‚¬ìš© ì‚¬ë¡€ |
|-------------|------|-----------|-----------|
| **ì‹œê°„ ì¸ë±ìŠ¤** | ì‹œê°„ ë²”ìœ„ ê¸°ë°˜ | ì‹œê°„ ì¿¼ë¦¬ ìµœì í™” | ë²”ìœ„ ì¿¼ë¦¬ |
| **íƒœê·¸ ì¸ë±ìŠ¤** | ë©”íƒ€ë°ì´í„° ê¸°ë°˜ | í•„í„°ë§ ìµœì í™” | ë‹¤ì°¨ì› ì¿¼ë¦¬ |
| **ë³µí•© ì¸ë±ìŠ¤** | ì‹œê°„ + íƒœê·¸ | ë³µí•© ì¡°ê±´ ìµœì í™” | ë³µì¡í•œ ì¿¼ë¦¬ |

#### ì¸ë±ìŠ¤ íƒ€ì…ë³„ íŠ¹ì„±

| ì¸ë±ìŠ¤ íƒ€ì… | êµ¬ì¡° | ì¥ì  | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ìœ ì§€ë³´ìˆ˜ |
|-------------|------|------|---------------|----------|
| **Time Index** | B+ Tree on timestamp | ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬ O(log n) | ì¤‘ê°„ | ë‚®ìŒ |
| **Tag Index** | Inverted index on tags | íƒœê·¸ í•„í„°ë§ O(1) | ë†’ìŒ | ë†’ìŒ |
| **Composite Index** | Multi-column index | ë³µí•© ì¡°ê±´ ìµœì í™” | ë§¤ìš° ë†’ìŒ | ë§¤ìš° ë†’ìŒ |
    
#### ì¿¼ë¦¬ íŒ¨í„´ ê¸°ë°˜ ì¸ë±ì‹± ì¶”ì²œ

| ì¿¼ë¦¬ íŒ¨í„´ | ì£¼ìš” ì¸ë±ìŠ¤ | ë³´ì¡° ì¸ë±ìŠ¤ | ìµœì í™” ì „ëµ |
|-----------|-------------|-------------|-------------|
| **ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬** | Time index | Tag index for filtering | íŒŒí‹°ì…”ë‹ + ì‹œê°„ ì¸ë±ìŠ¤ |
| **íƒœê·¸ í•„í„°ë§** | Tag index | Time index for range | íƒœê·¸ ì¹´ë””ë„ë¦¬í‹° ê³ ë ¤ |
| **ì§‘ê³„ ì¿¼ë¦¬** | Time index + pre-aggregation | Materialized views | ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ì§‘ê³„ |


#### 2. ì¿¼ë¦¬ ìµœì í™”

#### ì¿¼ë¦¬ ìµœì í™” ê¸°ë²•

| ìµœì í™” ê¸°ë²• | ì„¤ëª… | íš¨ê³¼ |
|-------------|------|------|
| **Predicate Pushdown** | ì¡°ê±´ì„ ìŠ¤í† ë¦¬ì§€ ë ˆì´ì–´ë¡œ í‘¸ì‹œ | ë¶ˆí•„ìš”í•œ ë°ì´í„° ìŠ¤ìº” ë°©ì§€ |
| **Column Pruning** | í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸° | I/O ìµœì í™” |
| **Time Range Optimization** | ì‹œê°„ ë²”ìœ„ ê¸°ë°˜ íŒŒí‹°ì…˜ í”„ë£¨ë‹ | ê´€ë ¨ íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº” |
| **Parallel Execution** | ì—¬ëŸ¬ íŒŒí‹°ì…˜ ë³‘ë ¬ ì²˜ë¦¬ | ì²˜ë¦¬ ì†ë„ í–¥ìƒ |
    
#### ì¿¼ë¦¬ ìµœì í™” ì „ëµ

| ì¿¼ë¦¬ íƒ€ì… | ìµœì í™” ì „ëµ | ì˜ˆì‹œ ì¿¼ë¦¬ | ì„±ëŠ¥ í–¥ìƒ |
|-----------|-------------|-----------|-----------|
| **ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬** | íŒŒí‹°ì…˜ í”„ë£¨ë‹ + ì¸ë±ìŠ¤ í™œìš© | `SELECT avg(temperature) FROM sensor_data WHERE time >= '2025-01-01' AND time < '2025-01-02'` | **10-100x** |
| **ì§‘ê³„ ì¿¼ë¦¬** | Pre-aggregation + ìºì‹± | `SELECT time_bucket('1h', time), avg(temperature) FROM sensor_data GROUP BY time_bucket('1h', time)` | **5-50x** |

#### ìµœì í™” ê¸°ë²• ì„¤ëª…

| ê¸°ë²• | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| **íŒŒí‹°ì…˜ í”„ë£¨ë‹** | í•´ë‹¹ ë‚ ì§œ íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº” | ë¶ˆí•„ìš”í•œ ë°ì´í„° ìŠ¤ìº” ì œê±° |
| **Pre-aggregation** | ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ë¯¸ë¦¬ ê³„ì‚° | ì‹¤ì‹œê°„ ì§‘ê³„ ì—°ì‚° ìµœì†Œí™” |
| **ìºì‹±** | ìì£¼ ì‚¬ìš©ë˜ëŠ” ê²°ê³¼ ì €ì¥ | ë°˜ë³µ ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• |


---

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: IoT ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ {#ì‹¤ë¬´-í”„ë¡œì íŠ¸-iot-ì„¼ì„œ-ë°ì´í„°-ìˆ˜ì§‘-ì‹œìŠ¤í…œ}

### ì‹¤ìŠµ: InfluxDB ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

#### 1. InfluxDB ì„¤ì¹˜ ë° ì´ˆê¸° ì„¤ì •

```bash
#!/bin/bash
# influxdb-setup.sh

echo "ğŸš€ InfluxDB ì„¤ì¹˜ ë° ì„¤ì • ì‹œì‘..."

# Dockerë¥¼ ì´ìš©í•œ InfluxDB ì„¤ì¹˜
docker run -d \
  --name influxdb \
  -p 8086:8086 \
  -e DOCKER_INFLUXDB_INIT_MODE=setup \
  -e DOCKER_INFLUXDB_INIT_USERNAME=admin \
  -e DOCKER_INFLUXDB_INIT_PASSWORD=admin123 \
  -e DOCKER_INFLUXDB_INIT_ORG=myorg \
  -e DOCKER_INFLUXDB_INIT_BUCKET=mybucket \
  influxdb:2.7-alpine

# ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°
echo "â³ InfluxDB ì‹œì‘ ëŒ€ê¸° ì¤‘..."
sleep 10

# í—¬ìŠ¤ ì²´í¬
echo "ğŸ” InfluxDB ìƒíƒœ í™•ì¸..."
curl -f http://localhost:8086/health

if [ $? -eq 0 ]; then
    echo "âœ… InfluxDBê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
    echo "ğŸŒ ì›¹ UI: http://localhost:8086"
    echo "ğŸ‘¤ ì‚¬ìš©ìëª…: admin"
    echo "ğŸ”‘ ë¹„ë°€ë²ˆí˜¸: admin123"
else
    echo "âŒ InfluxDB ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    exit 1
fi
```

#### 2. Python í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ìš©í•œ ë°ì´í„° ìˆ˜ì§‘

```python
# sensor_data_collector.py
import time
import random
import json
from datetime import datetime, timedelta
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
import logging

class SensorDataCollector:
    def __init__(self):
        # InfluxDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = InfluxDBClient(
            url="http://localhost:8086",
            token="admin-token",  # ì‹¤ì œ í† í°ìœ¼ë¡œ êµì²´ í•„ìš”
            org="myorg"
        )
        
        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
        self.query_api = self.client.query_api()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def generate_sensor_data(self, sensor_id, location, sensor_type):
        """ì„¼ì„œ ë°ì´í„° ìƒì„± ì‹œë®¬ë ˆì´í„°"""
        
        # ì„¼ì„œ íƒ€ì…ë³„ ë°ì´í„° ë²”ìœ„ ì„¤ì •
        ranges = {
            'temperature': (15, 35),    # ì˜¨ë„ (ì„­ì”¨)
            'humidity': (30, 80),       # ìŠµë„ (%)
            'pressure': (1000, 1030),   # ê¸°ì•• (hPa)
            'vibration': (0, 10)        # ì§„ë™ (mm/s)
        }
        
        base_value = random.uniform(*ranges.get(sensor_type, (0, 100)))
        
        # ì‹œê°„ì— ë”°ë¥¸ ë³€í™” ì‹œë®¬ë ˆì´ì…˜ (ì‚¬ì¸íŒŒ)
        time_factor = time.time() / 3600  # ì‹œê°„ ë‹¨ìœ„
        variation = 2 * random.uniform(-1, 1) * abs(base_value * 0.1)
        
        value = base_value + variation
        
        return {
            'measurement': 'sensor_data',
            'tags': {
                'sensor_id': sensor_id,
                'location': location,
                'sensor_type': sensor_type,
                'status': 'active'
            },
            'fields': {
                'value': round(value, 2),
                'quality': random.randint(85, 100),
                'battery_level': random.randint(20, 100),
                'signal_strength': random.randint(-80, -30)
            },
            'timestamp': datetime.utcnow()
        }
    
    def write_sensor_data(self, data_points):
        """ì„¼ì„œ ë°ì´í„°ë¥¼ InfluxDBì— ì €ì¥"""
        try:
            points = []
            for data in data_points:
                point = Point(data['measurement']) \
                    .tag('sensor_id', data['tags']['sensor_id']) \
                    .tag('location', data['tags']['location']) \
                    .tag('sensor_type', data['tags']['sensor_type']) \
                    .tag('status', data['tags']['status']) \
                    .field('value', data['fields']['value']) \
                    .field('quality', data['fields']['quality']) \
                    .field('battery_level', data['fields']['battery_level']) \
                    .field('signal_strength', data['fields']['signal_strength']) \
                    .time(data['timestamp'])
                
                points.append(point)
            
            # ë°°ì¹˜ ì“°ê¸°
            self.write_api.write(bucket="mybucket", record=points)
            
            self.logger.info(f"âœ… {len(points)}ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def query_sensor_data(self, sensor_type=None, location=None, hours=1):
        """ì„¼ì„œ ë°ì´í„° ì¿¼ë¦¬"""
        try:
            # Flux ì¿¼ë¦¬ êµ¬ì„±
            query = f'''
            from(bucket: "mybucket")
            |> range(start: -{hours}h)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            '''
            
            if sensor_type:
                query += f'|> filter(fn: (r) => r.sensor_type == "{sensor_type}")\n'
            
            if location:
                query += f'|> filter(fn: (r) => r.location == "{location}")\n'
            
            query += '|> sort(columns: ["_time"], desc: true)\n'
            query += '|> limit(n: 100)'
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            result = self.query_api.query(query)
            
            # ê²°ê³¼ ë³€í™˜
            data_points = []
            for table in result:
                for record in table.records:
                    data_points.append({
                        'time': record.get_time(),
                        'measurement': record.get_measurement(),
                        'field': record.get_field(),
                        'value': record.get_value(),
                        'sensor_id': record.values.get('sensor_id'),
                        'location': record.values.get('location'),
                        'sensor_type': record.values.get('sensor_type')
                    })
            
            return data_points
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def get_statistics(self, sensor_type, hours=24):
        """ì„¼ì„œ ë°ì´í„° í†µê³„ ì¡°íšŒ"""
        try:
            query = f'''
            from(bucket: "mybucket")
            |> range(start: -{hours}h)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            |> filter(fn: (r) => r.sensor_type == "{sensor_type}")
            |> filter(fn: (r) => r._field == "value")
            |> group(columns: ["location"])
            |> mean()
            |> yield(name: "mean")
            
            from(bucket: "mybucket")
            |> range(start: -{hours}h)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            |> filter(fn: (r) => r.sensor_type == "{sensor_type}")
            |> filter(fn: (r) => r._field == "value")
            |> group(columns: ["location"])
            |> max()
            |> yield(name: "max")
            
            from(bucket: "mybucket")
            |> range(start: -{hours}h)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            |> filter(fn: (r) => r.sensor_type == "{sensor_type}")
            |> filter(fn: (r) => r._field == "value")
            |> group(columns: ["location"])
            |> min()
            |> yield(name: "min")
            '''
            
            result = self.query_api.query(query)
            
            stats = {}
            for table in result:
                table_name = table.name
                for record in table.records:
                    location = record.values.get('location')
                    if location not in stats:
                        stats[location] = {}
                    stats[location][table_name] = record.get_value()
            
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def run_data_collection(self, duration_minutes=10):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        self.logger.info(f"ğŸ”„ {duration_minutes}ë¶„ê°„ ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì„¼ì„œ ì„¤ì •
        sensors = [
            {'id': 'sensor_001', 'location': 'seoul', 'type': 'temperature'},
            {'id': 'sensor_002', 'location': 'seoul', 'type': 'humidity'},
            {'id': 'sensor_003', 'location': 'busan', 'type': 'temperature'},
            {'id': 'sensor_004', 'location': 'busan', 'type': 'pressure'},
            {'id': 'sensor_005', 'location': 'daegu', 'type': 'vibration'},
        ]
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        while time.time() < end_time:
            # ê° ì„¼ì„œì—ì„œ ë°ì´í„° ìƒì„±
            data_points = []
            for sensor in sensors:
                data = self.generate_sensor_data(
                    sensor['id'], 
                    sensor['location'], 
                    sensor['type']
                )
                data_points.append(data)
            
            # ë°ì´í„° ì €ì¥
            self.write_sensor_data(data_points)
            
            # 5ì´ˆ ëŒ€ê¸°
            time.sleep(5)
        
        self.logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # ìˆ˜ì§‘ëœ ë°ì´í„° í†µê³„ ì¶œë ¥
        self.print_collection_summary()
    
    def print_collection_summary(self):
        """ìˆ˜ì§‘ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ìš”ì•½")
        print("=" * 50)
        
        sensor_types = ['temperature', 'humidity', 'pressure', 'vibration']
        
        for sensor_type in sensor_types:
            stats = self.get_statistics(sensor_type, hours=1)
            
            if stats:
                print(f"\nğŸ“ˆ {sensor_type.upper()} ì„¼ì„œ í†µê³„ (ìµœê·¼ 1ì‹œê°„):")
                for location, values in stats.items():
                    mean_val = values.get('mean', 0)
                    max_val = values.get('max', 0)
                    min_val = values.get('min', 0)
                    print(f"   {location}: í‰ê· ={mean_val:.2f}, ìµœëŒ€={max_val:.2f}, ìµœì†Œ={min_val:.2f}")

# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    collector = SensorDataCollector()
    
    # ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (5ë¶„ê°„)
    collector.run_data_collection(duration_minutes=5)
    
    # ìµœê·¼ ë°ì´í„° ì¡°íšŒ
    print("\nğŸ” ìµœê·¼ ì˜¨ë„ ì„¼ì„œ ë°ì´í„°:")
    recent_data = collector.query_sensor_data(sensor_type='temperature', hours=1)
    
    for data in recent_data[:5]:  # ìµœê·¼ 5ê°œë§Œ ì¶œë ¥
        print(f"   {data['time']}: {data['location']} - {data['value']}")
```

#### 3. TimescaleDB ë¹„êµ ì‹¤ìŠµ

```python
# timescale_comparison.py
import psycopg2
import time
from datetime import datetime, timedelta
import random

class TimescaleComparison:
    def __init__(self):
        # TimescaleDB ì—°ê²°
        self.conn = psycopg2.connect(
            host="localhost",
            database="timeseries",
            user="postgres",
            password="postgres"
        )
        self.cur = self.conn.cursor()
        
        self.setup_timescale()
    
    def setup_timescale(self):
        """TimescaleDB ì´ˆê¸° ì„¤ì •"""
        # TimescaleDB í™•ì¥ í™œì„±í™”
        self.cur.execute("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;")
        
        # ì„¼ì„œ ë°ì´í„° í…Œì´ë¸” ìƒì„±
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS sensor_data (
                time TIMESTAMPTZ NOT NULL,
                sensor_id TEXT NOT NULL,
                location TEXT NOT NULL,
                sensor_type TEXT NOT NULL,
                value DOUBLE PRECISION NOT NULL,
                quality INTEGER NOT NULL,
                battery_level INTEGER NOT NULL,
                signal_strength INTEGER NOT NULL
            );
        """)
        
        # í•˜ì´í¼í…Œì´ë¸”ë¡œ ë³€í™˜
        self.cur.execute("""
            SELECT create_hypertable('sensor_data', 'time', 
                                   if_not_exists => TRUE);
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self.cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_sensor_data_sensor_type 
            ON sensor_data (sensor_type, time DESC);
        """)
        
        self.conn.commit()
        print("âœ… TimescaleDB ì„¤ì • ì™„ë£Œ")
    
    def insert_data(self, num_records=1000):
        """ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ“ {num_records}ê°œ ë ˆì½”ë“œ ì‚½ì… í…ŒìŠ¤íŠ¸...")
        
        start_time = time.time()
        
        # ë°°ì¹˜ ì‚½ì…
        insert_query = """
            INSERT INTO sensor_data 
            (time, sensor_id, location, sensor_type, value, quality, battery_level, signal_strength)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
        """
        
        data = []
        for i in range(num_records):
            timestamp = datetime.utcnow() - timedelta(seconds=i)
            data.append((
                timestamp,
                f'sensor_{i % 100:03d}',
                random.choice(['seoul', 'busan', 'daegu']),
                random.choice(['temperature', 'humidity', 'pressure']),
                random.uniform(20, 30),
                random.randint(80, 100),
                random.randint(20, 100),
                random.randint(-80, -30)
            ))
        
        self.cur.executemany(insert_query, data)
        self.conn.commit()
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"âœ… ì‚½ì… ì™„ë£Œ: {duration:.2f}ì´ˆ ({num_records/duration:.0f} records/sec)")
        return duration
    
    def query_performance_test(self):
        """ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        queries = [
            {
                'name': 'ìµœê·¼ 1ì‹œê°„ ë°ì´í„°',
                'query': """
                    SELECT * FROM sensor_data 
                    WHERE time >= NOW() - INTERVAL '1 hour' 
                    ORDER BY time DESC 
                    LIMIT 100;
                """
            },
            {
                'name': 'ì˜¨ë„ ì„¼ì„œ í‰ê· ê°’',
                'query': """
                    SELECT location, AVG(value) as avg_temp
                    FROM sensor_data 
                    WHERE sensor_type = 'temperature' 
                    AND time >= NOW() - INTERVAL '1 hour'
                    GROUP BY location;
                """
            },
            {
                'name': 'ì‹œê°„ ìœˆë„ìš° ì§‘ê³„',
                'query': """
                    SELECT time_bucket('5 minutes', time) as bucket,
                           AVG(value) as avg_value
                    FROM sensor_data 
                    WHERE time >= NOW() - INTERVAL '1 hour'
                    GROUP BY bucket
                    ORDER BY bucket;
                """
            }
        ]
        
        results = {}
        
        for query_info in queries:
            start_time = time.time()
            
            self.cur.execute(query_info['query'])
            rows = self.cur.fetchall()
            
            end_time = time.time()
            duration = end_time - start_time
            
            results[query_info['name']] = {
                'duration': duration,
                'rows': len(rows)
            }
            
            print(f"   {query_info['name']}: {duration:.3f}ì´ˆ ({len(rows)} rows)")
        
        return results
    
    def compression_test(self):
        """ì••ì¶• í…ŒìŠ¤íŠ¸"""
        print("ğŸ—œï¸ ì••ì¶• í…ŒìŠ¤íŠ¸...")
        
        # ì••ì¶• í™œì„±í™”
        self.cur.execute("""
            ALTER TABLE sensor_data SET (
                timescaledb.compress,
                timescaledb.compress_segmentby = 'sensor_type, location'
            );
        """)
        
        # ì••ì¶• ì •ì±… ì„¤ì •
        self.cur.execute("""
            SELECT add_compression_policy('sensor_data', INTERVAL '7 days');
        """)
        
        self.conn.commit()
        
        # ì••ì¶• í†µê³„ ì¡°íšŒ
        self.cur.execute("""
            SELECT 
                schemaname, tablename, 
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
            FROM pg_tables 
            WHERE tablename = 'sensor_data';
        """)
        
        size_info = self.cur.fetchone()
        print(f"   í…Œì´ë¸” í¬ê¸°: {size_info[2]}")
        
        return size_info
    
    def cleanup(self):
        """ì •ë¦¬"""
        self.cur.close()
        self.conn.close()

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ TimescaleDB vs InfluxDB ì„±ëŠ¥ ë¹„êµ")
    print("=" * 50)
    
    # TimescaleDB í…ŒìŠ¤íŠ¸
    tsdb = TimescaleComparison()
    
    # ë°ì´í„° ì‚½ì… ì„±ëŠ¥
    insert_time = tsdb.insert_data(10000)
    
    # ì¿¼ë¦¬ ì„±ëŠ¥
    query_results = tsdb.query_performance_test()
    
    # ì••ì¶• í…ŒìŠ¤íŠ¸
    compression_info = tsdb.compression_test()
    
    tsdb.cleanup()
    
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
    print(f"   TimescaleDB ì‚½ì…: {insert_time:.2f}ì´ˆ")
    print("   ì¿¼ë¦¬ ì„±ëŠ¥:")
    for name, result in query_results.items():
        print(f"     {name}: {result['duration']:.3f}ì´ˆ")
```

#### 4. Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤ìŠµ

```python
# prometheus_metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import requests
import json
from datetime import datetime

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
sensor_data_total = Counter('sensor_data_points_total', 'Total sensor data points collected', ['sensor_type', 'location'])
data_quality_gauge = Gauge('sensor_data_quality', 'Sensor data quality score', ['sensor_type', 'location'])
battery_level_gauge = Gauge('sensor_battery_level', 'Sensor battery level', ['sensor_id'])
collection_duration = Histogram('data_collection_duration_seconds', 'Time spent collecting data')

class PrometheusMetricsCollector:
    def __init__(self, influxdb_url="http://localhost:8086"):
        self.influxdb_url = influxdb_url
        self.headers = {
            'Authorization': 'Token admin-token',
            'Content-Type': 'application/json'
        }
    
    @collection_duration.time()
    def collect_metrics(self):
        """InfluxDBì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ìµœê·¼ 5ë¶„ê°„ì˜ ë°ì´í„° ì¿¼ë¦¬
            query = {
                'query': '''
                from(bucket: "mybucket")
                |> range(start: -5m)
                |> filter(fn: (r) => r._measurement == "sensor_data")
                |> group(columns: ["sensor_type", "location"])
                |> count()
                '''
            }
            
            response = requests.post(
                f"{self.influxdb_url}/api/v2/query",
                headers=self.headers,
                json=query
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                for table in result.get('results', [{}])[0].get('series', []):
                    for row in table.get('values', []):
                        sensor_type = row[0]  # sensor_type íƒœê·¸
                        location = row[1]     # location íƒœê·¸
                        count = row[2]        # ì¹´ìš´íŠ¸ ê°’
                        
                        sensor_data_total.labels(
                            sensor_type=sensor_type,
                            location=location
                        ).inc(count)
                
                # í’ˆì§ˆ ì ìˆ˜ ë©”íŠ¸ë¦­
                self.collect_quality_metrics()
                
                # ë°°í„°ë¦¬ ë ˆë²¨ ë©”íŠ¸ë¦­
                self.collect_battery_metrics()
                
                print(f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now()}")
                
            else:
                print(f"âŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    def collect_quality_metrics(self):
        """í’ˆì§ˆ ì ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        query = {
            'query': '''
            from(bucket: "mybucket")
            |> range(start: -5m)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            |> filter(fn: (r) => r._field == "quality")
            |> group(columns: ["sensor_type", "location"])
            |> mean()
            '''
        }
        
        try:
            response = requests.post(
                f"{self.influxdb_url}/api/v2/query",
                headers=self.headers,
                json=query
            )
            
            if response.status_code == 200:
                result = response.json()
                
                for table in result.get('results', [{}])[0].get('series', []):
                    for row in table.get('values', []):
                        sensor_type = row[0]
                        location = row[1]
                        quality = row[2]
                        
                        data_quality_gauge.labels(
                            sensor_type=sensor_type,
                            location=location
                        ).set(quality)
                        
        except Exception as e:
            print(f"í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    def collect_battery_metrics(self):
        """ë°°í„°ë¦¬ ë ˆë²¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        query = {
            'query': '''
            from(bucket: "mybucket")
            |> range(start: -5m)
            |> filter(fn: (r) => r._measurement == "sensor_data")
            |> filter(fn: (r) => r._field == "battery_level")
            |> group(columns: ["sensor_id"])
            |> last()
            '''
        }
        
        try:
            response = requests.post(
                f"{self.influxdb_url}/api/v2/query",
                headers=self.headers,
                json=query
            )
            
            if response.status_code == 200:
                result = response.json()
                
                for table in result.get('results', [{}])[0].get('series', []):
                    for row in table.get('values', []):
                        sensor_id = row[0]
                        battery_level = row[2]
                        
                        battery_level_gauge.labels(
                            sensor_id=sensor_id
                        ).set(battery_level)
                        
        except Exception as e:
            print(f"ë°°í„°ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    def run_metrics_server(self, port=8000):
        """ë©”íŠ¸ë¦­ ì„œë²„ ì‹¤í–‰"""
        print(f"ğŸŒ Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: http://localhost:{port}/metrics")
        start_http_server(port)
        
        while True:
            self.collect_metrics()
            time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ìˆ˜ì§‘

if __name__ == "__main__":
    collector = PrometheusMetricsCollector()
    collector.run_metrics_server()
```

### í”„ë¡œì íŠ¸ ê°œìš”

ëŒ€ê·œëª¨ IoT ì„¼ì„œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì €ì¥, ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

#### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

| ìš”êµ¬ì‚¬í•­ | ì‚¬ì–‘ | ëª©í‘œ |
|----------|------|------|
| **ì„¼ì„œ ìˆ˜** | 10,000ê°œ ì„¼ì„œ | ì „ ì„¸ê³„ ë¶„ì‚° ë°°ì¹˜ |
| **ë°ì´í„° ìƒì„±ëŸ‰** | 1M í¬ì¸íŠ¸/ì´ˆ | ì‹¤ì‹œê°„ ì²˜ë¦¬ |
| **ë°ì´í„° ë³´ì¡´** | 1ë…„ê°„ ì €ì¥ | ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„ |
| **ì‘ë‹µ ì‹œê°„** | < 100ms | ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ |
| **ê°€ìš©ì„±** | 99.9% | 24/7 ìš´ì˜ |

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

#### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ êµ¬ì„±

| ê³„ì¸µ | êµ¬ì„±ìš”ì†Œ | ê¸°ìˆ  ìŠ¤íƒ | íŠ¹ì§• |
|------|----------|-----------|------|
| **ë°ì´í„° ìˆ˜ì§‘** | MQTT Broker, Message Queue, Data Ingestion | Eclipse Mosquitto, Apache Kafka, InfluxDB | ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥ |
| **ë°ì´í„° ì €ì¥** | Time Series DB, Data Compression, Retention Policy | InfluxDB Cluster, TSM Compression, Automated Cleanup | ì••ì¶•ë¥  50:1 ë‹¬ì„± |
| **ë°ì´í„° ì²˜ë¦¬** | Real-time Analytics, Alert Engine, Data Aggregation | Apache Flink, Custom Alert Rules, Time Window Functions | < 100ms ì‘ë‹µì‹œê°„ |
| **ë°ì´í„° ì‹œê°í™”** | Dashboard, Real-time Charts, Alert Management | Grafana, WebSocket, Push Notifications | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ |
    
#### ë°ì´í„° ëª¨ë¸ ì„¤ê³„

**Measurement**: `sensor_data`

| êµ¬ë¶„ | í•„ë“œëª… | ì„¤ëª… | ë°ì´í„° íƒ€ì… |
|------|--------|------|-------------|
| **Tags** | sensor_id | ê³ ìœ  ì„¼ì„œ ì‹ë³„ì | String |
| | location | ì„¼ì„œ ìœ„ì¹˜ (ê±´ë¬¼, ì¸µ, êµ¬ì—­) | String |
| | sensor_type | ì„¼ì„œ ìœ í˜• (ì˜¨ë„, ìŠµë„, ì••ë ¥) | String |
| | manufacturer | ì œì¡°ì‚¬ | String |
| | firmware_version | íŒì›¨ì–´ ë²„ì „ | String |
| **Fields** | value | ì¸¡ì •ê°’ | Float |
| | quality | ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (0-100) | Integer |
| | battery_level | ë°°í„°ë¦¬ ì”ëŸ‰ (%) | Float |
| | signal_strength | ì‹ í˜¸ ê°•ë„ (dBm) | Float |

#### ë°ì´í„° ë³´ì¡´ ì •ì±…

| ë°ì´í„° íƒ€ì… | ë³´ì¡´ ê¸°ê°„ | ìš©ë„ |
|-------------|-----------|------|
| **ì›ì‹œ ë°ì´í„°** | 30ì¼ | ì‹¤ì‹œê°„ ë¶„ì„, ë””ë²„ê¹… |
| **ì‹œê°„ë³„ ì§‘ê³„** | 1ë…„ | íŠ¸ë Œë“œ ë¶„ì„, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ |
| **ì¼ë³„ ì§‘ê³„** | 5ë…„ | ì¥ê¸° íŠ¸ë Œë“œ, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ |


### ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

#### íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | ê¸°ìˆ  ìŠ¤íƒ |
|----------|------|-----------|
| **MQTT Broker** | ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ | Eclipse Mosquitto |
| **Kafka Producer** | ë©”ì‹œì§€ íì‰ | Apache Kafka |
| **InfluxDB Client** | ì‹œê³„ì—´ ë°ì´í„° ì €ì¥ | InfluxDB |

#### íŒŒì´í”„ë¼ì¸ ì„¤ì •

| êµ¬ì„±ìš”ì†Œ | ì„¤ì • | ê°’ |
|----------|------|-----|
| **MQTT** | Broker Host | mqtt-broker.company.com:1883 |
| | Topics | sensors/+/temperature, sensors/+/humidity, sensors/+/pressure |
| | QoS | 1 |
| **Kafka** | Bootstrap Servers | kafka1:9092, kafka2:9092 |
| | Topic | sensor-data-raw |
| | Partitions | 10 |
| | Replication Factor | 3 |
| **InfluxDB** | Host | influxdb-cluster.company.com:8086 |
| | Database | iot_sensors |
| | Retention Policy | 30_days |
| | Batch Size | 5000 |
| | Flush Interval | 1000ms |

#### ì„¼ì„œ ë°ì´í„° ê²€ì¦ ê·œì¹™

| ê²€ì¦ í•­ëª© | ê·œì¹™ | ì„ê³„ê°’ |
|-----------|------|--------|
| **ì˜¨ë„ ì„¼ì„œ** | ê°’ ë²”ìœ„ | -50Â°C ~ 100Â°C |
| **ìŠµë„ ì„¼ì„œ** | ê°’ ë²”ìœ„ | 0% ~ 100% |
| **ì••ë ¥ ì„¼ì„œ** | ê°’ ë²”ìœ„ | 800hPa ~ 1200hPa |
| **ë°ì´í„° í’ˆì§ˆ** | í’ˆì§ˆ ì ìˆ˜ | â‰¥ 80 |
| **ë°°í„°ë¦¬ ë ˆë²¨** | ë°°í„°ë¦¬ ì”ëŸ‰ | â‰¥ 10% |
| **ì‹ í˜¸ ê°•ë„** | ì‹ í˜¸ ê°•ë„ | â‰¥ -80dBm |


### ì‹¤ì‹œê°„ ë¶„ì„ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

#### ì§‘ê³„ ìœˆë„ìš° ì„¤ì •

| ìœˆë„ìš° í¬ê¸° | ì‹œê°„(ì´ˆ) | ìš©ë„ |
|-------------|----------|------|
| **1ë¶„** | 60 | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ |
| **5ë¶„** | 300 | ë‹¨ê¸° íŠ¸ë Œë“œ ë¶„ì„ |
| **1ì‹œê°„** | 3600 | ì¤‘ê¸° íŒ¨í„´ ë¶„ì„ |
| **1ì¼** | 86400 | ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„ |

#### ì•Œë¦¼ ê·œì¹™ ì„¤ì •

| ê·œì¹™ëª… | ì¡°ê±´ | ì‹¬ê°ë„ | ì•Œë¦¼ ì±„ë„ | ì¿¨ë‹¤ìš´ |
|--------|------|--------|-----------|--------|
| **ì˜¨ë„ ì´ìƒ** | temperature > 35 OR temperature < -10 | CRITICAL | email, sms, slack | 5ë¶„ |
| **ë°°í„°ë¦¬ ë¶€ì¡±** | battery_level < 20 | WARNING | email | 1ì‹œê°„ |
| **ë°ì´í„° í’ˆì§ˆ** | quality < 80 | WARNING | slack | 30ë¶„ |
| **ì„¼ì„œ ì˜¤í”„ë¼ì¸** | no_data_received > 300ì´ˆ | CRITICAL | email, sms | 10ë¶„ |

#### ì‹¤ì‹œê°„ ë¶„ì„ ì²˜ë¦¬ íë¦„

| ì²˜ë¦¬ ë‹¨ê³„ | ì„¤ëª… | ì¶œë ¥ |
|-----------|------|------|
| **ì¦‰ì‹œ ì•Œë¦¼** | ì‹¤ì‹œê°„ ì¡°ê±´ ê²€ì‚¬ | ì•Œë¦¼ ì´ë²¤íŠ¸ |
| **ì§‘ê³„ ë©”íŠ¸ë¦­** | ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ê³„ì‚° | í‰ê· , ìµœëŒ€/ìµœì†Œ, ë¶„ì‚° |
| **íŠ¸ë Œë“œ ë¶„ì„** | íŒ¨í„´ ë° ì´ìƒ íƒì§€ | íŠ¸ë Œë“œ ì§€í‘œ |

#### ì§‘ê³„ ë©”íŠ¸ë¦­ ì¢…ë¥˜

| ë©”íŠ¸ë¦­ | ê³„ì‚° ë°©ë²• | ìœˆë„ìš° í¬ê¸° |
|--------|-----------|-------------|
| **ì´ë™ í‰ê· ** | ì—°ì† ê°’ë“¤ì˜ í‰ê·  | 1ë¶„, 5ë¶„ |
| **ìµœëŒ€/ìµœì†Œ** | ì‹œê°„ ë²”ìœ„ ë‚´ ê·¹ê°’ | 1ì‹œê°„ |
| **ë¶„ì‚°** | ê°’ë“¤ì˜ ë³€ë™ì„± | 5ë¶„ |


### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™”

#### ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •

| ë©”íŠ¸ë¦­ | ì„ê³„ê°’ | ë‹¨ìœ„ | ì„¤ëª… |
|--------|--------|------|------|
| **Ingestion Rate** | 1,000,000 | points/second | ë°ì´í„° ìˆ˜ì§‘ ì²˜ë¦¬ëŸ‰ |
| **Query Response Time** | 0.1 | seconds | ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„ |
| **Storage Utilization** | 0.8 | 80% | ì €ì¥ ê³µê°„ ì‚¬ìš©ë¥  |
| **Memory Usage** | 0.85 | 85% | ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  |
| **CPU Usage** | 0.8 | 80% | CPU ì‚¬ìš©ë¥  |

#### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

| ëª¨ë‹ˆí„°ë§ ì˜ì—­ | ë©”íŠ¸ë¦­ | ì„¤ëª… |
|---------------|--------|------|
| **Ingestion** | Current Rate, Peak Rate, Failed Writes, Queue Depth | ë°ì´í„° ìˆ˜ì§‘ ì„±ëŠ¥ |
| **Query** | Response Time, Throughput, Slow Queries, Cache Hit Rate | ì¿¼ë¦¬ ì„±ëŠ¥ |
| **Storage** | Disk Usage, Compression Ratio, Retention Effectiveness, Index Size | ì €ì¥ íš¨ìœ¨ì„± |
| **Resource** | Memory Usage, CPU Usage, Network I/O, Disk I/O | ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ |

#### ìë™ ìµœì í™” ì „ëµ

| ìµœì í™” íƒ€ì… | ì¡°ê±´ | ì•¡ì…˜ | ì˜ˆìƒ ê°œì„  íš¨ê³¼ |
|-------------|------|------|---------------|
| **ì“°ê¸° ìµœì í™”** | ì²˜ë¦¬ëŸ‰ ì„ê³„ê°’ ì´ˆê³¼ | ë°°ì¹˜ í¬ê¸° ì¦ê°€, ì••ì¶• ê°•í™” | 20-30% ì²˜ë¦¬ëŸ‰ í–¥ìƒ |
| **ì¿¼ë¦¬ ìµœì í™”** | ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ ì´ˆê³¼ | ì¸ë±ìŠ¤ ì¶”ê°€, Pre-aggregation | 50-70% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• |
| **ì €ì¥ ìµœì í™”** | ì €ì¥ ê³µê°„ ì„ê³„ê°’ ì´ˆê³¼ | ë³´ì¡´ ì •ì±… ì¡°ì •, ì••ì¶• ê°•í™” | 30-50% ì €ì¥ ê³µê°„ ì ˆì•½ |


---

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### í•µì‹¬ ê°œë… ì •ë¦¬

1. **ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±**
   - ì‹œê°„ ê¸°ë°˜ ì •ë ¬, ê³ ë¹ˆë„ ìƒì„±, ë¶ˆë³€ì„±
   - ì••ì¶• ê°€ëŠ¥ì„±, ë³´ì¡´ ì •ì±… í•„ìš”ì„±
   - ì „í†µì  ë°ì´í„°ë² ì´ìŠ¤ì˜ í•œê³„ ê·¹ë³µ

2. **ì£¼ìš” TDB ì†”ë£¨ì…˜**
   - **InfluxDB**: ì‹œê³„ì—´ ì „ìš©, ë†’ì€ ì„±ëŠ¥
   - **TimescaleDB**: PostgreSQL ê¸°ë°˜, SQL í˜¸í™˜
   - **Prometheus**: ë©”íŠ¸ë¦­ ì¤‘ì‹¬, ëª¨ë‹ˆí„°ë§ ìµœì í™”
   - **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤**: ê´€ë¦¬í˜• ì†”ë£¨ì…˜

3. **TDB ì•„í‚¤í…ì²˜**
   - ë‹¨ì¼ ë…¸ë“œ vs ë¶„ì‚° ì•„í‚¤í…ì²˜
   - í–‰ ê¸°ë°˜ vs ì—´ ê¸°ë°˜ ì €ì¥
   - ì••ì¶• ì•Œê³ ë¦¬ì¦˜ê³¼ ì¸ë±ì‹± ì „ëµ

4. **ì„±ëŠ¥ ìµœì í™”**
   - ë°°ì¹˜ ì“°ê¸°ì™€ ì••ì¶• ìµœì í™”
   - ì‹œê°„ ê¸°ë°˜ ì¸ë±ì‹±ê³¼ ì¿¼ë¦¬ ìµœì í™”
   - ì‹¤ì‹œê°„ ë¶„ì„ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ

5. **ì‹¤ë¬´ ì ìš©**
   - IoT ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
   - ì‹¤ì‹œê°„ ë¶„ì„ê³¼ ëŒ€ì‹œë³´ë“œ
   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ìë™ ìµœì í™”

### ë‹¤ìŒ ë‹¨ê³„

**Part 2ì—ì„œëŠ” ë‹¤ë£° ë‚´ìš©:**
- TDB ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ìµœì í™” ê¸°ë²•
- ë¶„ì‚° TDB í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- ì••ì¶• ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„° ë³´ì¡´ ì •ì±…
- ëŒ€ê·œëª¨ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹¤ë¬´ í”„ë¡œì íŠ¸

**í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸:**
- âœ… ì‹œê³„ì—´ ë°ì´í„°ì˜ ê³ ìœ í•œ íŠ¹ì„± ì´í•´
- âœ… ì£¼ìš” TDB ì†”ë£¨ì…˜ì˜ ì¥ë‹¨ì  ë¹„êµ
- âœ… TDB ì•„í‚¤í…ì²˜ì™€ ì €ì¥ ë°©ì‹ ì´í•´
- âœ… ì„±ëŠ¥ ìµœì í™” ì›ë¦¬ì™€ ì‹¤ë¬´ ì ìš©
- âœ… IoT ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶•

Time Series Databaseì˜ ê¸°ì´ˆë¥¼ ì™„ì „íˆ ì •ë³µí–ˆìŠµë‹ˆë‹¤! ğŸ‰

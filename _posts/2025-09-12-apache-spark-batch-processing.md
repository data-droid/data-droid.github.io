---
layout: post
lang: ko
title: "Part 2: Apache Spark ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš© - ì‹¤ë¬´ í”„ë¡œì íŠ¸"
description: "Apache Sparkì˜ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë²•, UDF ì‘ì„±, ê·¸ë¦¬ê³  Dockerì™€ Kubernetesë¥¼ í™œìš©í•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤."
date: 2025-09-12
author: Data Droid
category: data-engineering
tags: [Apache-Spark, UDF, ë°°ì¹˜ì²˜ë¦¬, Docker, Kubernetes, Airflow, ì„±ëŠ¥ìµœì í™”, Python]
series: apache-spark-complete-guide
series_order: 2
reading_time: "45ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 2: Apache Spark ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš© - ì‹¤ë¬´ í”„ë¡œì íŠ¸

> Apache Sparkì˜ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë²•, UDF ì‘ì„±, ê·¸ë¦¬ê³  Dockerì™€ Kubernetesë¥¼ í™œìš©í•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [UDF (User Defined Function) ì™„ì „ ì •ë¦¬](#udf-user-defined-function-ì™„ì „-ì •ë¦¬)
2. [ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜](#ê³ ê¸‰-ì§‘ê³„ì™€-ìœˆë„ìš°-í•¨ìˆ˜)
3. [íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™”](#íŒŒí‹°ì…”ë‹-ì „ëµê³¼-ì„±ëŠ¥-ìµœì í™”)
4. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì „ììƒê±°ë˜-ë°ì´í„°-ë¶„ì„)
5. [Dockerì™€ Kubernetes ë°°í¬](#dockerì™€-kubernetes-ë°°í¬)
6. [Airflow ìŠ¤ì¼€ì¤„ë§](#airflow-ìŠ¤ì¼€ì¤„ë§)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ”§ UDF (User Defined Function) ì™„ì „ ì •ë¦¬

### UDFë€?

UDFëŠ” Sparkì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì—¬ ë°ì´í„° ë³€í™˜ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### UDF ì‘ì„± ë°©ë²•

#### **1. ê¸°ë³¸ UDF ì‘ì„±**

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType

# ê°„ë‹¨í•œ ë¬¸ìì—´ ì²˜ë¦¬ UDF
@udf(returnType=StringType())
def clean_text(text):
    if text is None:
        return None
    return text.strip().lower().replace("  ", " ")

# ìˆ˜í•™ ê³„ì‚° UDF
@udf(returnType=FloatType())
def calculate_discount(price, discount_rate):
    if price is None or discount_rate is None:
        return None
    return float(price * (1 - discount_rate))

# ì¡°ê±´ë¶€ ì²˜ë¦¬ UDF
@udf(returnType=StringType())
def categorize_price(price):
    if price is None:
        return "Unknown"
    elif price < 100:
        return "Low"
    elif price < 500:
        return "Medium"
    else:
        return "High"

# ì‚¬ìš© ì˜ˆì œ
df = spark.createDataFrame([
    ("  Product A  ", 150.0, 0.1),
    ("Product B", 75.0, 0.2),
    (None, 600.0, 0.15)
], ["product_name", "price", "discount_rate"])

df.withColumn("clean_name", clean_text("product_name")) \
  .withColumn("final_price", calculate_discount("price", "discount_rate")) \
  .withColumn("price_category", categorize_price("price")) \
  .show()
```

#### **2. ë³µì¡í•œ UDF - JSON íŒŒì‹±**

```python
from pyspark.sql.types import MapType, StringType
import json

@udf(returnType=MapType(StringType(), StringType()))
def parse_json_metadata(json_str):
    try:
        if json_str is None:
            return {}
        data = json.loads(json_str)
        # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return {str(k): str(v) for k, v in data.items()}
    except:
        return {}

# ì‚¬ìš© ì˜ˆì œ
json_data = spark.createDataFrame([
    ('{"category": "electronics", "brand": "Samsung", "rating": 4.5}'),
    ('{"category": "clothing", "brand": "Nike", "rating": 4.2}'),
    ('invalid json')
], ["metadata"])

json_data.withColumn("parsed_metadata", parse_json_metadata("metadata")).show(truncate=False)
```

#### **3. ê³ ê¸‰ UDF - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©**

```python
from pyspark.sql.types import ArrayType, FloatType
import numpy as np
from sklearn.ensemble import IsolationForest

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
model = None

def initialize_model():
    global model
    # ê°„ë‹¨í•œ ì´ìƒ íƒì§€ ëª¨ë¸ ì´ˆê¸°í™”
    model = IsolationForest(contamination=0.1, random_state=42)
    # ë”ë¯¸ ë°ì´í„°ë¡œ í›ˆë ¨
    dummy_data = np.random.randn(1000, 3)
    model.fit(dummy_data)

@udf(returnType=FloatType())
def anomaly_score(features_array):
    global model
    if model is None:
        initialize_model()
    
    if features_array is None or len(features_array) != 3:
        return 0.0
    
    try:
        features = np.array(features_array).reshape(1, -1)
        score = model.decision_function(features)[0]
        return float(score)
    except:
        return 0.0

# ì‚¬ìš© ì˜ˆì œ
features_data = spark.createDataFrame([
    ([1.2, 3.4, 2.1],),
    ([10.5, 15.2, 8.9],),
    ([0.1, 0.3, 0.2],)
], ["features"])

features_data.withColumn("anomaly_score", anomaly_score("features")).show()
```

### UDF ìµœì í™” íŒ

#### **1. Vectorized UDF ì‚¬ìš©**

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import FloatType

# Pandas UDFëŠ” ë” ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µ
@pandas_udf(returnType=FloatType())
def fast_calculation(series: pd.Series) -> pd.Series:
    # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    return series * 2 + 1

# ì‚¬ìš©
df.withColumn("fast_result", fast_calculation("value")).show()
```

#### **2. UDF ìºì‹±ê³¼ ì¬ì‚¬ìš©**

```python
# UDFë¥¼ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ì¬ì‚¬ìš©
def create_text_processor():
    @udf(returnType=StringType())
    def process_text(text):
        return text.upper() if text else None
    return process_text

# ì—¬ëŸ¬ DataFrameì—ì„œ ì¬ì‚¬ìš©
text_processor = create_text_processor()
df1.withColumn("processed", text_processor("text1")).show()
df2.withColumn("processed", text_processor("text2")).show()
```

## ğŸ“Š ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜

### ê³ ê¸‰ ìœˆë„ìš° í•¨ìˆ˜

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import (
    row_number, rank, dense_rank, lag, lead, 
    first_value, last_value, nth_value,
    cume_dist, percent_rank, ntile
)

# ë³µì¡í•œ ìœˆë„ìš° ìŠ¤í™ ì •ì˜
window_spec = Window.partitionBy("category").orderBy("sales_amount")

# ë‹¤ì–‘í•œ ìœˆë„ìš° í•¨ìˆ˜ ì ìš©
df_advanced = df.withColumn("row_num", row_number().over(window_spec)) \
    .withColumn("rank", rank().over(window_spec)) \
    .withColumn("dense_rank", dense_rank().over(window_spec)) \
    .withColumn("prev_sales", lag("sales_amount", 1).over(window_spec)) \
    .withColumn("next_sales", lead("sales_amount", 1).over(window_spec)) \
    .withColumn("first_sales", first_value("sales_amount").over(window_spec)) \
    .withColumn("last_sales", last_value("sales_amount").over(window_spec)) \
    .withColumn("cumulative_dist", cume_dist().over(window_spec)) \
    .withColumn("percentile_rank", percent_rank().over(window_spec)) \
    .withColumn("quartile", ntile(4).over(window_spec))
```

### ê³ ê¸‰ ì§‘ê³„ í•¨ìˆ˜

```python
from pyspark.sql.functions import (
    collect_list, collect_set, array_agg,
    stddev, variance, skewness, kurtosis,
    corr, covar_pop, covar_samp
)

# í†µê³„ì  ì§‘ê³„
stats_df = df.groupBy("category") \
    .agg(
        count("*").alias("count"),
        avg("price").alias("avg_price"),
        stddev("price").alias("stddev_price"),
        variance("price").alias("variance_price"),
        skewness("price").alias("skewness_price"),
        kurtosis("price").alias("kurtosis_price"),
        collect_list("product_name").alias("all_products"),
        collect_set("brand").alias("unique_brands")
    )
```

## âš¡ íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™”

### íŒŒí‹°ì…”ë‹ ì „ëµ

```python
# 1. ì»¬ëŸ¼ ê¸°ë°˜ íŒŒí‹°ì…”ë‹
df.write.mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("path/to/partitioned_data")

# 2. ë²„í‚·íŒ…
df.write.mode("overwrite") \
    .bucketBy(10, "user_id") \
    .sortBy("timestamp") \
    .saveAsTable("bucketed_table")

# 3. ë™ì  íŒŒí‹°ì…”ë‹
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
```

### ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```python
# ë©”ëª¨ë¦¬ ìµœì í™”
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# ìºì‹± ì „ëµ
df.cache()  # ë©”ëª¨ë¦¬ ìºì‹±
df.persist(StorageLevel.MEMORY_AND_DISK)  # ë©”ëª¨ë¦¬+ë””ìŠ¤í¬ ìºì‹±

# ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")
```

## ğŸ›’ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ecommerce-analysis/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ analytics.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ spark_config.py
â”‚   â””â”€â”€ app_config.yaml
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_processing.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ spark-job.yaml
â”‚   â””â”€â”€ airflow-dag.py
â””â”€â”€ README.md
```

### 1. ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ

```python
# src/data_processing.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import logging

class EcommerceDataProcessor:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
        
    def load_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ"""
        self.logger.info(f"Loading data from {data_path}")
        
        # ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ
        orders_df = self.spark.read.parquet(f"{data_path}/orders")
        products_df = self.spark.read.parquet(f"{data_path}/products")
        customers_df = self.spark.read.parquet(f"{data_path}/customers")
        
        return orders_df, products_df, customers_df
    
    def clean_data(self, orders_df, products_df, customers_df):
        """ë°ì´í„° ì •ì œ"""
        self.logger.info("Cleaning data...")
        
        # ì¤‘ë³µ ì œê±°
        orders_clean = orders_df.dropDuplicates(["order_id"])
        products_clean = products_df.dropDuplicates(["product_id"])
        customers_clean = customers_df.dropDuplicates(["customer_id"])
        
        # NULL ê°’ ì²˜ë¦¬
        orders_clean = orders_clean.fillna({"quantity": 1, "discount": 0})
        products_clean = products_clean.fillna({"price": 0, "category": "Unknown"})
        
        return orders_clean, products_clean, customers_clean
    
    def enrich_data(self, orders_df, products_df, customers_df):
        """ë°ì´í„° í’ë¶€í™”"""
        self.logger.info("Enriching data...")
        
        # ì¡°ì¸ì„ í†µí•œ ë°ì´í„° í’ë¶€í™”
        enriched_df = orders_df \
            .join(products_df, "product_id", "left") \
            .join(customers_df, "customer_id", "left")
        
        # ê³„ì‚°ëœ ì»¬ëŸ¼ ì¶”ê°€
        enriched_df = enriched_df.withColumn(
            "total_amount", 
            col("quantity") * col("price") * (1 - col("discount"))
        )
        
        # ê³ ê° ë“±ê¸‰ ê³„ì‚° UDF
        @udf(returnType=StringType())
        def calculate_customer_tier(total_spent):
            if total_spent >= 10000:
                return "VIP"
            elif total_spent >= 5000:
                return "Gold"
            elif total_spent >= 1000:
                return "Silver"
            else:
                return "Bronze"
        
        # ê³ ê°ë³„ ì´ êµ¬ë§¤ì•¡ ê³„ì‚°
        customer_totals = enriched_df.groupBy("customer_id") \
            .agg(sum("total_amount").alias("total_spent"))
        
        enriched_df = enriched_df.join(customer_totals, "customer_id", "left") \
            .withColumn("customer_tier", calculate_customer_tier("total_spent"))
        
        return enriched_df
```

### 2. ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆ

```python
# src/analytics.py
from pyspark.sql.functions import *
from pyspark.sql.window import Window
import pandas as pd

class EcommerceAnalytics:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def sales_analysis(self, df):
        """ë§¤ì¶œ ë¶„ì„"""
        # ì¼ë³„ ë§¤ì¶œ íŠ¸ë Œë“œ
        daily_sales = df.groupBy("order_date") \
            .agg(
                sum("total_amount").alias("daily_revenue"),
                countDistinct("customer_id").alias("daily_customers"),
                avg("total_amount").alias("avg_order_value")
            ) \
            .orderBy("order_date")
        
        # ì œí’ˆë³„ ë§¤ì¶œ ë¶„ì„
        product_sales = df.groupBy("product_id", "product_name", "category") \
            .agg(
                sum("quantity").alias("total_quantity"),
                sum("total_amount").alias("total_revenue"),
                countDistinct("customer_id").alias("unique_customers")
            ) \
            .orderBy(desc("total_revenue"))
        
        return daily_sales, product_sales
    
    def customer_analysis(self, df):
        """ê³ ê° ë¶„ì„"""
        # ê³ ê°ë³„ êµ¬ë§¤ íŒ¨í„´
        customer_patterns = df.groupBy("customer_id", "customer_tier") \
            .agg(
                count("*").alias("total_orders"),
                sum("total_amount").alias("total_spent"),
                avg("total_amount").alias("avg_order_value"),
                min("order_date").alias("first_purchase"),
                max("order_date").alias("last_purchase")
            )
        
        # RFM ë¶„ì„ (Recency, Frequency, Monetary)
        rfm_analysis = df.groupBy("customer_id") \
            .agg(
                datediff(current_date(), max("order_date")).alias("recency"),
                count("*").alias("frequency"),
                sum("total_amount").alias("monetary")
            )
        
        return customer_patterns, rfm_analysis
    
    def advanced_analytics(self, df):
        """ê³ ê¸‰ ë¶„ì„"""
        # ì½”í˜¸íŠ¸ ë¶„ì„
        cohort_analysis = df.select("customer_id", "order_date") \
            .withColumn("cohort_month", date_format("order_date", "yyyy-MM")) \
            .groupBy("cohort_month") \
            .agg(countDistinct("customer_id").alias("cohort_size"))
        
        # ìƒí’ˆ ì¶”ì²œì„ ìœ„í•œ í˜‘ì—… í•„í„°ë§ ê¸°ì´ˆ
        window_spec = Window.partitionBy("customer_id").orderBy(desc("order_date"))
        
        customer_product_matrix = df.select("customer_id", "product_id", "total_amount") \
            .withColumn("rank", row_number().over(window_spec)) \
            .filter(col("rank") <= 10)  # ìµœê·¼ 10ê°œ êµ¬ë§¤ ìƒí’ˆ
        
        return cohort_analysis, customer_product_matrix
```

### 3. ì„¤ì • íŒŒì¼

```python
# config/spark_config.py
def get_spark_config():
    return {
        "spark.app.name": "EcommerceAnalysis",
        "spark.master": "local[*]",
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.sql.adaptive.advisoryPartitionSizeInBytes": "128MB",
        "spark.sql.adaptive.skewJoin.enabled": "true",
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
        "spark.sql.execution.arrow.pyspark.enabled": "true",
        "spark.sql.adaptive.skewJoin.skewedPartitionFactor": "5",
        "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes": "256MB"
    }
```

```yaml
# config/app_config.yaml
data:
  input_path: "/data/input"
  output_path: "/data/output"
  checkpoint_path: "/data/checkpoint"

processing:
  batch_size: 10000
  parallelism: 4
  cache_enabled: true

output:
  format: "parquet"
  compression: "snappy"
  partition_by: ["year", "month"]
```

## ğŸ³ Dockerì™€ Kubernetes ë°°í¬

### 1. Dockerfile

```dockerfile
# Dockerfile
FROM openjdk:8-jdk-alpine

# Python ì„¤ì¹˜
RUN apk add --no-cache python3 py3-pip

# Spark ì„¤ì¹˜
ENV SPARK_VERSION=3.4.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt /app/
WORKDIR /app
RUN pip3 install -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY src/ /app/src/
COPY config/ /app/config/

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
RUN chmod +x /app/src/main.py

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 4040 8080

# ì‹¤í–‰ ëª…ë ¹
CMD ["python3", "/app/src/main.py"]
```

### 2. Kubernetes Job ì„¤ì •

```yaml
# kubernetes/spark-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ecommerce-analysis-job
  namespace: data-engineering
spec:
  template:
    spec:
      containers:
      - name: spark-driver
        image: ecommerce-analysis:latest
        command: ["python3", "/app/src/main.py"]
        env:
        - name: SPARK_MASTER
          value: "k8s://https://kubernetes.default.svc.cluster.local:443"
        - name: SPARK_APP_NAME
          value: "ecommerce-analysis"
        - name: SPARK_DRIVER_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SPARK_DRIVER_PORT
          value: "7077"
        - name: SPARK_UI_PORT
          value: "4040"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        volumeMounts:
        - name: data-volume
          mountPath: /data
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: data-pvc
      - name: config-volume
        configMap:
          name: app-config
      restartPolicy: Never
  backoffLimit: 3
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
  namespace: data-engineering
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: data-engineering
data:
  app_config.yaml: |
    data:
      input_path: "/data/input"
      output_path: "/data/output"
    processing:
      batch_size: 10000
      parallelism: 4
```

### 3. Spark Application ì‹¤í–‰

```python
# src/main.py
import os
import sys
import logging
from pyspark.sql import SparkSession
from data_processing import EcommerceDataProcessor
from analytics import EcommerceAnalytics
from config.spark_config import get_spark_config

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def main():
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        # Spark ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .config(conf=get_spark_config()) \
            .getOrCreate()
        
        logger.info("Spark session created successfully")
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = EcommerceDataProcessor(spark)
        analytics = EcommerceAnalytics(spark)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        input_path = os.getenv("INPUT_PATH", "/data/input")
        output_path = os.getenv("OUTPUT_PATH", "/data/output")
        
        # ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        logger.info("Loading data...")
        orders_df, products_df, customers_df = processor.load_data(input_path)
        
        logger.info("Cleaning data...")
        orders_clean, products_clean, customers_clean = processor.clean_data(
            orders_df, products_df, customers_df
        )
        
        logger.info("Enriching data...")
        enriched_df = processor.enrich_data(
            orders_clean, products_clean, customers_clean
        )
        
        # ë¶„ì„ ìˆ˜í–‰
        logger.info("Performing sales analysis...")
        daily_sales, product_sales = analytics.sales_analysis(enriched_df)
        
        logger.info("Performing customer analysis...")
        customer_patterns, rfm_analysis = analytics.customer_analysis(enriched_df)
        
        logger.info("Performing advanced analytics...")
        cohort_analysis, customer_product_matrix = analytics.advanced_analytics(enriched_df)
        
        # ê²°ê³¼ ì €ì¥
        logger.info("Saving results...")
        enriched_df.write.mode("overwrite").parquet(f"{output_path}/enriched_data")
        daily_sales.write.mode("overwrite").parquet(f"{output_path}/daily_sales")
        product_sales.write.mode("overwrite").parquet(f"{output_path}/product_sales")
        customer_patterns.write.mode("overwrite").parquet(f"{output_path}/customer_patterns")
        rfm_analysis.write.mode("overwrite").parquet(f"{output_path}/rfm_analysis")
        
        logger.info("Job completed successfully!")
        
    except Exception as e:
        logger.error(f"Job failed with error: {str(e)}")
        sys.exit(1)
    
    finally:
        if 'spark' in locals():
            spark.stop()

if __name__ == "__main__":
    main()
```

## ğŸ”„ Airflow ìŠ¤ì¼€ì¤„ë§

### Airflow DAG

```python
# kubernetes/airflow-dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.kubernetes import KubernetesPodOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator

# ê¸°ë³¸ ì¸ìˆ˜
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
dag = DAG(
    'ecommerce_analysis_pipeline',
    default_args=default_args,
    description='ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
    catchup=False,
    tags=['ecommerce', 'spark', 'analytics'],
)

# ì‹œì‘ ì‘ì—…
start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag,
)

# ë°ì´í„° ê²€ì¦ ì‘ì—…
def validate_input_data():
    import os
    input_path = "/data/input"
    required_files = ["orders", "products", "customers"]
    
    for file_name in required_files:
        file_path = f"{input_path}/{file_name}"
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Required file not found: {file_path}")
    
    print("Input data validation completed successfully")

validate_task = PythonOperator(
    task_id='validate_input_data',
    python_callable=validate_input_data,
    dag=dag,
)

# Spark ë¶„ì„ ì‘ì—…
spark_analysis_task = KubernetesPodOperator(
    task_id='spark_ecommerce_analysis',
    namespace='data-engineering',
    image='ecommerce-analysis:latest',
    name='spark-analysis-pod',
    cmds=['python3', '/app/src/main.py'],
    env_vars={
        'INPUT_PATH': '/data/input',
        'OUTPUT_PATH': '/data/output',
        'SPARK_MASTER': 'k8s://https://kubernetes.default.svc.cluster.local:443',
        'SPARK_APP_NAME': 'ecommerce-analysis',
    },
    resources={
        'request_memory': '2Gi',
        'request_cpu': '1',
        'limit_memory': '4Gi',
        'limit_cpu': '2',
    },
    volumes=[
        {
            'name': 'data-volume',
            'persistentVolumeClaim': {
                'claimName': 'data-pvc'
            }
        }
    ],
    volume_mounts=[
        {
            'name': 'data-volume',
            'mountPath': '/data'
        }
    ],
    get_logs=True,
    is_delete_operator_pod=True,
    dag=dag,
)

# ê²°ê³¼ ê²€ì¦ ì‘ì—…
def validate_output_data():
    import os
    output_path = "/data/output"
    required_outputs = [
        "enriched_data",
        "daily_sales", 
        "product_sales",
        "customer_patterns",
        "rfm_analysis"
    ]
    
    for output_name in required_outputs:
        output_dir = f"{output_path}/{output_name}"
        if not os.path.exists(output_dir):
            raise FileNotFoundError(f"Output directory not found: {output_dir}")
        
        # íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        files = os.listdir(output_dir)
        if not files:
            raise ValueError(f"Output directory is empty: {output_dir}")
    
    print("Output data validation completed successfully")

validate_output_task = PythonOperator(
    task_id='validate_output_data',
    python_callable=validate_output_data,
    dag=dag,
)

# ì•Œë¦¼ ì‘ì—…
def send_completion_notification():
    print("Ecommerce analysis pipeline completed successfully!")
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Email ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
    # slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
    # send_slack_notification(slack_webhook_url, "Pipeline completed successfully")

notification_task = PythonOperator(
    task_id='send_completion_notification',
    python_callable=send_completion_notification,
    dag=dag,
)

# ì¢…ë£Œ ì‘ì—…
end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag,
)

# ì‘ì—… ì˜ì¡´ì„± ì •ì˜
start_task >> validate_task >> spark_analysis_task >> validate_output_task >> notification_task >> end_task
```

### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# deploy.sh

echo "Building Docker image..."
docker build -t ecommerce-analysis:latest .

echo "Loading image to Kubernetes cluster..."
kind load docker-image ecommerce-analysis:latest

echo "Applying Kubernetes manifests..."
kubectl apply -f kubernetes/spark-job.yaml

echo "Deploying Airflow DAG..."
kubectl cp kubernetes/airflow-dag.py airflow-web-0:/opt/airflow/dags/

echo "Deployment completed!"
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **UDF (User Defined Function)**
   - ê¸°ë³¸ UDF ì‘ì„±ê³¼ í™œìš©
   - ë³µì¡í•œ UDF (JSON íŒŒì‹±, ML ëª¨ë¸ ì ìš©)
   - UDF ìµœì í™” ê¸°ë²•

2. **ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜**
   - ë‹¤ì–‘í•œ ìœˆë„ìš° í•¨ìˆ˜ í™œìš©
   - í†µê³„ì  ì§‘ê³„ í•¨ìˆ˜
   - RFM ë¶„ì„ ë“± ê³ ê¸‰ ë¶„ì„

3. **ì„±ëŠ¥ ìµœì í™”**
   - íŒŒí‹°ì…”ë‹ ì „ëµ
   - ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
   - ìºì‹± ì „ëµ

4. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
   - ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì¡°
   - ì—ëŸ¬ ì²˜ë¦¬ì™€ ë¡œê¹…

5. **í”„ë¡œë•ì…˜ ë°°í¬**
   - Docker ì»¨í…Œì´ë„ˆí™”
   - Kubernetes Job ë°°í¬
   - Airflow ìŠ¤ì¼€ì¤„ë§

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **UDF** | ì»¤ìŠ¤í…€ ë°ì´í„° ë³€í™˜ | â­â­â­â­â­ |
| **ìœˆë„ìš° í•¨ìˆ˜** | ê³ ê¸‰ ë¶„ì„ | â­â­â­â­â­ |
| **Docker** | ì»¨í…Œì´ë„ˆí™” | â­â­â­â­ |
| **Kubernetes** | ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | â­â­â­â­ |
| **Airflow** | ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 3: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- Spark Streamingê³¼ Structured Streaming
- Kafka ì—°ë™ê³¼ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
- ì›Œí„°ë§ˆí‚¹ê³¼ ì§€ì—° ë°ì´í„° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 3: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ Kafka ì—°ë™](/data-engineering/2025/09/13/apache-spark-streaming.html)

---

*ì´ì œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤.* ğŸš€

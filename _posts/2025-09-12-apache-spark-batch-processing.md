---
layout: post
lang: ko
title: "Part 2: Apache Spark ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš© - ì‹¤ë¬´ í”„ë¡œì íŠ¸"
description: "Apache Sparkì˜ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë²•, UDF ì‘ì„±, ê·¸ë¦¬ê³  Dockerì™€ Kubernetesë¥¼ í™œìš©í•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤."
date: 2025-09-12
author: Data Droid
category: data-engineering
tags: [Apache-Spark, UDF, ë°°ì¹˜ì²˜ë¦¬, Docker, Kubernetes, Airflow, ì„±ëŠ¥ìµœì í™”, Python]
series: apache-spark-complete-guide
series_order: 2
reading_time: "45ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 2: Apache Spark ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš© - ì‹¤ë¬´ í”„ë¡œì íŠ¸

> Apache Sparkì˜ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë²•, UDF ì‘ì„±, ê·¸ë¦¬ê³  Dockerì™€ Kubernetesë¥¼ í™œìš©í•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [UDF (User Defined Function) ì™„ì „ ì •ë¦¬](#udf-user-defined-function-ì™„ì „-ì •ë¦¬)
2. [ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜](#ê³ ê¸‰-ì§‘ê³„ì™€-ìœˆë„ìš°-í•¨ìˆ˜)
3. [íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™”](#íŒŒí‹°ì…”ë‹-ì „ëµê³¼-ì„±ëŠ¥-ìµœì í™”)
4. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì „ììƒê±°ë˜-ë°ì´í„°-ë¶„ì„)
5. [Dockerì™€ Kubernetes ë°°í¬](#dockerì™€-kubernetes-ë°°í¬)
6. [Airflow ìŠ¤ì¼€ì¤„ë§](#airflow-ìŠ¤ì¼€ì¤„ë§)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ”§ UDF (User Defined Function) ì™„ì „ ì •ë¦¬ {#udf-user-defined-function-ì™„ì „-ì •ë¦¬}

### UDFë€?

UDFëŠ” Sparkì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì—¬ ë°ì´í„° ë³€í™˜ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### UDF ì‘ì„± ë°©ë²•

#### **1. ê¸°ë³¸ UDF ì‘ì„±**

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType

# ê°„ë‹¨í•œ ë¬¸ìì—´ ì²˜ë¦¬ UDF
@udf(returnType=StringType())
def clean_text(text):
    if text is None:
        return None
    return text.strip().lower().replace("  ", " ")

# ìˆ˜í•™ ê³„ì‚° UDF
@udf(returnType=FloatType())
def calculate_discount(price, discount_rate):
    if price is None or discount_rate is None:
        return None
    return float(price * (1 - discount_rate))

# ì¡°ê±´ë¶€ ì²˜ë¦¬ UDF
@udf(returnType=StringType())
def categorize_price(price):
    if price is None:
        return "Unknown"
    elif price < 100:
        return "Low"
    elif price < 500:
        return "Medium"
    else:
        return "High"

# ì‚¬ìš© ì˜ˆì œ
df = spark.createDataFrame([
    ("  Product A  ", 150.0, 0.1),
    ("Product B", 75.0, 0.2),
    (None, 600.0, 0.15)
], ["product_name", "price", "discount_rate"])

df.withColumn("clean_name", clean_text("product_name")) \
  .withColumn("final_price", calculate_discount("price", "discount_rate")) \
  .withColumn("price_category", categorize_price("price")) \
  .show()
```

#### **2. ë³µì¡í•œ UDF - JSON íŒŒì‹±**

```python
from pyspark.sql.types import MapType, StringType
import json

@udf(returnType=MapType(StringType(), StringType()))
def parse_json_metadata(json_str):
    try:
        if json_str is None:
            return {}
        data = json.loads(json_str)
        # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return {str(k): str(v) for k, v in data.items()}
    except:
        return {}

# ì‚¬ìš© ì˜ˆì œ
json_data = spark.createDataFrame([
    ('{"category": "electronics", "brand": "Samsung", "rating": 4.5}'),
    ('{"category": "clothing", "brand": "Nike", "rating": 4.2}'),
    ('invalid json')
], ["metadata"])

json_data.withColumn("parsed_metadata", parse_json_metadata("metadata")).show(truncate=False)
```

#### **3. ê³ ê¸‰ UDF - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©**

```python
from pyspark.sql.types import ArrayType, FloatType
import numpy as np
from sklearn.ensemble import IsolationForest

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
model = None

def initialize_model():
    global model
    # ê°„ë‹¨í•œ ì´ìƒ íƒì§€ ëª¨ë¸ ì´ˆê¸°í™”
    model = IsolationForest(contamination=0.1, random_state=42)
    # ë”ë¯¸ ë°ì´í„°ë¡œ í›ˆë ¨
    dummy_data = np.random.randn(1000, 3)
    model.fit(dummy_data)

@udf(returnType=FloatType())
def anomaly_score(features_array):
    global model
    if model is None:
        initialize_model()
    
    if features_array is None or len(features_array) != 3:
        return 0.0
    
    try:
        features = np.array(features_array).reshape(1, -1)
        score = model.decision_function(features)[0]
        return float(score)
    except:
        return 0.0

# ì‚¬ìš© ì˜ˆì œ
features_data = spark.createDataFrame([
    ([1.2, 3.4, 2.1],),
    ([10.5, 15.2, 8.9],),
    ([0.1, 0.3, 0.2],)
], ["features"])

features_data.withColumn("anomaly_score", anomaly_score("features")).show()
```

### UDF ìµœì í™” íŒ

#### **1. Vectorized UDF ì‚¬ìš©**

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import FloatType

# Pandas UDFëŠ” ë” ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µ
@pandas_udf(returnType=FloatType())
def fast_calculation(series: pd.Series) -> pd.Series:
    # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    return series * 2 + 1

# ì‚¬ìš©
df.withColumn("fast_result", fast_calculation("value")).show()
```

#### **2. UDF ìºì‹±ê³¼ ì¬ì‚¬ìš©**

```python
# UDFë¥¼ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ì¬ì‚¬ìš©
def create_text_processor():
    @udf(returnType=StringType())
    def process_text(text):
        return text.upper() if text else None
    return process_text

# ì—¬ëŸ¬ DataFrameì—ì„œ ì¬ì‚¬ìš©
text_processor = create_text_processor()
df1.withColumn("processed", text_processor("text1")).show()
df2.withColumn("processed", text_processor("text2")).show()
```

## ğŸ“Š ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜ {#ê³ ê¸‰-ì§‘ê³„ì™€-ìœˆë„ìš°-í•¨ìˆ˜}

### ê³ ê¸‰ ìœˆë„ìš° í•¨ìˆ˜

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import (
    row_number, rank, dense_rank, lag, lead, 
    first_value, last_value, nth_value,
    cume_dist, percent_rank, ntile
)

# ë³µì¡í•œ ìœˆë„ìš° ìŠ¤í™ ì •ì˜
window_spec = Window.partitionBy("category").orderBy("sales_amount")

# ë‹¤ì–‘í•œ ìœˆë„ìš° í•¨ìˆ˜ ì ìš©
df_advanced = df.withColumn("row_num", row_number().over(window_spec)) \
    .withColumn("rank", rank().over(window_spec)) \
    .withColumn("dense_rank", dense_rank().over(window_spec)) \
    .withColumn("prev_sales", lag("sales_amount", 1).over(window_spec)) \
    .withColumn("next_sales", lead("sales_amount", 1).over(window_spec)) \
    .withColumn("first_sales", first_value("sales_amount").over(window_spec)) \
    .withColumn("last_sales", last_value("sales_amount").over(window_spec)) \
    .withColumn("cumulative_dist", cume_dist().over(window_spec)) \
    .withColumn("percentile_rank", percent_rank().over(window_spec)) \
    .withColumn("quartile", ntile(4).over(window_spec))
```

### ê³ ê¸‰ ì§‘ê³„ í•¨ìˆ˜

```python
from pyspark.sql.functions import (
    collect_list, collect_set, array_agg,
    stddev, variance, skewness, kurtosis,
    corr, covar_pop, covar_samp
)

# í†µê³„ì  ì§‘ê³„
stats_df = df.groupBy("category") \
    .agg(
        count("*").alias("count"),
        avg("price").alias("avg_price"),
        stddev("price").alias("stddev_price"),
        variance("price").alias("variance_price"),
        skewness("price").alias("skewness_price"),
        kurtosis("price").alias("kurtosis_price"),
        collect_list("product_name").alias("all_products"),
        collect_set("brand").alias("unique_brands")
    )
```

## âš¡ íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™” {#íŒŒí‹°ì…”ë‹-ì „ëµê³¼-ì„±ëŠ¥-ìµœì í™”}

### íŒŒí‹°ì…”ë‹ ì „ëµ

```python
# 1. ì»¬ëŸ¼ ê¸°ë°˜ íŒŒí‹°ì…”ë‹
df.write.mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("path/to/partitioned_data")

# 2. ë²„í‚·íŒ…
df.write.mode("overwrite") \
    .bucketBy(10, "user_id") \
    .sortBy("timestamp") \
    .saveAsTable("bucketed_table")

# 3. ë™ì  íŒŒí‹°ì…”ë‹
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
```

### ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```python
# ë©”ëª¨ë¦¬ ìµœì í™”
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# ìºì‹± ì „ëµ
df.cache()  # ë©”ëª¨ë¦¬ ìºì‹±
df.persist(StorageLevel.MEMORY_AND_DISK)  # ë©”ëª¨ë¦¬+ë””ìŠ¤í¬ ìºì‹±

# ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")
```

## ğŸ›’ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ecommerce-analysis/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ analytics.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ spark_config.py
â”‚   â””â”€â”€ app_config.yaml
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_processing.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ spark-job.yaml
â”‚   â””â”€â”€ airflow-dag.py
â””â”€â”€ README.md
```

### 1. ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ

```python
# src/data_processing.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import logging

class EcommerceDataProcessor:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
        
    def load_data(self, data_path):
        """ë°ì´í„° ë¡œë“œ"""
        self.logger.info(f"Loading data from {data_path}")
        
        # ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ
        orders_df = self.spark.read.parquet(f"{data_path}/orders")
        products_df = self.spark.read.parquet(f"{data_path}/products")
        customers_df = self.spark.read.parquet(f"{data_path}/customers")
        
        return orders_df, products_df, customers_df
    
    def clean_data(self, orders_df, products_df, customers_df):
        """ë°ì´í„° ì •ì œ"""
        self.logger.info("Cleaning data...")
        
        # ì¤‘ë³µ ì œê±°
        orders_clean = orders_df.dropDuplicates(["order_id"])
        products_clean = products_df.dropDuplicates(["product_id"])
        customers_clean = customers_df.dropDuplicates(["customer_id"])
        
        # NULL ê°’ ì²˜ë¦¬
        orders_clean = orders_clean.fillna({"quantity": 1, "discount": 0})
        products_clean = products_clean.fillna({"price": 0, "category": "Unknown"})
        
        return orders_clean, products_clean, customers_clean
    
    def enrich_data(self, orders_df, products_df, customers_df):
        """ë°ì´í„° í’ë¶€í™”"""
        self.logger.info("Enriching data...")
        
        # ì¡°ì¸ì„ í†µí•œ ë°ì´í„° í’ë¶€í™”
        enriched_df = orders_df \
            .join(products_df, "product_id", "left") \
            .join(customers_df, "customer_id", "left")
        
        # ê³„ì‚°ëœ ì»¬ëŸ¼ ì¶”ê°€
        enriched_df = enriched_df.withColumn(
            "total_amount", 
            col("quantity") * col("price") * (1 - col("discount"))
        )
        
        # ê³ ê° ë“±ê¸‰ ê³„ì‚° UDF
        @udf(returnType=StringType())
        def calculate_customer_tier(total_spent):
            if total_spent >= 10000:
                return "VIP"
            elif total_spent >= 5000:
                return "Gold"
            elif total_spent >= 1000:
                return "Silver"
            else:
                return "Bronze"
        
        # ê³ ê°ë³„ ì´ êµ¬ë§¤ì•¡ ê³„ì‚°
        customer_totals = enriched_df.groupBy("customer_id") \
            .agg(sum("total_amount").alias("total_spent"))
        
        enriched_df = enriched_df.join(customer_totals, "customer_id", "left") \
            .withColumn("customer_tier", calculate_customer_tier("total_spent"))
        
        return enriched_df
```

### 2. ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆ

```python
# src/analytics.py
from pyspark.sql.functions import *
from pyspark.sql.window import Window
import pandas as pd

class EcommerceAnalytics:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def sales_analysis(self, df):
        """ë§¤ì¶œ ë¶„ì„"""
        # ì¼ë³„ ë§¤ì¶œ íŠ¸ë Œë“œ
        daily_sales = df.groupBy("order_date") \
            .agg(
                sum("total_amount").alias("daily_revenue"),
                countDistinct("customer_id").alias("daily_customers"),
                avg("total_amount").alias("avg_order_value")
            ) \
            .orderBy("order_date")
        
        # ì œí’ˆë³„ ë§¤ì¶œ ë¶„ì„
        product_sales = df.groupBy("product_id", "product_name", "category") \
            .agg(
                sum("quantity").alias("total_quantity"),
                sum("total_amount").alias("total_revenue"),
                countDistinct("customer_id").alias("unique_customers")
            ) \
            .orderBy(desc("total_revenue"))
        
        return daily_sales, product_sales
    
    def customer_analysis(self, df):
        """ê³ ê° ë¶„ì„"""
        # ê³ ê°ë³„ êµ¬ë§¤ íŒ¨í„´
        customer_patterns = df.groupBy("customer_id", "customer_tier") \
            .agg(
                count("*").alias("total_orders"),
                sum("total_amount").alias("total_spent"),
                avg("total_amount").alias("avg_order_value"),
                min("order_date").alias("first_purchase"),
                max("order_date").alias("last_purchase")
            )
        
        # RFM ë¶„ì„ (Recency, Frequency, Monetary)
        rfm_analysis = df.groupBy("customer_id") \
            .agg(
                datediff(current_date(), max("order_date")).alias("recency"),
                count("*").alias("frequency"),
                sum("total_amount").alias("monetary")
            )
        
        return customer_patterns, rfm_analysis
    
    def advanced_analytics(self, df):
        """ê³ ê¸‰ ë¶„ì„"""
        # ì½”í˜¸íŠ¸ ë¶„ì„
        cohort_analysis = df.select("customer_id", "order_date") \
            .withColumn("cohort_month", date_format("order_date", "yyyy-MM")) \
            .groupBy("cohort_month") \
            .agg(countDistinct("customer_id").alias("cohort_size"))
        
        # ìƒí’ˆ ì¶”ì²œì„ ìœ„í•œ í˜‘ì—… í•„í„°ë§ ê¸°ì´ˆ
        window_spec = Window.partitionBy("customer_id").orderBy(desc("order_date"))
        
        customer_product_matrix = df.select("customer_id", "product_id", "total_amount") \
            .withColumn("rank", row_number().over(window_spec)) \
            .filter(col("rank") <= 10)  # ìµœê·¼ 10ê°œ êµ¬ë§¤ ìƒí’ˆ
        
        return cohort_analysis, customer_product_matrix
```

### 3. ì„¤ì • íŒŒì¼

```python
# config/spark_config.py
def get_spark_config():
    return {
        "spark.app.name": "EcommerceAnalysis",
        "spark.master": "local[*]",
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.sql.adaptive.advisoryPartitionSizeInBytes": "128MB",
        "spark.sql.adaptive.skewJoin.enabled": "true",
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
        "spark.sql.execution.arrow.pyspark.enabled": "true",
        "spark.sql.adaptive.skewJoin.skewedPartitionFactor": "5",
        "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes": "256MB"
    }
```

```yaml
# config/app_config.yaml
data:
  input_path: "/data/input"
  output_path: "/data/output"
  checkpoint_path: "/data/checkpoint"

processing:
  batch_size: 10000
  parallelism: 4
  cache_enabled: true

output:
  format: "parquet"
  compression: "snappy"
  partition_by: ["year", "month"]
```

## ğŸ³ Dockerì™€ Kubernetes ë°°í¬

### 1. Dockerfile

```dockerfile
# Dockerfile
FROM openjdk:8-jdk-alpine

# Python ì„¤ì¹˜
RUN apk add --no-cache python3 py3-pip

# Spark ì„¤ì¹˜
ENV SPARK_VERSION=3.4.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt /app/
WORKDIR /app
RUN pip3 install -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY src/ /app/src/
COPY config/ /app/config/

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
RUN chmod +x /app/src/main.py

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 4040 8080

# ì‹¤í–‰ ëª…ë ¹
CMD ["python3", "/app/src/main.py"]
```

### 2. Kubernetes Job ì„¤ì •

```yaml
# kubernetes/spark-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ecommerce-analysis-job
  namespace: data-engineering
spec:
  template:
    spec:
      containers:
      - name: spark-driver
        image: ecommerce-analysis:latest
        command: ["python3", "/app/src/main.py"]
        env:
        - name: SPARK_MASTER
          value: "k8s://https://kubernetes.default.svc.cluster.local:443"
        - name: SPARK_APP_NAME
          value: "ecommerce-analysis"
        - name: SPARK_DRIVER_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SPARK_DRIVER_PORT
          value: "7077"
        - name: SPARK_UI_PORT
          value: "4040"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        volumeMounts:
        - name: data-volume
          mountPath: /data
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: data-pvc
      - name: config-volume
        configMap:
          name: app-config
      restartPolicy: Never
  backoffLimit: 3
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
  namespace: data-engineering
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: data-engineering
data:
  app_config.yaml: |
    data:
      input_path: "/data/input"
      output_path: "/data/output"
    processing:
      batch_size: 10000
      parallelism: 4
```

### 3. Spark Application ì‹¤í–‰

```python
# src/main.py
import os
import sys
import logging
from pyspark.sql import SparkSession
from data_processing import EcommerceDataProcessor
from analytics import EcommerceAnalytics
from config.spark_config import get_spark_config

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def main():
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        # Spark ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .config(conf=get_spark_config()) \
            .getOrCreate()
        
        logger.info("Spark session created successfully")
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = EcommerceDataProcessor(spark)
        analytics = EcommerceAnalytics(spark)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        input_path = os.getenv("INPUT_PATH", "/data/input")
        output_path = os.getenv("OUTPUT_PATH", "/data/output")
        
        # ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        logger.info("Loading data...")
        orders_df, products_df, customers_df = processor.load_data(input_path)
        
        logger.info("Cleaning data...")
        orders_clean, products_clean, customers_clean = processor.clean_data(
            orders_df, products_df, customers_df
        )
        
        logger.info("Enriching data...")
        enriched_df = processor.enrich_data(
            orders_clean, products_clean, customers_clean
        )
        
        # ë¶„ì„ ìˆ˜í–‰
        logger.info("Performing sales analysis...")
        daily_sales, product_sales = analytics.sales_analysis(enriched_df)
        
        logger.info("Performing customer analysis...")
        customer_patterns, rfm_analysis = analytics.customer_analysis(enriched_df)
        
        logger.info("Performing advanced analytics...")
        cohort_analysis, customer_product_matrix = analytics.advanced_analytics(enriched_df)
        
        # ê²°ê³¼ ì €ì¥
        logger.info("Saving results...")
        enriched_df.write.mode("overwrite").parquet(f"{output_path}/enriched_data")
        daily_sales.write.mode("overwrite").parquet(f"{output_path}/daily_sales")
        product_sales.write.mode("overwrite").parquet(f"{output_path}/product_sales")
        customer_patterns.write.mode("overwrite").parquet(f"{output_path}/customer_patterns")
        rfm_analysis.write.mode("overwrite").parquet(f"{output_path}/rfm_analysis")
        
        logger.info("Job completed successfully!")
        
    except Exception as e:
        logger.error(f"Job failed with error: {str(e)}")
        sys.exit(1)
    
    finally:
        if 'spark' in locals():
            spark.stop()

if __name__ == "__main__":
    main()
```

## ğŸ”„ Airflow ìŠ¤ì¼€ì¤„ë§ {#airflow-ìŠ¤ì¼€ì¤„ë§}

### Airflow DAG

```python
# kubernetes/airflow-dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.kubernetes import KubernetesPodOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator

# ê¸°ë³¸ ì¸ìˆ˜
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
dag = DAG(
    'ecommerce_analysis_pipeline',
    default_args=default_args,
    description='ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
    catchup=False,
    tags=['ecommerce', 'spark', 'analytics'],
)

# ì‹œì‘ ì‘ì—…
start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag,
)

# ë°ì´í„° ê²€ì¦ ì‘ì—…
def validate_input_data():
    import os
    input_path = "/data/input"
    required_files = ["orders", "products", "customers"]
    
    for file_name in required_files:
        file_path = f"{input_path}/{file_name}"
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Required file not found: {file_path}")
    
    print("Input data validation completed successfully")

validate_task = PythonOperator(
    task_id='validate_input_data',
    python_callable=validate_input_data,
    dag=dag,
)

# Spark ë¶„ì„ ì‘ì—…
spark_analysis_task = KubernetesPodOperator(
    task_id='spark_ecommerce_analysis',
    namespace='data-engineering',
    image='ecommerce-analysis:latest',
    name='spark-analysis-pod',
    cmds=['python3', '/app/src/main.py'],
    env_vars={
        'INPUT_PATH': '/data/input',
        'OUTPUT_PATH': '/data/output',
        'SPARK_MASTER': 'k8s://https://kubernetes.default.svc.cluster.local:443',
        'SPARK_APP_NAME': 'ecommerce-analysis',
    },
    resources={
        'request_memory': '2Gi',
        'request_cpu': '1',
        'limit_memory': '4Gi',
        'limit_cpu': '2',
    },
    volumes=[
        {
            'name': 'data-volume',
            'persistentVolumeClaim': {
                'claimName': 'data-pvc'
            }
        }
    ],
    volume_mounts=[
        {
            'name': 'data-volume',
            'mountPath': '/data'
        }
    ],
    get_logs=True,
    is_delete_operator_pod=True,
    dag=dag,
)

# ê²°ê³¼ ê²€ì¦ ì‘ì—…
def validate_output_data():
    import os
    output_path = "/data/output"
    required_outputs = [
        "enriched_data",
        "daily_sales", 
        "product_sales",
        "customer_patterns",
        "rfm_analysis"
    ]
    
    for output_name in required_outputs:
        output_dir = f"{output_path}/{output_name}"
        if not os.path.exists(output_dir):
            raise FileNotFoundError(f"Output directory not found: {output_dir}")
        
        # íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        files = os.listdir(output_dir)
        if not files:
            raise ValueError(f"Output directory is empty: {output_dir}")
    
    print("Output data validation completed successfully")

validate_output_task = PythonOperator(
    task_id='validate_output_data',
    python_callable=validate_output_data,
    dag=dag,
)

# ì•Œë¦¼ ì‘ì—…
def send_completion_notification():
    print("Ecommerce analysis pipeline completed successfully!")
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Email ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
    # slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
    # send_slack_notification(slack_webhook_url, "Pipeline completed successfully")

notification_task = PythonOperator(
    task_id='send_completion_notification',
    python_callable=send_completion_notification,
    dag=dag,
)

# ì¢…ë£Œ ì‘ì—…
end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag,
)

# ì‘ì—… ì˜ì¡´ì„± ì •ì˜
start_task >> validate_task >> spark_analysis_task >> validate_output_task >> notification_task >> end_task
```

### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# deploy.sh

echo "Building Docker image..."
docker build -t ecommerce-analysis:latest .

echo "Loading image to Kubernetes cluster..."
kind load docker-image ecommerce-analysis:latest

echo "Applying Kubernetes manifests..."
kubectl apply -f kubernetes/spark-job.yaml

echo "Deploying Airflow DAG..."
kubectl cp kubernetes/airflow-dag.py airflow-web-0:/opt/airflow/dags/

echo "Deployment completed!"
```

## âš¡ ë°°ì¹˜ ì²˜ë¦¬ ì²˜ë¦¬ëŸ‰ ìµœì í™” {#ë°°ì¹˜-ì²˜ë¦¬-ì²˜ë¦¬ëŸ‰-ìµœì í™”}

### ì²˜ë¦¬ëŸ‰ ë¶„ì„ ë„êµ¬

```python
# ë°°ì¹˜ ì²˜ë¦¬ëŸ‰ ë¶„ì„ ë° ìµœì í™” ë„êµ¬
class BatchThroughputOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
        
    def analyze_throughput(self, df, operation_name="batch_operation"):
        """ì²˜ë¦¬ëŸ‰ ë¶„ì„"""
        import time
        from pyspark.sql.functions import count
        
        # ë°ì´í„° í¬ê¸° ì¸¡ì •
        row_count = df.count()
        num_partitions = df.rdd.getNumPartitions()
        
        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        # ìƒ˜í”Œ ì‘ì—… ì‹¤í–‰ (ì‹¤ì œ ì‘ì—…ìœ¼ë¡œ ëŒ€ì²´)
        result = df.select(count("*").alias("total_count")).collect()
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
        throughput_records_per_second = row_count / processing_time if processing_time > 0 else 0
        
        return {
            'operation_name': operation_name,
            'total_records': row_count,
            'num_partitions': num_partitions,
            'processing_time_seconds': processing_time,
            'throughput_records_per_second': throughput_records_per_second,
            'throughput_records_per_minute': throughput_records_per_second * 60,
            'avg_records_per_partition': row_count / num_partitions if num_partitions > 0 else 0
        }
    
    def optimize_partitioning_for_throughput(self, df, target_records_per_partition=100000):
        """ì²˜ë¦¬ëŸ‰ì„ ìœ„í•œ íŒŒí‹°ì…”ë‹ ìµœì í™”"""
        current_partitions = df.rdd.getNumPartitions()
        row_count = df.count()
        
        # ìµœì  íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚°
        optimal_partitions = max(1, row_count // target_records_per_partition)
        
        # íŒŒí‹°ì…˜ í¬ê¸° ê¸°ë°˜ ì¡°ì •
        if optimal_partitions > current_partitions:
            # íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
            optimized_df = df.repartition(optimal_partitions)
            action = f"repartitioned from {current_partitions} to {optimal_partitions} partitions"
        elif optimal_partitions < current_partitions:
            # íŒŒí‹°ì…˜ ìˆ˜ ê°ì†Œ
            optimized_df = df.coalesce(optimal_partitions)
            action = f"coalesced from {current_partitions} to {optimal_partitions} partitions"
        else:
            optimized_df = df
            action = "no partitioning change needed"
        
        return {
            'optimized_dataframe': optimized_df,
            'original_partitions': current_partitions,
            'optimized_partitions': optimal_partitions,
            'action_taken': action,
            'target_records_per_partition': target_records_per_partition
        }
    
    def optimize_memory_for_throughput(self, spark_session):
        """ì²˜ë¦¬ëŸ‰ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        memory_configs = {
            # ë©”ëª¨ë¦¬ ê´€ë ¨ ì„¤ì •
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
            'spark.sql.adaptive.advisoryPartitionSizeInBytes': '128MB',
            'spark.sql.adaptive.skewJoin.enabled': 'true',
            
            # ì§ë ¬í™” ìµœì í™”
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            'spark.sql.execution.arrow.pyspark.enabled': 'true',
            'spark.sql.execution.arrow.maxRecordsPerBatch': '10000',
            
            # ìºì‹± ìµœì í™”
            'spark.sql.execution.arrow.pyspark.fallback.enabled': 'true',
            'spark.sql.adaptive.localShuffleReader.enabled': 'true',
            
            # ì¡°ì¸ ìµœì í™”
            'spark.sql.adaptive.skewJoin.skewedPartitionFactor': '5',
            'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes': '256MB'
        }
        
        for key, value in memory_configs.items():
            spark_session.conf.set(key, value)
        
        return memory_configs
    
    def implement_parallel_processing(self, df, parallelism_factor=2):
        """ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„"""
        current_partitions = df.rdd.getNumPartitions()
        optimized_partitions = current_partitions * parallelism_factor
        
        return df.repartition(optimized_partitions)
    
    def optimize_data_loading(self, file_path, format_type="parquet"):
        """ë°ì´í„° ë¡œë”© ìµœì í™”"""
        if format_type == "parquet":
            # Parquet íŒŒì¼ ìµœì í™”
            df = self.spark.read.parquet(file_path)
            
            # íŒŒí‹°ì…˜ í”„ë£¨ë‹ í™œì„±í™”
            self.spark.conf.set("spark.sql.optimizer.metadataOnly", "true")
            self.spark.conf.set("spark.sql.parquet.filterPushdown", "true")
            self.spark.conf.set("spark.sql.parquet.mergeSchema", "false")
            
        elif format_type == "csv":
            # CSV íŒŒì¼ ìµœì í™”
            df = self.spark.read.option("header", "true").option("inferSchema", "true").csv(file_path)
            
            # ë³‘ë ¬ ë¡œë”©ì„ ìœ„í•œ íŒŒí‹°ì…”ë‹
            df = df.repartition(200)
            
        return df

# ì²˜ë¦¬ëŸ‰ ìµœì í™” ì˜ˆì œ
def batch_throughput_optimization_example():
    spark = SparkSession.builder.appName("BatchThroughputOptimization").getOrCreate()
    optimizer = BatchThroughputOptimizer(spark)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = [(i, f"product_{i}", i * 10.5, i % 10) for i in range(1000000)]
    df = spark.createDataFrame(data, ["id", "name", "price", "category"])
    
    # ì²˜ë¦¬ëŸ‰ ë¶„ì„
    throughput_analysis = optimizer.analyze_throughput(df, "sample_analysis")
    print("=== Batch Throughput Analysis ===")
    print(f"Total Records: {throughput_analysis['total_records']:,}")
    print(f"Processing Time: {throughput_analysis['processing_time_seconds']:.2f}s")
    print(f"Throughput: {throughput_analysis['throughput_records_per_second']:.2f} records/sec")
    print(f"Partitions: {throughput_analysis['num_partitions']}")
    
    # íŒŒí‹°ì…”ë‹ ìµœì í™”
    partitioning_result = optimizer.optimize_partitioning_for_throughput(df)
    print(f"\nPartitioning Optimization: {partitioning_result['action_taken']}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    memory_configs = optimizer.optimize_memory_for_throughput(spark)
    print("\n=== Memory Optimization Configs ===")
    for key, value in memory_configs.items():
        print(f"{key}: {value}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
    parallel_df = optimizer.implement_parallel_processing(df)
    print(f"\nParallel Processing: {parallel_df.rdd.getNumPartitions()} partitions")
    
    return optimizer
```

### ê³ ê¸‰ ì²˜ë¦¬ëŸ‰ ìµœì í™” ê¸°ë²•

```python
# ê³ ê¸‰ ì²˜ë¦¬ëŸ‰ ìµœì í™” í´ë˜ìŠ¤
class AdvancedThroughputOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def implement_columnar_processing(self, df):
        """ì»¬ëŸ¼í˜• ì²˜ë¦¬ ìµœì í™”"""
        # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ì»¬ëŸ¼í˜• ì²˜ë¦¬ í™œì„±í™”
        temp_path = "/tmp/optimized_data"
        
        df.write.mode("overwrite").parquet(temp_path)
        
        # ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë‹¤ì‹œ ì½ê¸°
        optimized_configs = {
            'spark.sql.parquet.filterPushdown': 'true',
            'spark.sql.parquet.mergeSchema': 'false',
            'spark.sql.parquet.enableVectorizedReader': 'true',
            'spark.sql.parquet.columnarReaderBatchSize': '4096'
        }
        
        for key, value in optimized_configs.items():
            self.spark.conf.set(key, value)
        
        return self.spark.read.parquet(temp_path)
    
    def optimize_joins_for_throughput(self, df1, df2, join_key):
        """ì¡°ì¸ ìµœì í™”"""
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ íŒíŠ¸ ì ìš© (ì‘ì€ í…Œì´ë¸”ì˜ ê²½ìš°)
        from pyspark.sql.functions import broadcast
        
        # í…Œì´ë¸” í¬ê¸° ë¹„êµ
        size1 = df1.count()
        size2 = df2.count()
        
        if size1 < size2 and size1 < 100000:  # 10ë§Œ ë ˆì½”ë“œ ë¯¸ë§Œ
            optimized_df1 = broadcast(df1)
            optimized_df2 = df2
        elif size2 < 100000:
            optimized_df1 = df1
            optimized_df2 = broadcast(df2)
        else:
            optimized_df1 = df1
            optimized_df2 = df2
        
        # ì¡°ì¸ ì‹¤í–‰
        result = optimized_df1.join(optimized_df2, join_key, "inner")
        
        return result
    
    def implement_bucketing_strategy(self, df, bucket_columns, num_buckets=200):
        """ë²„í‚·íŒ… ì „ëµ êµ¬í˜„"""
        # ë²„í‚·íŒ…ì„ ìœ„í•œ ì„ì‹œ í…Œì´ë¸” ìƒì„±
        temp_table_name = "temp_bucketed_table"
        
        # ë²„í‚·íŒ… ì„¤ì •
        bucketing_configs = {
            'spark.sql.sources.bucketing.enabled': 'true',
            'spark.sql.sources.bucketing.autoBucketedScan.enabled': 'true'
        }
        
        for key, value in bucketing_configs.items():
            self.spark.conf.set(key, value)
        
        # ë²„í‚·íŒ…ëœ í…Œì´ë¸”ë¡œ ì €ì¥
        df.write \
            .bucketBy(num_buckets, *bucket_columns) \
            .mode("overwrite") \
            .saveAsTable(temp_table_name)
        
        # ë²„í‚·íŒ…ëœ í…Œì´ë¸” ì½ê¸°
        bucketed_df = self.spark.table(temp_table_name)
        
        return bucketed_df
    
    def optimize_aggregations(self, df, group_columns, agg_columns):
        """ì§‘ê³„ ìµœì í™”"""
        from pyspark.sql.functions import col, sum as spark_sum, avg, count, max as spark_max
        
        # ì§‘ê³„ ì „ íŒŒí‹°ì…”ë‹ ìµœì í™”
        optimized_df = df.repartition(*group_columns)
        
        # ì§‘ê³„ ì‹¤í–‰
        aggregation_result = optimized_df.groupBy(*group_columns).agg(
            count("*").alias("record_count"),
            spark_sum(agg_columns[0]).alias(f"total_{agg_columns[0]}"),
            avg(agg_columns[0]).alias(f"avg_{agg_columns[0]}"),
            spark_max(agg_columns[0]).alias(f"max_{agg_columns[0]}")
        )
        
        return aggregation_result
    
    def implement_caching_strategy(self, df, access_frequency="high"):
        """ìºì‹± ì „ëµ êµ¬í˜„"""
        from pyspark import StorageLevel
        
        if access_frequency == "high":
            # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì— ìºì‹±
            cached_df = df.persist(StorageLevel.MEMORY_ONLY)
        elif access_frequency == "medium":
            # ì¤‘ê°„ ë¹ˆë„ëŠ” ë©”ëª¨ë¦¬+ë””ìŠ¤í¬ ìºì‹±
            cached_df = df.persist(StorageLevel.MEMORY_AND_DISK_SER)
        else:
            # ë‚®ì€ ë¹ˆë„ëŠ” ë””ìŠ¤í¬ ìºì‹±
            cached_df = df.persist(StorageLevel.DISK_ONLY)
        
        return cached_df
    
    def optimize_file_formats(self, df, output_path, format_type="parquet"):
        """íŒŒì¼ í˜•ì‹ ìµœì í™”"""
        if format_type == "parquet":
            # Parquet ìµœì í™” ì„¤ì •
            df.write \
                .mode("overwrite") \
                .option("compression", "snappy") \
                .option("parquet.block.size", "134217728") \
                .option("parquet.page.size", "1048576") \
                .parquet(output_path)
                
        elif format_type == "orc":
            # ORC ìµœì í™” ì„¤ì •
            df.write \
                .mode("overwrite") \
                .option("compression", "zlib") \
                .option("orc.stripe.size", "67108864") \
                .orc(output_path)
                
        elif format_type == "delta":
            # Delta Lake ìµœì í™” ì„¤ì •
            df.write \
                .mode("overwrite") \
                .format("delta") \
                .option("delta.autoOptimize.optimizeWrite", "true") \
                .option("delta.autoOptimize.autoCompact", "true") \
                .save(output_path)
        
        return f"Data saved to {output_path} in {format_type} format"

# ê³ ê¸‰ ì²˜ë¦¬ëŸ‰ ìµœì í™” ì˜ˆì œ
def advanced_throughput_optimization_example():
    spark = SparkSession.builder.appName("AdvancedThroughputOptimization").getOrCreate()
    optimizer = AdvancedThroughputOptimizer(spark)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data1 = [(i, f"product_{i}", i * 10.5, i % 100) for i in range(500000)]
    df1 = spark.createDataFrame(data1, ["id", "name", "price", "category_id"])
    
    data2 = [(i, f"category_{i}") for i in range(100)]
    df2 = spark.createDataFrame(data2, ["category_id", "category_name"])
    
    # ì»¬ëŸ¼í˜• ì²˜ë¦¬ ìµœì í™”
    print("=== Implementing Columnar Processing ===")
    columnar_df = optimizer.implement_columnar_processing(df1)
    print(f"Columnar processing implemented: {columnar_df.count()} records")
    
    # ì¡°ì¸ ìµœì í™”
    print("\n=== Optimizing Joins ===")
    joined_df = optimizer.optimize_joins_for_throughput(df1, df2, "category_id")
    print(f"Join optimization completed: {joined_df.count()} records")
    
    # ì§‘ê³„ ìµœì í™”
    print("\n=== Optimizing Aggregations ===")
    aggregated_df = optimizer.optimize_aggregations(joined_df, ["category_name"], ["price"])
    print(f"Aggregation optimization completed: {aggregated_df.count()} groups")
    
    # íŒŒì¼ í˜•ì‹ ìµœì í™”
    print("\n=== Optimizing File Formats ===")
    output_result = optimizer.optimize_file_formats(aggregated_df, "/tmp/optimized_output", "parquet")
    print(output_result)
    
    return optimizer
```

### ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
# ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
class ThroughputMonitoringSystem:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.metrics_history = []
    
    def monitor_batch_throughput(self, df, operation_name, batch_size=100000):
        """ë°°ì¹˜ ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        import time
        
        total_records = df.count()
        num_batches = (total_records + batch_size - 1) // batch_size
        
        batch_metrics = []
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, total_records)
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_df = df.limit(end_idx).subtract(df.limit(start_idx))
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            batch_count = batch_df.count()
            end_time = time.time()
            
            processing_time = end_time - start_time
            throughput = batch_count / processing_time if processing_time > 0 else 0
            
            batch_metric = {
                'operation_name': operation_name,
                'batch_idx': batch_idx,
                'batch_size': batch_count,
                'processing_time': processing_time,
                'throughput_records_per_second': throughput,
                'timestamp': time.time()
            }
            
            batch_metrics.append(batch_metric)
            print(f"Batch {batch_idx + 1}/{num_batches}: {throughput:.2f} records/sec")
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_time = sum(m['processing_time'] for m in batch_metrics)
        avg_throughput = total_records / total_time if total_time > 0 else 0
        
        summary = {
            'operation_name': operation_name,
            'total_records': total_records,
            'total_batches': num_batches,
            'total_processing_time': total_time,
            'average_throughput': avg_throughput,
            'batch_metrics': batch_metrics
        }
        
        self.metrics_history.append(summary)
        return summary
    
    def generate_throughput_report(self):
        """ì²˜ë¦¬ëŸ‰ ë³´ê³ ì„œ ìƒì„±"""
        if not self.metrics_history:
            return {"message": "No metrics available"}
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_operations = len(self.metrics_history)
        total_records = sum(m['total_records'] for m in self.metrics_history)
        total_time = sum(m['total_processing_time'] for m in self.metrics_history)
        overall_throughput = total_records / total_time if total_time > 0 else 0
        
        # ìµœê³ /ìµœì € ì²˜ë¦¬ëŸ‰ ì°¾ê¸°
        max_throughput = max(m['average_throughput'] for m in self.metrics_history)
        min_throughput = min(m['average_throughput'] for m in self.metrics_history)
        
        report = {
            'summary': {
                'total_operations': total_operations,
                'total_records_processed': total_records,
                'total_processing_time': total_time,
                'overall_throughput': overall_throughput,
                'max_throughput': max_throughput,
                'min_throughput': min_throughput
            },
            'operation_details': self.metrics_history,
            'recommendations': self._generate_throughput_recommendations()
        }
        
        return report
    
    def _generate_throughput_recommendations(self):
        """ì²˜ë¦¬ëŸ‰ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        # í‰ê·  ì²˜ë¦¬ëŸ‰ ê³„ì‚°
        avg_throughput = sum(m['average_throughput'] for m in self.metrics_history) / len(self.metrics_history)
        
        # ì²˜ë¦¬ëŸ‰ì´ ë‚®ì€ ê²½ìš° ê¶Œì¥ì‚¬í•­
        if avg_throughput < 10000:  # 10,000 records/sec ë¯¸ë§Œ
            recommendations.append({
                'priority': 'high',
                'category': 'partitioning',
                'action': 'íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í–¥ìƒì‹œí‚¤ì„¸ìš”',
                'details': 'repartition() ë˜ëŠ” coalesce() ì‚¬ìš© ê³ ë ¤'
            })
            
            recommendations.append({
                'priority': 'medium',
                'category': 'caching',
                'action': 'ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ìºì‹±í•˜ì„¸ìš”',
                'details': 'cache() ë˜ëŠ” persist() ì‚¬ìš©'
            })
        
        # ì²˜ë¦¬ëŸ‰ ë³€ë™ì´ í° ê²½ìš°
        throughputs = [m['average_throughput'] for m in self.metrics_history]
        throughput_variance = max(throughputs) - min(throughputs)
        
        if throughput_variance > avg_throughput * 0.5:  # ë³€ë™ì´ í‰ê· ì˜ 50% ì´ìƒ
            recommendations.append({
                'priority': 'medium',
                'category': 'stability',
                'action': 'ì²˜ë¦¬ëŸ‰ ì•ˆì •ì„±ì„ ìœ„í•´ ë°ì´í„° ë¶„í¬ë¥¼ ê°œì„ í•˜ì„¸ìš”',
                'details': 'ë°ì´í„° ìŠ¤í ë¬¸ì œ í™•ì¸ ë° í•´ê²°'
            })
        
        return recommendations
    
    def export_metrics_to_monitoring_system(self, report):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œìœ¼ë¡œ ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°"""
        import requests
        import json
        
        # Prometheus í˜•ì‹ìœ¼ë¡œ ë©”íŠ¸ë¦­ ë³€í™˜
        prometheus_metrics = []
        
        for operation in report['operation_details']:
            metric = {
                'metric_name': 'spark_batch_throughput',
                'labels': {
                    'operation': operation['operation_name']
                },
                'value': operation['average_throughput'],
                'timestamp': operation.get('timestamp', time.time())
            }
            prometheus_metrics.append(metric)
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Prometheus Pushgatewayë¡œ ì „ì†¡
        print("=== Exported Metrics to Monitoring System ===")
        for metric in prometheus_metrics:
            print(f"{metric['metric_name']} {metric['labels']} = {metric['value']:.2f}")

# ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§ ì˜ˆì œ
def throughput_monitoring_example():
    spark = SparkSession.builder.appName("ThroughputMonitoring").getOrCreate()
    monitor = ThroughputMonitoringSystem(spark)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = [(i, f"data_{i}", i * 1.5) for i in range(1000000)]
    df = spark.createDataFrame(data, ["id", "value", "score"])
    
    # ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§
    throughput_summary = monitor.monitor_batch_throughput(df, "sample_processing", batch_size=100000)
    
    print("\n=== Throughput Summary ===")
    print(f"Total Records: {throughput_summary['total_records']:,}")
    print(f"Average Throughput: {throughput_summary['average_throughput']:.2f} records/sec")
    print(f"Total Processing Time: {throughput_summary['total_processing_time']:.2f}s")
    
    # ë³´ê³ ì„œ ìƒì„±
    report = monitor.generate_throughput_report()
    
    print("\n=== Throughput Report ===")
    print(f"Overall Throughput: {report['summary']['overall_throughput']:.2f} records/sec")
    print(f"Max Throughput: {report['summary']['max_throughput']:.2f} records/sec")
    print(f"Min Throughput: {report['summary']['min_throughput']:.2f} records/sec")
    
    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    if report['recommendations']:
        print("\n=== Recommendations ===")
        for rec in report['recommendations']:
            priority_icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡" if rec['priority'] == 'medium' else "ğŸŸ¢"
            print(f"{priority_icon} [{rec['priority'].upper()}] {rec['action']}")
            print(f"   Details: {rec['details']}")
    
    # ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°
    monitor.export_metrics_to_monitoring_system(report)
    
    return monitor
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **UDF (User Defined Function)**
   - ê¸°ë³¸ UDF ì‘ì„±ê³¼ í™œìš©
   - ë³µì¡í•œ UDF (JSON íŒŒì‹±, ML ëª¨ë¸ ì ìš©)
   - UDF ìµœì í™” ê¸°ë²•

2. **ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜**
   - ë‹¤ì–‘í•œ ìœˆë„ìš° í•¨ìˆ˜ í™œìš©
   - í†µê³„ì  ì§‘ê³„ í•¨ìˆ˜
   - RFM ë¶„ì„ ë“± ê³ ê¸‰ ë¶„ì„

3. **ì„±ëŠ¥ ìµœì í™”**
   - íŒŒí‹°ì…”ë‹ ì „ëµ
   - ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
   - ìºì‹± ì „ëµ

4. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
   - ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì¡°
   - ì—ëŸ¬ ì²˜ë¦¬ì™€ ë¡œê¹…

5. **í”„ë¡œë•ì…˜ ë°°í¬**
   - Docker ì»¨í…Œì´ë„ˆí™”
   - Kubernetes Job ë°°í¬
   - Airflow ìŠ¤ì¼€ì¤„ë§

6. **ë°°ì¹˜ ì²˜ë¦¬ ì²˜ë¦¬ëŸ‰ ìµœì í™”**
   - ì²˜ë¦¬ëŸ‰ ë¶„ì„ ë„êµ¬
   - ê³ ê¸‰ ì²˜ë¦¬ëŸ‰ ìµœì í™” ê¸°ë²•
   - ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **UDF** | ì»¤ìŠ¤í…€ ë°ì´í„° ë³€í™˜ | â­â­â­â­â­ |
| **ìœˆë„ìš° í•¨ìˆ˜** | ê³ ê¸‰ ë¶„ì„ | â­â­â­â­â­ |
| **Docker** | ì»¨í…Œì´ë„ˆí™” | â­â­â­â­ |
| **Kubernetes** | ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | â­â­â­â­ |
| **Airflow** | ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 3: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- Spark Streamingê³¼ Structured Streaming
- Kafka ì—°ë™ê³¼ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
- ì›Œí„°ë§ˆí‚¹ê³¼ ì§€ì—° ë°ì´í„° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 3: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ Kafka ì—°ë™](/data-engineering/2025/09/13/apache-spark-streaming.html)

---

*ì´ì œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤.* ğŸš€

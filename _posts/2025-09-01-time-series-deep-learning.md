---
layout: post
lang: ko
title: "Part 2: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ - N-BEATSì™€ DeepAR"
description: "ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•µì‹¬ì„ ë°°ìš°ê³  N-BEATSì™€ DeepARì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì„¸ìš”."
date: 2025-09-01
author: Data Droid
category: data-ai
tags: [ì‹œê³„ì—´ì˜ˆì¸¡, ë”¥ëŸ¬ë‹, N-BEATS, DeepAR, PyTorch, ë¨¸ì‹ ëŸ¬ë‹]
series: time-series-forecasting
series_order: 2
reading_time: "30ë¶„"
difficulty: "ì¤‘ê¸‰"
---

# Part 2: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ - N-BEATSì™€ DeepAR

> ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•µì‹¬ì„ ë°°ìš°ê³  N-BEATSì™€ DeepARì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì„¸ìš”.

## ğŸ“‹ ëª©ì°¨

1. [ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥](#ë”¥ëŸ¬ë‹-ê¸°ë°˜-ì‹œê³„ì—´-ì˜ˆì¸¡ì˜-ë“±ì¥)
2. [N-BEATS: í•´ì„ ê°€ëŠ¥í•œ ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ëª¨ë¸](#n-beats-í•´ì„-ê°€ëŠ¥í•œ-ë”¥ëŸ¬ë‹-ì‹œê³„ì—´-ëª¨ë¸)
3. [DeepAR: í™•ë¥ ì  ì‹œê³„ì—´ ì˜ˆì¸¡](#deepar-í™•ë¥ ì -ì‹œê³„ì—´-ì˜ˆì¸¡)
4. [ì‹¤ìŠµ: N-BEATSì™€ DeepAR êµ¬í˜„](#ì‹¤ìŠµ-n-beatsì™€-deepar-êµ¬í˜„)
5. [ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„](#ëª¨ë¸-ì„±ëŠ¥-ë¹„êµ-ë°-ë¶„ì„)
6. [ë‹¤ìŒ ë‹¨ê³„ ë° í™•ì¥](#ë‹¤ìŒ-ë‹¨ê³„-ë°-í™•ì¥)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸš€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥

### ì „í†µì  ë°©ë²•ì˜ í•œê³„

Part 1ì—ì„œ í•™ìŠµí•œ ARIMAì™€ Prophetì€ í›Œë¥­í•œ ëª¨ë¸ì´ì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤:

1. **ì„ í˜•ì„± ê°€ì •**: ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
2. **ìˆ˜ë™ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**: ë„ë©”ì¸ ì§€ì‹ì— ì˜ì¡´ì 
3. **ì¥ê¸° ì˜ì¡´ì„±**: ë¨¼ ê³¼ê±°ì˜ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í•¨
4. **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: ì˜ˆì¸¡ì˜ ì‹ ë¢° êµ¬ê°„ì„ ì •í™•íˆ ì¶”ì •í•˜ê¸° ì–´ë ¤ì›€

### ë”¥ëŸ¬ë‹ì˜ í˜ì‹ 

ë”¥ëŸ¬ë‹ì€ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì‹œê³„ì—´ ì˜ˆì¸¡ì— ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì—ˆìŠµë‹ˆë‹¤:

- **ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ**: ë³µì¡í•œ ê´€ê³„ë¥¼ ìë™ìœ¼ë¡œ ë°œê²¬
- **ìë™ íŠ¹ì„± ì¶”ì¶œ**: ë„ë©”ì¸ ì§€ì‹ ì—†ì´ë„ íŒ¨í„´ í•™ìŠµ
- **ì¥ê¸° ì˜ì¡´ì„± ëª¨ë¸ë§**: LSTM, Transformer ë“±ì„ í†µí•œ ì‹œí€€ìŠ¤ í•™ìŠµ
- **í™•ë¥ ì  ì˜ˆì¸¡**: ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ì—¬ ì‹ ë¢° êµ¬ê°„ ì œê³µ

## ğŸ—ï¸ N-BEATS: í•´ì„ ê°€ëŠ¥í•œ ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ëª¨ë¸

### N-BEATSì˜ í•µì‹¬ ì•„ì´ë””ì–´

N-BEATS (Neural Basis Expansion Analysis for Time Series)ëŠ” 2019ë…„ Element AIì—ì„œ ê°œë°œí•œ ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤.

#### **1. ë¸”ë¡ ê¸°ë°˜ ì•„í‚¤í…ì²˜**

```
ì…ë ¥ ì‹œê³„ì—´ â†’ Backcast ë¸”ë¡ â†’ Forecast ë¸”ë¡ â†’ ìµœì¢… ì˜ˆì¸¡
     â†“              â†“              â†“
  Lookback      ê³¼ê±° ì¬êµ¬ì„±     ë¯¸ë˜ ì˜ˆì¸¡
```

#### **2. í•´ì„ ê°€ëŠ¥í•œ ë¸”ë¡ êµ¬ì¡°**

ê° ë¸”ë¡ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì„± ìš”ì†Œë¥¼ ê°€ì§‘ë‹ˆë‹¤:

- **Linear Layer**: ì…ë ¥ì„ ê³ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
- **ReLU Activation**: ë¹„ì„ í˜•ì„± ì¶”ê°€
- **Linear Layer**: ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
- **Residual Connection**: ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ 

#### **3. ì´ì¤‘ ì¶œë ¥ êµ¬ì¡°**

- **Backcast**: ì…ë ¥ ì‹œê³„ì—´ì„ ì¬êµ¬ì„±í•˜ì—¬ íŒ¨í„´ í•™ìŠµ
- **Forecast**: ë¯¸ë˜ ê°’ì„ ì˜ˆì¸¡

### N-BEATSì˜ ì¥ì 

1. **í•´ì„ ê°€ëŠ¥ì„±**: ê° ë¸”ë¡ì´ í•™ìŠµí•˜ëŠ” íŒ¨í„´ì„ ë¶„ì„ ê°€ëŠ¥
2. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ì‹œê³„ì—´ ê¸¸ì´ì— ì ìš© ê°€ëŠ¥
3. **íš¨ìœ¨ì„±**: ë¹ ë¥¸ í•™ìŠµê³¼ ì˜ˆì¸¡
4. **ì•ˆì •ì„±**: ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ í•´ê²°

## ğŸ”® DeepAR: í™•ë¥ ì  ì‹œê³„ì—´ ì˜ˆì¸¡

### DeepARì˜ í•µì‹¬ ê°œë…

DeepARì€ Amazonì—ì„œ ê°œë°œí•œ í™•ë¥ ì  ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë¡œ, **í™•ë¥  ë¶„í¬ë¥¼ ì§ì ‘ ëª¨ë¸ë§**í•©ë‹ˆë‹¤.

#### **1. í™•ë¥ ì  ì ‘ê·¼ë²•**

ì „í†µì  ëª¨ë¸ë“¤ì´ ì  ì˜ˆì¸¡ì„ í•˜ëŠ” ë°˜ë©´, DeepARì€:
- **í™•ë¥  ë¶„í¬ í•™ìŠµ**: ì •ê·œë¶„í¬, ìŒì´í•­ë¶„í¬ ë“±
- **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: ì˜ˆì¸¡ì˜ ì‹ ë¢° êµ¬ê°„ ì œê³µ
- **ì•™ìƒë¸” íš¨ê³¼**: ì—¬ëŸ¬ ìƒ˜í”Œì„ í†µí•œ ì•ˆì •ì  ì˜ˆì¸¡

#### **2. LSTM ê¸°ë°˜ ì•„í‚¤í…ì²˜**

```
ì‹œê³„ì—´ ì…ë ¥ â†’ LSTM ì…€ â†’ í™•ë¥  ë¶„í¬ íŒŒë¼ë¯¸í„° â†’ ìƒ˜í”Œë§ â†’ ì˜ˆì¸¡
     â†“           â†“              â†“              â†“
  Lookback    ìƒíƒœ ì—…ë°ì´íŠ¸    Î¼, Ïƒ í•™ìŠµ     í™•ë¥ ì  ì˜ˆì¸¡
```

#### **3. ì¡°ê±´ë¶€ í™•ë¥  ëª¨ë¸ë§**

DeepARì€ ë‹¤ìŒ í™•ë¥ ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤:

```
P(y_{t+1} | y_1, y_2, ..., y_t, x_1, x_2, ..., x_t)
```

ì—¬ê¸°ì„œ `x_t`ëŠ” ì™¸ë¶€ ë³€ìˆ˜(íœ´ì¼, ê³„ì ˆ ë“±)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

### DeepARì˜ ì¥ì 

1. **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ë¥¼ ì •í™•íˆ ì¸¡ì •
2. **ì™¸ë¶€ ë³€ìˆ˜ í™œìš©**: ê³„ì ˆì„±, íœ´ì¼ ë“± ì¶”ê°€ ì •ë³´ ë°˜ì˜
3. **ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´**: ì—¬ëŸ¬ ì‹œê³„ì—´ì„ ë™ì‹œì— ëª¨ë¸ë§
4. **ì‹¤ì‹œê°„ í•™ìŠµ**: ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì§€ì†ì  ì—…ë°ì´íŠ¸

## ğŸ› ï¸ ì‹¤ìŠµ: N-BEATSì™€ DeepAR êµ¬í˜„

### 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# pip install torch torchvision torchaudio
# pip install pandas numpy matplotlib seaborn scikit-learn

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# PyTorch ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ì‹œê°í™” ì„¤ì •
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
```

### 2. ë³µì¡í•œ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±

```python
def generate_complex_timeseries(n=2000, seed=42):
    """ë³µì¡í•œ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©)"""
    np.random.seed(seed)
    
    # ì‹œê°„ ì¸ë±ìŠ¤
    t = np.arange(n)
    
    # ë¹„ì„ í˜• íŠ¸ë Œë“œ
    trend = 0.05 * t + 0.0001 * t**2 + 0.0000001 * t**3
    
    # ë‹¤ì¤‘ ì£¼ê¸° ê³„ì ˆì„±
    seasonality_1 = 15 * np.sin(2 * np.pi * t / 365)  # ì—°ê°„
    seasonality_2 = 8 * np.sin(2 * np.pi * t / 7)     # ì£¼ê°„
    seasonality_3 = 3 * np.sin(2 * np.pi * t / 24)    # ì¼ê°„
    seasonality_4 = 5 * np.sin(2 * np.pi * t / 12)    # ì›”ê°„
    
    # êµ¬ì¡°ì  ë³€í™” (ì—¬ëŸ¬ ë²ˆ ë°œìƒ)
    structural_changes = np.zeros(n)
    change_points = [n//4, n//2, 3*n//4]
    for i, cp in enumerate(change_points):
        structural_changes[cp:] += (i+1) * 2 * np.sin(2 * np.pi * t[cp:] / 200)
    
    # ë¹„ì„ í˜• ìƒí˜¸ì‘ìš©
    interaction = 0.1 * np.sin(2 * np.pi * t / 100) * np.cos(2 * np.pi * t / 50)
    
    # ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ë…¸ì´ì¦ˆ
    noise = np.random.normal(0, 1 + 0.02 * t, n)
    
    # ì´ìƒì¹˜ ì¶”ê°€
    outliers = np.random.choice(n, size=n//40, replace=False)
    noise[outliers] += np.random.normal(0, 15, len(outliers))
    
    # ì „ì²´ ì‹œê³„ì—´
    ts = trend + seasonality_1 + seasonality_2 + seasonality_3 + seasonality_4 + structural_changes + interaction + noise
    
    return pd.Series(ts, index=pd.date_range('2020-01-01', periods=n, freq='D'))

# ë³µì¡í•œ ì‹œê³„ì—´ ìƒì„±
complex_ts = generate_complex_timeseries(2000)

# ì‹œê°í™”
plt.figure(figsize=(15, 12))

plt.subplot(4, 1, 1)
plt.plot(complex_ts.index, complex_ts.values, alpha=0.7)
plt.title('ë³µì¡í•œ ì‹œê³„ì—´ ë°ì´í„° (ì „ì²´)')
plt.ylabel('ê°’')

plt.subplot(4, 1, 2)
plt.plot(complex_ts.index[:200], complex_ts.values[:200], alpha=0.7)
plt.title('ì²« 200ì¼ ë°ì´í„° (í™•ëŒ€)')
plt.ylabel('ê°’')

plt.subplot(4, 1, 3)
plt.plot(complex_ts.index[800:1000], complex_ts.values[800:1000], alpha=0.7)
plt.title('800-1000ì¼ ë°ì´í„° (í™•ëŒ€)')
plt.ylabel('ê°’')

plt.subplot(4, 1, 4)
plt.plot(complex_ts.index[-200:], complex_ts.values[-200:], alpha=0.7)
plt.title('ë§ˆì§€ë§‰ 200ì¼ ë°ì´í„° (í™•ëŒ€)')
plt.ylabel('ê°’')

plt.tight_layout()
plt.show()

print(f"ì‹œê³„ì—´ ê¸¸ì´: {len(complex_ts)}")
print(f"ê°’ ë²”ìœ„: {complex_ts.min():.2f} ~ {complex_ts.max():.2f}")
print(f"í‰ê· : {complex_ts.mean():.2f}")
print(f"í‘œì¤€í¸ì°¨: {complex_ts.std():.2f}")
```

### 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±

```python
def create_sequences(data, lookback, forecast_horizon):
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
    sequences = []
    targets = []
    
    for i in range(len(data) - lookback - forecast_horizon + 1):
        seq = data[i:i + lookback]
        target = data[i + lookback:i + lookback + forecast_horizon]
        sequences.append(seq)
        targets.append(target)
    
    return np.array(sequences), np.array(targets)

def normalize_data(data):
    """ë°ì´í„° ì •ê·œí™”"""
    mean = np.mean(data)
    std = np.std(data)
    normalized = (data - mean) / std
    return normalized, mean, std

def denormalize_data(normalized_data, mean, std):
    """ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›"""
    return normalized_data * std + mean

# ë°ì´í„° ì •ê·œí™”
ts_normalized, mean_val, std_val = normalize_data(complex_ts.values)

# ì‹œí€€ìŠ¤ ìƒì„±
lookback = 100  # ê³¼ê±° 100ì¼ ë°ì´í„° ì‚¬ìš©
forecast_horizon = 30  # 30ì¼ ì˜ˆì¸¡
sequences, targets = create_sequences(ts_normalized, lookback, forecast_horizon)

print(f"ì‹œí€€ìŠ¤ ê°œìˆ˜: {len(sequences)}")
print(f"ì…ë ¥ í˜•íƒœ: {sequences.shape}")
print(f"íƒ€ê²Ÿ í˜•íƒœ: {targets.shape}")

# PyTorch í…ì„œë¡œ ë³€í™˜
sequences_tensor = torch.FloatTensor(sequences).to(device)
targets_tensor = torch.FloatTensor(targets).to(device)

# ë°ì´í„°ì…‹ ë¶„í•  (í•™ìŠµ:ê²€ì¦:í…ŒìŠ¤íŠ¸ = 7:2:1)
total_samples = len(sequences)
train_size = int(total_samples * 0.7)
val_size = int(total_samples * 0.2)
test_size = total_samples - train_size - val_size

train_sequences = sequences_tensor[:train_size]
train_targets = targets_tensor[:train_size]
val_sequences = sequences_tensor[train_size:train_size + val_size]
val_targets = targets_tensor[train_size:train_size + val_size]
test_sequences = sequences_tensor[train_size + val_size:]
test_targets = targets_tensor[train_size + val_size:]

print(f"í•™ìŠµ ë°ì´í„°: {len(train_sequences)}")
print(f"ê²€ì¦ ë°ì´í„°: {len(val_sequences)}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_sequences)}")

# DataLoader ìƒì„±
batch_size = 32
train_dataset = TensorDataset(train_sequences, train_targets)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = TensorDataset(val_sequences, val_targets)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

test_dataset = TensorDataset(test_sequences, test_targets)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

### 4. N-BEATS ëª¨ë¸ êµ¬í˜„

```python
class NBEATSBlock(nn.Module):
    """N-BEATS ë¸”ë¡ êµ¬í˜„"""
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(NBEATSBlock, self).__init__()
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_dim)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # ì…ë ¥ì„ ê³ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        h1 = self.relu(self.fc1(x))
        h1 = self.dropout(h1)
        
        h2 = self.relu(self.fc2(h1))
        h2 = self.dropout(h2)
        
        h3 = self.relu(self.fc3(h2))
        h3 = self.dropout(h3)
        
        # ì¶œë ¥ ìƒì„±
        output = self.fc4(h3)
        
        return output

class NBEATS(nn.Module):
    """N-BEATS ëª¨ë¸ êµ¬í˜„"""
    def __init__(self, lookback, forecast_horizon, hidden_dim=128, num_blocks=3):
        super(NBEATS, self).__init__()
        
        self.lookback = lookback
        self.forecast_horizon = forecast_horizon
        self.num_blocks = num_blocks
        
        # N-BEATS ë¸”ë¡ë“¤
        self.blocks = nn.ModuleList([
            NBEATSBlock(lookback, hidden_dim, lookback + forecast_horizon)
            for _ in range(num_blocks)
        ])
        
        # ìµœì¢… ì˜ˆì¸¡ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.final_layer = nn.Linear(lookback, forecast_horizon)
        
    def forward(self, x):
        # ì…ë ¥: (batch_size, lookback)
        batch_size = x.size(0)
        
        # ê° ë¸”ë¡ì„ í†µê³¼í•˜ë©° residual connection
        residual = x
        for i, block in enumerate(self.blocks):
            # ë¸”ë¡ ì¶œë ¥
            block_output = block(residual)
            
            # Backcastì™€ Forecast ë¶„ë¦¬
            backcast = block_output[:, :self.lookback]
            forecast = block_output[:, self.lookback:]
            
            # Residual connection
            residual = residual - backcast
            
            # ì²« ë²ˆì§¸ ë¸”ë¡ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ forecast ëˆ„ì 
            if i == 0:
                total_forecast = forecast
            else:
                total_forecast = total_forecast + forecast
        
        # ìµœì¢… ì˜ˆì¸¡
        final_forecast = self.final_layer(x) + total_forecast
        
        return final_forecast

# N-BEATS ëª¨ë¸ ìƒì„±
nbeats_model = NBEATS(
    lookback=lookback,
    forecast_horizon=forecast_horizon,
    hidden_dim=128,
    num_blocks=3
).to(device)

print(f"N-BEATS ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in nbeats_model.parameters())}")

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.MSELoss()
optimizer = optim.Adam(nbeats_model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)

# í•™ìŠµ í•¨ìˆ˜
def train_epoch(model, dataloader, criterion, optimizer):
    model.train()
    total_loss = 0
    
    for sequences, targets in dataloader:
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(sequences)
        loss = criterion(outputs, targets)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

def validate_epoch(model, dataloader, criterion):
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for sequences, targets in dataloader:
            outputs = model(sequences)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
    
    return total_loss / len(dataloader)

# N-BEATS ëª¨ë¸ í•™ìŠµ
print("=== N-BEATS ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
num_epochs = 100
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    train_loss = train_epoch(nbeats_model, train_loader, criterion, optimizer)
    val_loss = validate_epoch(nbeats_model, val_loader, criterion)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    # Learning rate ìŠ¤ì¼€ì¤„ë§
    scheduler.step(val_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
    
    # Early stopping
    if len(val_losses) > 20 and val_losses[-1] > val_losses[-20]:
        print(f"Early stopping at epoch {epoch+1}")
        break

# í•™ìŠµ ê³¼ì • ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(train_losses, label='Training Loss', alpha=0.7)
plt.plot(val_losses, label='Validation Loss', alpha=0.7)
plt.title('N-BEATS ëª¨ë¸ í•™ìŠµ ê³¼ì •')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.6f}")
```

### 5. DeepAR ëª¨ë¸ êµ¬í˜„

```python
class DeepAR(nn.Module):
    """DeepAR ëª¨ë¸ êµ¬í˜„"""
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(DeepAR, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.output_dim = output_dim
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1
        )
        
        # í™•ë¥  ë¶„í¬ íŒŒë¼ë¯¸í„°ë¥¼ ìœ„í•œ ì¶œë ¥ ë ˆì´ì–´
        self.fc_mu = nn.Linear(hidden_dim, output_dim)
        self.fc_sigma = nn.Linear(hidden_dim, output_dim)
        
        # í™œì„±í™” í•¨ìˆ˜
        self.softplus = nn.Softplus()
        
    def forward(self, x, hidden=None):
        # LSTM ìˆœì „íŒŒ
        lstm_out, hidden = self.lstm(x, hidden)
        
        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©
        last_output = lstm_out[:, -1, :]
        
        # í™•ë¥  ë¶„í¬ íŒŒë¼ë¯¸í„° ê³„ì‚°
        mu = self.fc_mu(last_output)
        sigma = self.softplus(self.fc_sigma(last_output)) + 1e-6  # ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’ ì¶”ê°€
        
        return mu, sigma, hidden
    
    def init_hidden(self, batch_size):
        """ì€ë‹‰ ìƒíƒœ ì´ˆê¸°í™”"""
        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),
                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))

# DeepAR ëª¨ë¸ ìƒì„±
deepar_model = DeepAR(
    input_dim=1,
    hidden_dim=128,
    num_layers=2,
    output_dim=forecast_horizon
).to(device)

print(f"DeepAR ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in deepar_model.parameters())}")

# DeepARìš© ì†ì‹¤ í•¨ìˆ˜ (Negative Log Likelihood)
def nll_loss(mu, sigma, targets):
    """ì •ê·œë¶„í¬ ê°€ì • í•˜ì˜ Negative Log Likelihood ì†ì‹¤"""
    # ì •ê·œë¶„í¬ì˜ NLL ê³„ì‚°
    nll = 0.5 * torch.log(2 * np.pi * sigma**2) + 0.5 * ((targets - mu) / sigma)**2
    return torch.mean(nll)

# DeepAR ëª¨ë¸ í•™ìŠµ
print("=== DeepAR ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
deepar_optimizer = optim.Adam(deepar_model.parameters(), lr=0.001)
deepar_scheduler = optim.lr_scheduler.ReduceLROnPlateau(deepar_optimizer, mode='min', patience=10, factor=0.5)

deepar_train_losses = []
deepar_val_losses = []

for epoch in range(num_epochs):
    # í•™ìŠµ
    deepar_model.train()
    train_loss = 0
    
    for sequences, targets in train_loader:
        deepar_optimizer.zero_grad()
        
        # ì…ë ¥ í˜•íƒœ ì¡°ì • (batch_size, lookback, 1)
        sequences_reshaped = sequences.unsqueeze(-1)
        
        mu, sigma, _ = deepar_model(sequences_reshaped)
        loss = nll_loss(mu, sigma, targets)
        
        loss.backward()
        deepar_optimizer.step()
        
        train_loss += loss.item()
    
    train_loss = train_loss / len(train_loader)
    
    # ê²€ì¦
    deepar_model.eval()
    val_loss = 0
    
    with torch.no_grad():
        for sequences, targets in val_loader:
            sequences_reshaped = sequences.unsqueeze(-1)
            mu, sigma, _ = deepar_model(sequences_reshaped)
            loss = nll_loss(mu, sigma, targets)
            val_loss += loss.item()
    
    val_loss = val_loss / len(val_loader)
    
    deepar_train_losses.append(train_loss)
    deepar_val_losses.append(val_loss)
    
    # Learning rate ìŠ¤ì¼€ì¤„ë§
    deepar_scheduler.step(val_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
    
    # Early stopping
    if len(deepar_val_losses) > 20 and deepar_val_losses[-1] > deepar_val_losses[-20]:
        print(f"Early stopping at epoch {epoch+1}")
        break

# DeepAR í•™ìŠµ ê³¼ì • ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(deepar_train_losses, label='Training Loss', alpha=0.7)
plt.plot(deepar_val_losses, label='Validation Loss', alpha=0.7)
plt.title('DeepAR ëª¨ë¸ í•™ìŠµ ê³¼ì •')
plt.xlabel('Epoch')
plt.ylabel('Loss (NLL)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"DeepAR ìµœì¢… ê²€ì¦ ì†ì‹¤: {deepar_val_losses[-1]:.6f}")
```

### 6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

```python
def evaluate_model(model, dataloader, model_name, is_deepar=False):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for sequences, targets in dataloader:
            if is_deepar:
                # DeepAR ëª¨ë¸
                sequences_reshaped = sequences.unsqueeze(-1)
                mu, sigma, _ = model(sequences_reshaped)
                predictions = mu
            else:
                # N-BEATS ëª¨ë¸
                predictions = model(sequences)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    
    # ì˜ˆì¸¡ê°’ê³¼ íƒ€ê²Ÿì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mse = mean_squared_error(all_targets.flatten(), all_predictions.flatten())
    mae = mean_absolute_error(all_targets.flatten(), all_predictions.flatten())
    rmse = np.sqrt(mse)
    
    print(f"\n{model_name} ì„±ëŠ¥:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    
    return all_predictions, all_targets, (mse, mae, rmse)

# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
print("=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")

# N-BEATS ì„±ëŠ¥
nbeats_pred, nbeats_target, nbeats_metrics = evaluate_model(
    nbeats_model, test_loader, "N-BEATS", is_deepar=False
)

# DeepAR ì„±ëŠ¥
deepar_pred, deepar_target, deepar_metrics = evaluate_model(
    deepar_model, test_loader, "DeepAR", is_deepar=True
)

# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
plt.figure(figsize=(15, 12))

# 1. ì˜ˆì¸¡ vs ì‹¤ì œê°’ ë¹„êµ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
sample_idx = 0

plt.subplot(3, 1, 1)
plt.plot(nbeats_target[sample_idx], label='ì‹¤ì œê°’', linewidth=2, alpha=0.8)
plt.plot(nbeats_pred[sample_idx], label='N-BEATS ì˜ˆì¸¡', color='red', alpha=0.8)
plt.title('N-BEATS: ì˜ˆì¸¡ vs ì‹¤ì œê°’ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)')
plt.xlabel('ì˜ˆì¸¡ ê¸°ê°„')
plt.ylabel('ê°’')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(3, 1, 2)
plt.plot(deepar_target[sample_idx], label='ì‹¤ì œê°’', linewidth=2, alpha=0.8)
plt.plot(deepar_pred[sample_idx], label='DeepAR ì˜ˆì¸¡', color='green', alpha=0.8)
plt.title('DeepAR: ì˜ˆì¸¡ vs ì‹¤ì œê°’ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)')
plt.xlabel('ì˜ˆì¸¡ ê¸°ê°„')
plt.ylabel('ê°’')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. ì „ì²´ ì„±ëŠ¥ ë¹„êµ
plt.subplot(3, 1, 3)
models = ['N-BEATS', 'DeepAR']
rmse_values = [nbeats_metrics[2], deepar_metrics[2]]
mae_values = [nbeats_metrics[1], deepar_metrics[1]]

x = np.arange(len(models))
width = 0.35

plt.bar(x - width/2, rmse_values, width, label='RMSE', alpha=0.8)
plt.bar(x + width/2, mae_values, width, label='MAE', alpha=0.8)

plt.xlabel('ëª¨ë¸')
plt.ylabel('ì˜¤ì°¨')
plt.title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')
plt.xticks(x, models)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
results_df = pd.DataFrame({
    'ëª¨ë¸': ['N-BEATS', 'DeepAR'],
    'MSE': [nbeats_metrics[0], deepar_metrics[0]],
    'MAE': [nbeats_metrics[1], deepar_metrics[1]],
    'RMSE': [nbeats_metrics[2], deepar_metrics[2]]
})

print("\n=== ì„±ëŠ¥ ë¹„êµ ìš”ì•½ ===")
print(results_df.to_string(index=False))

# ìŠ¹ì ê²°ì •
if nbeats_metrics[2] < deepar_metrics[2]:  # RMSE ê¸°ì¤€
    print(f"\nğŸ† N-BEATS ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤! (RMSE: {nbeats_metrics[2]:.4f})")
else:
    print(f"\nğŸ† DeepAR ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤! (RMSE: {deepar_metrics[2]:.4f})")

# 3. í™•ë¥ ì  ì˜ˆì¸¡ ì‹œê°í™” (DeepAR)
if len(test_loader) > 0:
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
    test_batch = next(iter(test_loader))
    test_sequences, test_targets = test_batch
    
    deepar_model.eval()
    with torch.no_grad():
        sequences_reshaped = test_sequences.unsqueeze(-1).to(device)
        mu, sigma, _ = deepar_model(sequences_reshaped)
        
        # ì—¬ëŸ¬ ìƒ˜í”Œ ìƒì„± (Monte Carlo ì‹œë®¬ë ˆì´ì…˜)
        num_samples = 100
        samples = []
        
        for _ in range(num_samples):
            # ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            noise = torch.randn_like(mu)
            sample = mu + sigma * noise
            samples.append(sample.cpu().numpy())
        
        samples = np.array(samples)  # (num_samples, batch_size, forecast_horizon)
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ ì˜ˆì¸¡ êµ¬ê°„ ì‹œê°í™”
        sample_idx = 0
        forecast_horizons = np.arange(forecast_horizon)
        
        plt.figure(figsize=(15, 8))
        
        # ì‹¤ì œê°’
        plt.plot(forecast_horizons, test_targets[sample_idx], 
                label='ì‹¤ì œê°’', linewidth=3, color='black')
        
        # ì˜ˆì¸¡ê°’ (í‰ê· )
        plt.plot(forecast_horizons, mu[sample_idx].cpu().numpy(), 
                label='DeepAR ì˜ˆì¸¡ (í‰ê· )', linewidth=2, color='red')
        
        # ì‹ ë¢° êµ¬ê°„
        percentiles = [5, 25, 75, 95]
        for i, p in enumerate(percentiles):
            percentile_values = np.percentile(samples[:, sample_idx, :], p, axis=0)
            if i < 2:  # í•˜ìœ„ êµ¬ê°„
                plt.fill_between(forecast_horizons, percentile_values, 
                               alpha=0.3, color='blue', 
                               label=f'{p}% ì‹ ë¢° êµ¬ê°„' if i == 0 else '')
            else:  # ìƒìœ„ êµ¬ê°„
                plt.fill_between(forecast_horizons, percentile_values, 
                               alpha=0.3, color='blue')
        
        plt.title('DeepAR í™•ë¥ ì  ì˜ˆì¸¡ ë° ì‹ ë¢° êµ¬ê°„')
        plt.xlabel('ì˜ˆì¸¡ ê¸°ê°„')
        plt.ylabel('ê°’')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        print(f"DeepAR ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±:")
        print(f"  í‰ê·  í‘œì¤€í¸ì°¨: {sigma.mean().item():.4f}")
        print(f"  ìµœëŒ€ í‘œì¤€í¸ì°¨: {sigma.max().item():.4f}")
        print(f"  ìµœì†Œ í‘œì¤€í¸ì°¨: {sigma.min().item():.4f}")
```

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ë° í™•ì¥

### Part 3ì—ì„œ ë‹¤ë£° ë‚´ìš©

1. **Transformer ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡**
   - Informer: íš¨ìœ¨ì ì¸ Attention ë©”ì»¤ë‹ˆì¦˜
   - Autoformer: ìë™í™”ëœ ì‹œê³„ì—´ ë¶„í•´
   - FEDformer: ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ê¸°ë°˜ ì˜ˆì¸¡

2. **ìµœì‹  LLM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡**
   - TimeGPT: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡
   - Lag-Llama: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆ

3. **ì‹¤ë¬´ ì ìš© ë° ìµœì í™”**
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
   - ì•™ìƒë¸” ë°©ë²•
   - ë„ë©”ì¸ë³„ ìµœì í™”

### ì¶”ê°€ ê°œì„  ë°©í–¥

1. **ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ **
   - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
   - Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…
   - Residual connection ìµœì í™”

2. **ë°ì´í„° ì¦ê°•**
   - ë…¸ì´ì¦ˆ ì¶”ê°€
   - ì‹œê³„ì—´ ë³€í˜•
   - ì ëŒ€ì  í•™ìŠµ

3. **ì•™ìƒë¸” ë°©ë²•**
   - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°í•©
   - ê°€ì¤‘ í‰ê· 
   - Stacking

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥ ë°°ê²½**
   - ì „í†µì  ë°©ë²•ì˜ í•œê³„
   - ë”¥ëŸ¬ë‹ì˜ í˜ì‹ ì  íŠ¹ì§•

2. **N-BEATS ëª¨ë¸**
   - ë¸”ë¡ ê¸°ë°˜ ì•„í‚¤í…ì²˜
   - í•´ì„ ê°€ëŠ¥í•œ êµ¬ì¡°
   - ì´ì¤‘ ì¶œë ¥ (Backcast + Forecast)

3. **DeepAR ëª¨ë¸**
   - í™•ë¥ ì  ì˜ˆì¸¡
   - LSTM ê¸°ë°˜ ì•„í‚¤í…ì²˜
   - ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

4. **ì‹¤ì œ êµ¬í˜„**
   - PyTorchë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ êµ¬í˜„
   - ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±
   - ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€

### í•µì‹¬ ê°œë… ì •ë¦¬

| ëª¨ë¸ | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **N-BEATS** | ë¸”ë¡ ê¸°ë°˜, í•´ì„ ê°€ëŠ¥ | ë¹ ë¥¸ í•™ìŠµ, í™•ì¥ì„± | ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ í•œê³„ |
| **DeepAR** | í™•ë¥ ì  ì˜ˆì¸¡, LSTM ê¸°ë°˜ | ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”, ì™¸ë¶€ ë³€ìˆ˜ í™œìš© | í•™ìŠµ ì‹œê°„, ê³„ì‚° ë³µì¡ë„ |

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

1. **ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ**
   - ë‹¨ìˆœí•œ íŒ¨í„´: N-BEATS
   - ë³µì¡í•œ íŒ¨í„´ + ë¶ˆí™•ì‹¤ì„±: DeepAR

2. **ê³„ì‚° ìì› ê³ ë ¤**
   - ì œí•œëœ ìì›: N-BEATS
   - ì¶©ë¶„í•œ ìì›: DeepAR

3. **í•´ì„ ê°€ëŠ¥ì„± ìš”êµ¬ì‚¬í•­**
   - ë†’ì€ ìš”êµ¬ì‚¬í•­: N-BEATS
   - ì¤‘ê°„ ìš”êµ¬ì‚¬í•­: DeepAR

---

*ì´ë²ˆ íŒŒíŠ¸ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì˜ í˜ì„ í™œìš©í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. N-BEATSì˜ í•´ì„ ê°€ëŠ¥í•œ êµ¬ì¡°ì™€ DeepARì˜ í™•ë¥ ì  ì˜ˆì¸¡ì„ í†µí•´ ë”ìš± ì •êµí•œ ì‹œê³„ì—´ ë¶„ì„ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤!* ğŸ¯

*ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” Transformer ê¸°ë°˜ ëª¨ë¸ë“¤ì„ í†µí•´ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ìµœì‹  ì‹œê³„ì—´ ì˜ˆì¸¡ ê¸°ìˆ ì˜ ìµœì „ì„ ì„ ê²½í—˜í•´ë³´ì„¸ìš”!* ğŸš€

---

## ğŸ”— ë‹¤ìŒ íŒŒíŠ¸ë¡œ ì´ë™

**Part 3: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤**  
Informer, Autoformer, FEDformer, PatchTST ë“± ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤.

[**Part 3 ì½ê¸° â†’**](/data-ai/2025/09/06/time-series-transformers.html)

---
layout: post
title: "Part 3: Time Series Database í†µí•©ê³¼ ë°°í¬ - í˜„ëŒ€ì  TDB ìƒíƒœê³„ ì™„ì„±"
description: "TDBì™€ ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ë¶€í„° í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜, ìµœì‹  íŠ¸ë Œë“œ, ê·¸ë¦¬ê³  ì‹¤ì œ í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ í˜„ëŒ€ì  TDB ìƒíƒœê³„ì˜ ì™„ì„±ëœ ëª¨ìŠµì„ ë‹¤ë£¹ë‹ˆë‹¤."
excerpt: "TDBì™€ ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ë¶€í„° í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜, ìµœì‹  íŠ¸ë Œë“œ, ê·¸ë¦¬ê³  ì‹¤ì œ í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ í˜„ëŒ€ì  TDB ìƒíƒœê³„ì˜ ì™„ì„±ëœ ëª¨ìŠµì„ ë‹¤ë£¹ë‹ˆë‹¤"
category: data-engineering
tags: [TimeSeriesDatabase, ì‹œìŠ¤í…œí†µí•©, í´ë¼ìš°ë“œë„¤ì´í‹°ë¸Œ, í”„ë¡œë•ì…˜ë°°í¬, í˜„ëŒ€ì ì•„í‚¤í…ì²˜, ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤, DevOps]
series: time-series-database-master
series_order: 3
date: 2025-09-29
author: Data Droid
lang: ko
reading_time: "60ë¶„"
difficulty: "ê³ ê¸‰"
---

## ğŸŒŸ ì†Œê°œ

Part 1ê³¼ Part 2ì—ì„œ TDBì˜ ê¸°ì´ˆë¶€í„° ê³ ê¸‰ ìµœì í™”ê¹Œì§€ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì´ì œ Part 3ì—ì„œëŠ” í˜„ëŒ€ì  TDB ìƒíƒœê³„ì˜ ì™„ì„±ëœ ëª¨ìŠµì„ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.

### ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ë°°ìš¸ ë‚´ìš©

- **ì‹œìŠ¤í…œ í†µí•©**: TDBì™€ ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™
- **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: Kubernetes ê¸°ë°˜ TDB ë°°í¬
- **ìµœì‹  íŠ¸ë Œë“œ**: Edge Computing, AI/ML í†µí•©
- **í”„ë¡œë•ì…˜ ë°°í¬**: ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ ì „ëµ
- **ì™„ì„±ëœ í”„ë¡œì íŠ¸**: ì „ì²´ TDB ì‹œìŠ¤í…œ êµ¬ì¶•

---

## ğŸ”— TDBì™€ ë‹¤ë¥¸ ì‹œìŠ¤í…œ í†µí•© {#tdbì™€-ë‹¤ë¥¸-ì‹œìŠ¤í…œ-í†µí•©}

### ë°ì´í„° íŒŒì´í”„ë¼ì¸ í†µí•©

#### 1. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í†µí•©

| í†µí•© ëŒ€ìƒ | ê¸°ìˆ  ìŠ¤íƒ | í†µí•© ë°©ì‹ | ì„±ëŠ¥ ëª©í‘œ | ë³µì¡ë„ |
|-----------|-----------|-----------|-----------|--------|
| **Apache Kafka** | Kafka Connect, InfluxDB Sink | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ | < 100ms ì§€ì—° | ì¤‘ê°„ |
| **Apache Flink** | Flink InfluxDB Connector | ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì—”ì§„ | < 50ms ì§€ì—° | ë†’ìŒ |
| **Apache Spark** | Spark InfluxDB Connector | ë°°ì¹˜ + ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ | < 1ì´ˆ ì§€ì—° | ì¤‘ê°„ |
| **Apache Pulsar** | Pulsar IO Connector | ë¶„ì‚° ë©”ì‹œì§• | < 10ms ì§€ì—° | ì¤‘ê°„ |

#### 2. ë°ì´í„°ë² ì´ìŠ¤ í†µí•©

| ë°ì´í„°ë² ì´ìŠ¤ | í†µí•© ë°©ì‹ | ë™ê¸°í™” ë°©ì‹ | ì¼ê´€ì„± | ì„±ëŠ¥ |
|--------------|-----------|-------------|--------|------|
| **PostgreSQL** | TimescaleDB í™•ì¥ | ì‹¤ì‹œê°„ ë™ê¸°í™” | ê°•í•¨ | ë†’ìŒ |
| **MySQL** | CDC (Change Data Capture) | ê·¼ì‹¤ì‹œê°„ ë™ê¸°í™” | ì¤‘ê°„ | ì¤‘ê°„ |
| **MongoDB** | Change Streams | ì‹¤ì‹œê°„ ë™ê¸°í™” | ì•½í•¨ | ë†’ìŒ |
| **ClickHouse** | MaterializedView | ë°°ì¹˜ ë™ê¸°í™” | ê°•í•¨ | ë§¤ìš° ë†’ìŒ |

#### 3. í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ í†µí•©

| í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ | í†µí•© ë°©ì‹ | ê´€ë¦¬ ë°©ì‹ | ë¹„ìš© íš¨ìœ¨ | í™•ì¥ì„± |
|----------------|-----------|-----------|-----------|--------|
| **AWS Timestream** | ë„¤ì´í‹°ë¸Œ ì„œë¹„ìŠ¤ | ì™„ì „ ê´€ë¦¬í˜• | ë†’ìŒ | ìë™ |
| **Azure Time Series Insights** | PaaS ì„œë¹„ìŠ¤ | ë°˜ê´€ë¦¬í˜• | ì¤‘ê°„ | ìë™ |
| **Google Cloud IoT Core** | GCP ì„œë¹„ìŠ¤ | ì™„ì „ ê´€ë¦¬í˜• | ë†’ìŒ | ìë™ |
| **Snowflake** | ì™¸ë¶€ í…Œì´ë¸” | í•˜ì´ë¸Œë¦¬ë“œ | ë‚®ìŒ | ìˆ˜ë™ |

### API ë° ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í†µí•©

#### 1. REST API í†µí•©

| API íŒ¨í„´ | ì„¤ëª… | êµ¬í˜„ ë°©ì‹ | ì„±ëŠ¥ | ë³´ì•ˆ |
|----------|------|-----------|------|------|
| **GraphQL** | ìœ ì—°í•œ ì¿¼ë¦¬ API | Schema ì •ì˜ | ì¤‘ê°„ | ê°•í•¨ |
| **RESTful API** | í‘œì¤€ REST ì¸í„°í˜ì´ìŠ¤ | OpenAPI ìŠ¤í™ | ë†’ìŒ | ê°•í•¨ |
| **gRPC** | ê³ ì„±ëŠ¥ RPC | Protocol Buffers | ë§¤ìš° ë†’ìŒ | ê°•í•¨ |
| **WebSocket** | ì‹¤ì‹œê°„ ì–‘ë°©í–¥ í†µì‹  | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ | ë†’ìŒ | ì¤‘ê°„ |

#### 2. ì„œë¹„ìŠ¤ ë©”ì‹œ í†µí•©

| ì„œë¹„ìŠ¤ ë©”ì‹œ | ì„¤ëª… | í†µí•© ë°©ì‹ | ê´€ì°°ì„± | ë³´ì•ˆ |
|-------------|------|-----------|--------|------|
| **Istio** | ì„œë¹„ìŠ¤ ë©”ì‹œ í”Œë«í¼ | Sidecar íŒ¨í„´ | ë§¤ìš° ë†’ìŒ | ë§¤ìš° ê°•í•¨ |
| **Linkerd** | ê²½ëŸ‰ ì„œë¹„ìŠ¤ ë©”ì‹œ | í”„ë¡ì‹œ ê¸°ë°˜ | ë†’ìŒ | ê°•í•¨ |
| **Consul Connect** | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ | í”„ë¡ì‹œ ê¸°ë°˜ | ì¤‘ê°„ | ê°•í•¨ |
| **AWS App Mesh** | AWS ë„¤ì´í‹°ë¸Œ | Envoy í”„ë¡ì‹œ | ë†’ìŒ | ê°•í•¨ |

---

## â˜ï¸ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ TDB ì•„í‚¤í…ì²˜ {#í´ë¼ìš°ë“œ-ë„¤ì´í‹°ë¸Œ-tdb-ì•„í‚¤í…ì²˜}

### ì‹¤ìŠµ: ì™„ì „í•œ TDB ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ êµ¬í˜„

#### 1. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ êµ¬ì¡° ì„¤ê³„

```python
# microservices/timeseries-service/main.py
from fastapi import FastAPI, HTTPException, Depends
from influxdb_client import InfluxDBClient
from pydantic import BaseModel
from typing import List, Optional
import asyncio
import json
from datetime import datetime, timedelta
import logging

app = FastAPI(title="Time Series Service", version="1.0.0")

# ì„¤ì •
INFLUXDB_URL = "http://influxdb:8086"
INFLUXDB_TOKEN = "admin-token"
INFLUXDB_ORG = "myorg"
INFLUXDB_BUCKET = "mybucket"

# InfluxDB í´ë¼ì´ì–¸íŠ¸
influx_client = InfluxDBClient(
    url=INFLUXDB_URL,
    token=INFLUXDB_TOKEN,
    org=INFLUXDB_ORG
)

class TimeSeriesData(BaseModel):
    measurement: str
    tags: dict
    fields: dict
    timestamp: Optional[datetime] = None

class QueryRequest(BaseModel):
    start: datetime
    stop: datetime
    measurement: str
    filters: Optional[dict] = None
    aggregation: Optional[str] = None

class TimeSeriesService:
    def __init__(self, client: InfluxDBClient):
        self.client = client
        self.write_api = client.write_api()
        self.query_api = client.query_api()
    
    async def write_data(self, data: TimeSeriesData) -> bool:
        """ì‹œê³„ì—´ ë°ì´í„° ì“°ê¸°"""
        try:
            point = {
                "measurement": data.measurement,
                "tags": data.tags,
                "fields": data.fields,
                "time": data.timestamp or datetime.utcnow()
            }
            
            self.write_api.write(
                bucket=INFLUXDB_BUCKET,
                record=point
            )
            
            return True
        except Exception as e:
            logging.error(f"ë°ì´í„° ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    async def query_data(self, request: QueryRequest) -> List[dict]:
        """ì‹œê³„ì—´ ë°ì´í„° ì¿¼ë¦¬"""
        try:
            # Flux ì¿¼ë¦¬ êµ¬ì„±
            flux_query = f'''
            from(bucket: "{INFLUXDB_BUCKET}")
            |> range(start: {request.start.isoformat()}, stop: {request.stop.isoformat()})
            |> filter(fn: (r) => r._measurement == "{request.measurement}")
            '''
            
            # í•„í„° ì¶”ê°€
            if request.filters:
                for key, value in request.filters.items():
                    flux_query += f'|> filter(fn: (r) => r.{key} == "{value}")\n'
            
            # ì§‘ê³„ ì¶”ê°€
            if request.aggregation:
                flux_query += f'|> {request.aggregation}()\n'
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            result = self.query_api.query(flux_query)
            
            # ê²°ê³¼ ë³€í™˜
            data_points = []
            for table in result:
                for record in table.records:
                    data_points.append({
                        "time": record.get_time(),
                        "measurement": record.get_measurement(),
                        "field": record.get_field(),
                        "value": record.get_value(),
                        "tags": record.values
                    })
            
            return data_points
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=str(e))

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
ts_service = TimeSeriesService(influx_client)

@app.post("/data")
async def write_timeseries_data(data: TimeSeriesData):
    """ì‹œê³„ì—´ ë°ì´í„° ì“°ê¸° ì—”ë“œí¬ì¸íŠ¸"""
    success = await ts_service.write_data(data)
    if success:
        return {"status": "success", "message": "ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"}
    else:
        raise HTTPException(status_code=500, detail="ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")

@app.post("/query")
async def query_timeseries_data(request: QueryRequest):
    """ì‹œê³„ì—´ ë°ì´í„° ì¿¼ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    data = await ts_service.query_data(request)
    return {
        "status": "success",
        "data": data,
        "count": len(data)
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.utcnow()}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 2. Docker Composeë¡œ ì „ì²´ ìŠ¤íƒ ë°°í¬

```yaml
# docker-compose.yml
version: '3.8'

services:
  # InfluxDB í´ëŸ¬ìŠ¤í„°
  influxdb:
    image: influxdb:2.7-alpine
    container_name: influxdb
    ports:
      - "8086:8086"
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=admin123
      - DOCKER_INFLUXDB_INIT_ORG=myorg
      - DOCKER_INFLUXDB_INIT_BUCKET=mybucket
    volumes:
      - influxdb_data:/var/lib/influxdb2
      - influxdb_config:/etc/influxdb2
    networks:
      - timeseries-network

  # TimescaleDB (ë¹„êµìš©)
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: timescaledb
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=timeseries
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
    networks:
      - timeseries-network

  # Kafka (ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°)
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - timeseries-network

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - timeseries-network

  # Time Series Service
  timeseries-service:
    build: ./microservices/timeseries-service
    container_name: timeseries-service
    ports:
      - "8000:8000"
    depends_on:
      - influxdb
      - kafka
    environment:
      - INFLUXDB_URL=http://influxdb:8086
      - INFLUXDB_TOKEN=admin-token
      - KAFKA_BROKERS=kafka:9092
    networks:
      - timeseries-network

  # Data Ingestion Service
  data-ingestion:
    build: ./microservices/data-ingestion
    container_name: data-ingestion
    depends_on:
      - kafka
      - influxdb
    environment:
      - KAFKA_BROKERS=kafka:9092
      - INFLUXDB_URL=http://influxdb:8086
    networks:
      - timeseries-network

  # Analytics Service
  analytics-service:
    build: ./microservices/analytics-service
    container_name: analytics-service
    ports:
      - "8001:8001"
    depends_on:
      - influxdb
      - timescaledb
    environment:
      - INFLUXDB_URL=http://influxdb:8086
      - TIMESCALEDB_URL=postgresql://postgres:postgres@timescaledb:5432/timeseries
    networks:
      - timeseries-network

  # Grafana (ì‹œê°í™”)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - timeseries-network

  # Prometheus (ëª¨ë‹ˆí„°ë§)
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - timeseries-network

volumes:
  influxdb_data:
  influxdb_config:
  timescaledb_data:
  grafana_data:
  prometheus_data:

networks:
  timeseries-network:
    driver: bridge
```

#### 3. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# microservices/data-ingestion/main.py
import asyncio
import json
import logging
from datetime import datetime
from kafka import KafkaConsumer, KafkaProducer
from influxdb_client import InfluxDBClient
import random

class DataIngestionPipeline:
    def __init__(self):
        self.kafka_consumer = KafkaConsumer(
            'sensor-data',
            bootstrap_servers=['kafka:9092'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            group_id='data-ingestion-group'
        )
        
        self.kafka_producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        
        self.influx_client = InfluxDBClient(
            url="http://influxdb:8086",
            token="admin-token",
            org="myorg"
        )
        
        self.write_api = self.influx_client.write_api()
    
    async def generate_sensor_data(self):
        """ì„¼ì„œ ë°ì´í„° ìƒì„± ì‹œë®¬ë ˆì´í„°"""
        sensor_types = ['temperature', 'humidity', 'pressure', 'vibration']
        locations = ['seoul', 'busan', 'daegu', 'incheon']
        
        while True:
            data = {
                'timestamp': datetime.utcnow().isoformat(),
                'sensor_type': random.choice(sensor_types),
                'location': random.choice(locations),
                'value': round(random.uniform(20, 30), 2),
                'quality': random.randint(80, 100)
            }
            
            # Kafkaë¡œ ì „ì†¡
            self.kafka_producer.send('sensor-data', data)
            
            await asyncio.sleep(1)  # 1ì´ˆë§ˆë‹¤ ë°ì´í„° ìƒì„±
    
    async def process_kafka_data(self):
        """Kafkaì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì„œ InfluxDBì— ì €ì¥"""
        for message in self.kafka_consumer:
            try:
                data = message.value
                
                # ë°ì´í„° ê²€ì¦
                if self.validate_data(data):
                    # InfluxDBì— ì €ì¥
                    await self.store_to_influxdb(data)
                    
                    # ë¶„ì„ìš©ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„° ì „ì†¡
                    processed_data = self.process_data(data)
                    self.kafka_producer.send('processed-data', processed_data)
                    
                else:
                    logging.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°: {data}")
                    
            except Exception as e:
                logging.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def validate_data(self, data):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        required_fields = ['timestamp', 'sensor_type', 'location', 'value', 'quality']
        
        if not all(field in data for field in required_fields):
            return False
        
        # ê°’ ë²”ìœ„ ê²€ì¦
        if data['value'] < -50 or data['value'] > 100:
            return False
        
        if data['quality'] < 0 or data['quality'] > 100:
            return False
        
        return True
    
    async def store_to_influxdb(self, data):
        """InfluxDBì— ë°ì´í„° ì €ì¥"""
        try:
            point = {
                "measurement": "sensor_data",
                "tags": {
                    "sensor_type": data['sensor_type'],
                    "location": data['location']
                },
                "fields": {
                    "value": data['value'],
                    "quality": data['quality']
                },
                "time": data['timestamp']
            }
            
            self.write_api.write(
                bucket="mybucket",
                record=point
            )
            
            logging.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {data['sensor_type']} at {data['location']}")
            
        except Exception as e:
            logging.error(f"InfluxDB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def process_data(self, data):
        """ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜"""
        processed = data.copy()
        
        # ì´ìƒì¹˜ íƒì§€
        if data['value'] > 35 or data['value'] < 15:
            processed['anomaly'] = True
            processed['anomaly_type'] = 'outlier'
        else:
            processed['anomaly'] = False
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜
        if data['quality'] >= 95:
            processed['quality_level'] = 'excellent'
        elif data['quality'] >= 85:
            processed['quality_level'] = 'good'
        else:
            processed['quality_level'] = 'poor'
        
        processed['processed_at'] = datetime.utcnow().isoformat()
        
        return processed
    
    async def run(self):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logging.info("ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        # ë³‘ë ¬ ì‹¤í–‰
        await asyncio.gather(
            self.generate_sensor_data(),
            self.process_kafka_data()
        )

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    pipeline = DataIngestionPipeline()
    asyncio.run(pipeline.run())
```

#### 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# microservices/monitoring/main.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import asyncio
import aiohttp
from datetime import datetime, timedelta

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
write_requests_total = Counter('influxdb_write_requests_total', 'Total write requests')
write_request_duration = Histogram('influxdb_write_request_duration_seconds', 'Write request duration')
query_requests_total = Counter('influxdb_query_requests_total', 'Total query requests')
query_request_duration = Histogram('influxdb_query_request_duration_seconds', 'Query request duration')
data_points_total = Counter('data_points_processed_total', 'Total data points processed')
system_health = Gauge('system_health_score', 'System health score (0-100)')

class TDBMonitoring:
    def __init__(self):
        self.influx_url = "http://influxdb:8086"
        self.headers = {
            'Authorization': 'Token admin-token',
            'Content-Type': 'application/json'
        }
    
    async def collect_metrics(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        while True:
            try:
                # InfluxDB ìƒíƒœ í™•ì¸
                async with aiohttp.ClientSession() as session:
                    # í—¬ìŠ¤ ì²´í¬
                    async with session.get(f"{self.influx_url}/health", headers=self.headers) as response:
                        if response.status == 200:
                            system_health.set(100)
                        else:
                            system_health.set(50)
                    
                    # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ í™•ì¸
                    query = {
                        'query': 'from(bucket: "mybucket") |> range(start: -1h) |> count()'
                    }
                    
                    start_time = time.time()
                    async with session.post(f"{self.influx_url}/api/v2/query", headers=self.headers, json=query) as response:
                        duration = time.time() - start_time
                        
                        if response.status == 200:
                            query_requests_total.inc()
                            query_request_duration.observe(duration)
                            
                            # ê²°ê³¼ì—ì„œ ì¹´ìš´íŠ¸ ì¶”ì¶œ
                            result = await response.json()
                            if result and 'results' in result:
                                for table in result['results'][0]['series']:
                                    for row in table['values']:
                                        count = row[1] if len(row) > 1 else 0
                                        data_points_total.inc(count)
                
                # 30ì´ˆë§ˆë‹¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                await asyncio.sleep(30)
                
            except Exception as e:
                print(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                system_health.set(0)
                await asyncio.sleep(30)

# ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
start_http_server(8000)

# ëª¨ë‹ˆí„°ë§ ì‹œì‘
monitor = TDBMonitoring()
asyncio.run(monitor.collect_metrics())
```

### Kubernetes ê¸°ë°˜ ë°°í¬

#### 1. ì»¨í…Œì´ë„ˆí™” ì „ëµ

| êµ¬ì„±ìš”ì†Œ | ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ | ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ | ìŠ¤ì¼€ì¼ë§ ì „ëµ |
|----------|-----------------|-----------------|---------------|
| **InfluxDB** | influxdb:2.7-alpine | CPU: 2 cores, Memory: 4GB | ìˆ˜í‰ í™•ì¥ |
| **TimescaleDB** | timescale/timescaledb:latest | CPU: 4 cores, Memory: 8GB | ìˆ˜ì§ í™•ì¥ |
| **Prometheus** | prom/prometheus:latest | CPU: 1 core, Memory: 2GB | ìˆ˜í‰ í™•ì¥ |
| **Grafana** | grafana/grafana:latest | CPU: 1 core, Memory: 1GB | ìˆ˜í‰ í™•ì¥ |

#### 2. Kubernetes ë¦¬ì†ŒìŠ¤ êµ¬ì„±

| ë¦¬ì†ŒìŠ¤ íƒ€ì… | êµ¬ì„± | ìš©ë„ | ê´€ë¦¬ ë°©ì‹ |
|-------------|------|------|-----------|
| **Deployment** | Stateless ì„œë¹„ìŠ¤ | API ì„œë²„, ì›¹ ì¸í„°í˜ì´ìŠ¤ | Rolling Update |
| **StatefulSet** | Stateful ì„œë¹„ìŠ¤ | ë°ì´í„°ë² ì´ìŠ¤, ë©”ì‹œì§€ í | Ordered Deployment |
| **DaemonSet** | ë…¸ë“œë³„ ì„œë¹„ìŠ¤ | ë¡œê·¸ ìˆ˜ì§‘, ëª¨ë‹ˆí„°ë§ | ëª¨ë“  ë…¸ë“œ ë°°í¬ |
| **Job/CronJob** | ë°°ì¹˜ ì‘ì—… | ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜, ë°±ì—… | ì¼íšŒì„±/ì£¼ê¸°ì  ì‹¤í–‰ |

#### 3. ìŠ¤í† ë¦¬ì§€ ì „ëµ

| ìŠ¤í† ë¦¬ì§€ íƒ€ì… | ìš©ë„ | ì„±ëŠ¥ | ë¹„ìš© | ì§€ì†ì„± |
|---------------|------|------|------|--------|
| **SSD Persistent Volume** | í•« ë°ì´í„° | ë†’ìŒ | ë†’ìŒ | ë†’ìŒ |
| **HDD Persistent Volume** | ì›œ ë°ì´í„° | ì¤‘ê°„ | ì¤‘ê°„ | ë†’ìŒ |
| **Object Storage** | ì½œë“œ ë°ì´í„° | ë‚®ìŒ | ë‚®ìŒ | ë§¤ìš° ë†’ìŒ |
| **Memory Storage** | ìºì‹œ ë°ì´í„° | ë§¤ìš° ë†’ìŒ | ë†’ìŒ | ì—†ìŒ |

### Helm ì°¨íŠ¸ ë°°í¬

#### 1. ì°¨íŠ¸ êµ¬ì¡°

| ì°¨íŠ¸ êµ¬ì„± | ì„¤ëª… | ê´€ë¦¬ ë²”ìœ„ | ì—…ë°ì´íŠ¸ ë°©ì‹ |
|-----------|------|-----------|---------------|
| **Core Charts** | ê¸°ë³¸ TDB ì„œë¹„ìŠ¤ | í•µì‹¬ ê¸°ëŠ¥ | ìˆ˜ë™ |
| **Addon Charts** | í™•ì¥ ê¸°ëŠ¥ | ëª¨ë‹ˆí„°ë§, ë°±ì—… | ìë™ |
| **Custom Charts** | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ | ì• í”Œë¦¬ì¼€ì´ì…˜ | CI/CD |
| **Dependency Charts** | ì˜ì¡´ì„± ì„œë¹„ìŠ¤ | ì¸í”„ë¼ | ë²„ì „ ê´€ë¦¬ |

#### 2. ë°°í¬ ì „ëµ

| ë°°í¬ ë°©ì‹ | ì„¤ëª… | ë‹¤ìš´íƒ€ì„ | ë¡¤ë°± ì‹œê°„ | ë³µì¡ë„ |
|-----------|------|----------|-----------|--------|
| **Blue-Green** | ì „ì²´ í™˜ê²½ êµì²´ | 1-5ë¶„ | ì¦‰ì‹œ | ë†’ìŒ |
| **Canary** | ì ì§„ì  íŠ¸ë˜í”½ ì´ë™ | 0ì´ˆ | 1-5ë¶„ | ë†’ìŒ |
| **Rolling Update** | ìˆœì°¨ì  ì—…ë°ì´íŠ¸ | 0ì´ˆ | 5-10ë¶„ | ì¤‘ê°„ |
| **A/B Testing** | ë²„ì „ë³„ í…ŒìŠ¤íŠ¸ | 0ì´ˆ | ì¦‰ì‹œ | ë§¤ìš° ë†’ìŒ |

---

## ğŸš€ ìµœì‹  TDB íŠ¸ë Œë“œì™€ ê¸°ìˆ  {#ìµœì‹ -tdb-íŠ¸ë Œë“œì™€-ê¸°ìˆ }

### Edge Computing í†µí•©

#### 1. Edge TDB ì•„í‚¤í…ì²˜

| Edge ê³„ì¸µ | ì—­í•  | ê¸°ìˆ  ìŠ¤íƒ | ì²˜ë¦¬ ìš©ëŸ‰ | ì§€ì—°ì‹œê°„ |
|-----------|------|-----------|-----------|----------|
| **Device Edge** | ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ | InfluxDB Edge, SQLite | 1K points/sec | < 1ms |
| **Gateway Edge** | ë¡œì»¬ ì§‘ê³„ ì²˜ë¦¬ | InfluxDB OSS, Redis | 10K points/sec | < 10ms |
| **Regional Edge** | ì§€ì—­ë³„ ë°ì´í„° ì²˜ë¦¬ | TimescaleDB, PostgreSQL | 100K points/sec | < 100ms |
| **Cloud Edge** | ì „ì—­ ë°ì´í„° í†µí•© | Cloud TDB Services | 1M+ points/sec | < 1ì´ˆ |

#### 2. í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜

| ì•„í‚¤í…ì²˜ íŒ¨í„´ | ì„¤ëª… | ì¥ì  | ë‹¨ì  | ì ìš© ì‚¬ë¡€ |
|---------------|------|------|------|-----------|
| **Edge-First** | Edgeì—ì„œ ì£¼ìš” ì²˜ë¦¬ | ë‚®ì€ ì§€ì—°ì‹œê°„, ì˜¤í”„ë¼ì¸ ë™ì‘ | ë³µì¡í•œ ê´€ë¦¬ | IoT ì„¼ì„œ |
| **Cloud-First** | í´ë¼ìš°ë“œ ì¤‘ì‹¬ ì²˜ë¦¬ | ë‹¨ìˆœí•œ ê´€ë¦¬, ë†’ì€ ì²˜ë¦¬ëŸ‰ | ë†’ì€ ì§€ì—°ì‹œê°„ | ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ |
| **Hybrid** | Edgeì™€ í´ë¼ìš°ë“œ ì¡°í•© | ê· í˜•ì¡íŒ ì„±ëŠ¥ | ë³µì¡í•œ ë™ê¸°í™” | ìŠ¤ë§ˆíŠ¸ ì‹œí‹° |
| **Multi-Cloud** | ë‹¤ì¤‘ í´ë¼ìš°ë“œ ì‚¬ìš© | ë²¤ë” ë½ì¸ ë°©ì§€ | ë†’ì€ ë³µì¡ë„ | ì—”í„°í”„ë¼ì´ì¦ˆ |

### AI/ML í†µí•©

#### 1. ì‹¤ì‹œê°„ ML íŒŒì´í”„ë¼ì¸

| ML ë‹¨ê³„ | ê¸°ìˆ  ìŠ¤íƒ | ì²˜ë¦¬ ë°©ì‹ | ì§€ì—°ì‹œê°„ | ì •í™•ë„ |
|---------|-----------|-----------|----------|--------|
| **ë°ì´í„° ìˆ˜ì§‘** | Kafka, InfluxDB | ìŠ¤íŠ¸ë¦¬ë° | < 10ms | 100% |
| **íŠ¹ì„± ì¶”ì¶œ** | Apache Flink, Python | ì‹¤ì‹œê°„ ì²˜ë¦¬ | < 100ms | 95% |
| **ëª¨ë¸ ì¶”ë¡ ** | TensorFlow Serving, ONNX | ì‹¤ì‹œê°„ ì¶”ë¡  | < 50ms | 90% |
| **ê²°ê³¼ ì €ì¥** | InfluxDB, Redis | ì‹¤ì‹œê°„ ì €ì¥ | < 10ms | 100% |

#### 2. ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸

| ëª¨ë¸ íƒ€ì… | ì„¤ëª… | ì •í™•ë„ | ì²˜ë¦¬ ì†ë„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|-----------|------|--------|-----------|---------------|
| **ARIMA** | ì „í†µì  í†µê³„ ëª¨ë¸ | 70-80% | ë¹ ë¦„ | ë‚®ìŒ |
| **LSTM** | ë”¥ëŸ¬ë‹ ëª¨ë¸ | 80-90% | ì¤‘ê°„ | ì¤‘ê°„ |
| **Transformer** | ì–´í…ì…˜ ê¸°ë°˜ ëª¨ë¸ | 85-95% | ëŠë¦¼ | ë†’ìŒ |
| **Prophet** | Facebook ì‹œê³„ì—´ ëª¨ë¸ | 75-85% | ì¤‘ê°„ | ë‚®ìŒ |

### ì„œë²„ë¦¬ìŠ¤ TDB

#### 1. ì„œë²„ë¦¬ìŠ¤ ì•„í‚¤í…ì²˜

| ì„œë¹„ìŠ¤ íƒ€ì… | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ | ë¹„ìš© ëª¨ë¸ | í™•ì¥ì„± |
|-------------|------|-----------|-----------|--------|
| **AWS Lambda + Timestream** | ì„œë²„ë¦¬ìŠ¤ í•¨ìˆ˜ + TDB | ì‹¤ì‹œê°„ ì•Œë¦¼ | ì‹¤í–‰ ì‹œê°„ | ìë™ |
| **Azure Functions + Time Series** | ì„œë²„ë¦¬ìŠ¤ + ì‹œê³„ì—´ DB | IoT ë°ì´í„° ì²˜ë¦¬ | ì‹¤í–‰ ì‹œê°„ | ìë™ |
| **Google Cloud Functions + BigQuery** | ì„œë²„ë¦¬ìŠ¤ + ë¶„ì„ DB | ë°°ì¹˜ ë¶„ì„ | ì‹¤í–‰ ì‹œê°„ | ìë™ |
| **Knative + InfluxDB** | ì„œë²„ë¦¬ìŠ¤ í”Œë«í¼ | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ | ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ | ìˆ˜ë™ |

#### 2. ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜

| ì´ë²¤íŠ¸ íŒ¨í„´ | ì„¤ëª… | êµ¬í˜„ ë°©ì‹ | ì¥ì  | ë‹¨ì  |
|-------------|------|-----------|------|------|
| **Event Sourcing** | ëª¨ë“  ìƒíƒœ ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì €ì¥ | Event Store + TDB | ì™„ì „í•œ ê°ì‚¬ ì¶”ì  | ë³µì¡í•œ êµ¬í˜„ |
| **CQRS** | ëª…ë ¹ê³¼ ì¿¼ë¦¬ ë¶„ë¦¬ | Write DB + Read DB | ì„±ëŠ¥ ìµœì í™” | ë°ì´í„° ì¼ê´€ì„± |
| **Saga Pattern** | ë¶„ì‚° íŠ¸ëœì­ì…˜ ê´€ë¦¬ | ì´ë²¤íŠ¸ ê¸°ë°˜ ì¡°ì • | ë†’ì€ ê°€ìš©ì„± | ë³µì¡í•œ ë³µêµ¬ |
| **Event Streaming** | ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ | Kafka + TDB | ì‹¤ì‹œê°„ ì²˜ë¦¬ | ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥ |

---

## ğŸ—ï¸ í”„ë¡œë•ì…˜ ë°°í¬ ì „ëµ {#í”„ë¡œë•ì…˜-ë°°í¬-ì „ëµ}

### ë°°í¬ í™˜ê²½ êµ¬ì„±

#### 1. í™˜ê²½ë³„ êµ¬ì„±

| í™˜ê²½ | ëª©ì  | êµ¬ì„± | ë°ì´í„° | ì ‘ê·¼ ê¶Œí•œ |
|------|------|------|--------|-----------|
| **Development** | ê°œë°œ ë° í…ŒìŠ¤íŠ¸ | ìµœì†Œ êµ¬ì„± | ìƒ˜í”Œ ë°ì´í„° | ê°œë°œì ì „ìš© |
| **Staging** | í†µí•© í…ŒìŠ¤íŠ¸ | í”„ë¡œë•ì…˜ ìœ ì‚¬ | í”„ë¡œë•ì…˜ ë³µì‚¬ë³¸ | QA íŒ€ |
| **Production** | ì‹¤ì œ ì„œë¹„ìŠ¤ | ì™„ì „ êµ¬ì„± | ì‹¤ì œ ë°ì´í„° | ì œí•œì  ì ‘ê·¼ |
| **Disaster Recovery** | ì¬í•´ ë³µêµ¬ | ë°±ì—… êµ¬ì„± | ë°±ì—… ë°ì´í„° | ìš´ì˜íŒ€ |

#### 2. ì¸í”„ë¼ êµ¬ì„±

| ì¸í”„ë¼ ì˜ì—­ | Development | Staging | Production | DR |
|-------------|-------------|---------|------------|-----|
| **ì„œë²„ ìˆ˜** | 3ëŒ€ | 5ëŒ€ | 20ëŒ€ | 10ëŒ€ |
| **CPU/ì„œë²„** | 2 cores | 4 cores | 8 cores | 4 cores |
| **Memory/ì„œë²„** | 4GB | 8GB | 32GB | 16GB |
| **Storage/ì„œë²„** | 100GB | 500GB | 2TB | 1TB |
| **ë„¤íŠ¸ì›Œí¬** | 1Gbps | 10Gbps | 40Gbps | 10Gbps |

### CI/CD íŒŒì´í”„ë¼ì¸

#### 1. íŒŒì´í”„ë¼ì¸ ë‹¨ê³„

| ë‹¨ê³„ | ì‘ì—… | ë„êµ¬ | ì‹œê°„ | ìŠ¹ì¸ |
|------|------|------|------|------|
| **Build** | ì½”ë“œ ì»´íŒŒì¼, í…ŒìŠ¤íŠ¸ | Jenkins, GitLab CI | 5ë¶„ | ìë™ |
| **Test** | ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸ | JUnit, TestNG | 10ë¶„ | ìë™ |
| **Security Scan** | ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬ | SonarQube, OWASP | 15ë¶„ | ìë™ |
| **Deploy** | í™˜ê²½ë³„ ë°°í¬ | Helm, ArgoCD | 20ë¶„ | ìˆ˜ë™ |
| **Verify** | ë°°í¬ ê²€ì¦ | Smoke Tests | 5ë¶„ | ìë™ |

#### 2. ë°°í¬ ìë™í™”

| ìë™í™” ì˜ì—­ | ë„êµ¬ | íŠ¸ë¦¬ê±° | ì‹¤í–‰ ì‹œê°„ | ë¡¤ë°± |
|-------------|------|--------|-----------|------|
| **ì½”ë“œ ë°°í¬** | GitLab CI/CD | Git Push | 10ë¶„ | ìë™ |
| **ì¸í”„ë¼ ë°°í¬** | Terraform | ì½”ë“œ ë³€ê²½ | 30ë¶„ | ìˆ˜ë™ |
| **ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜** | Flyway | ìŠ¤í‚¤ë§ˆ ë³€ê²½ | 5ë¶„ | ìˆ˜ë™ |
| **ëª¨ë‹ˆí„°ë§ ì„¤ì •** | Prometheus | ì„œë¹„ìŠ¤ ë°°í¬ | 2ë¶„ | ìë™ |

### ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

#### 1. ê´€ì°°ì„± 3ìš”ì†Œ

| ê´€ì°°ì„± ìš”ì†Œ | ë„êµ¬ | ìˆ˜ì§‘ ì£¼ê¸° | ë³´ì¡´ ê¸°ê°„ | ì•Œë¦¼ |
|-------------|------|-----------|-----------|------|
| **ë©”íŠ¸ë¦­** | Prometheus | 15ì´ˆ | 30ì¼ | ì„ê³„ê°’ ì´ˆê³¼ |
| **ë¡œê·¸** | ELK Stack | ì‹¤ì‹œê°„ | 90ì¼ | ì—ëŸ¬ íŒ¨í„´ |
| **íŠ¸ë ˆì´ìŠ¤** | Jaeger | ìš”ì²­ë³„ | 7ì¼ | ì§€ì—°ì‹œê°„ ì´ˆê³¼ |
| **ì´ë²¤íŠ¸** | EventBridge | ì‹¤ì‹œê°„ | 30ì¼ | ì¤‘ìš” ì´ë²¤íŠ¸ |

#### 2. ì•Œë¦¼ ë° ëŒ€ì‘

| ì•Œë¦¼ ë ˆë²¨ | ì¡°ê±´ | ì±„ë„ | ì‘ë‹µ ì‹œê°„ | ëŒ€ì‘ì |
|-----------|------|------|-----------|--------|
| **Critical** | ì„œë¹„ìŠ¤ ì¤‘ë‹¨ | PagerDuty, SMS | 5ë¶„ | ì˜¨ì½œ ì—”ì§€ë‹ˆì–´ |
| **Warning** | ì„±ëŠ¥ ì €í•˜ | Slack, Email | 15ë¶„ | ê°œë°œíŒ€ |
| **Info** | ìƒíƒœ ë³€ê²½ | Dashboard | 1ì‹œê°„ | ìš´ì˜íŒ€ |
| **Debug** | ìƒì„¸ ì •ë³´ | ë¡œê·¸ ì‹œìŠ¤í…œ | 24ì‹œê°„ | ê°œë°œì |

---

## ğŸ¯ ì™„ì„±ëœ í”„ë¡œì íŠ¸: ê¸€ë¡œë²Œ IoT í”Œë«í¼ {#ì™„ì„±ëœ-í”„ë¡œì íŠ¸-ê¸€ë¡œë²Œ-iot-í”Œë«í¼}

### ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

#### 1. ì‹œìŠ¤í…œ ê°œìš”

| êµ¬ì„± ìš”ì†Œ | ê¸°ìˆ  ìŠ¤íƒ | ì—­í•  | ì²˜ë¦¬ ìš©ëŸ‰ |
|-----------|-----------|------|-----------|
| **Edge Gateway** | Kubernetes, InfluxDB Edge | ë¡œì»¬ ë°ì´í„° ìˆ˜ì§‘ | 100K points/sec |
| **Message Queue** | Apache Kafka | ê¸€ë¡œë²Œ ë©”ì‹œì§€ ì „ë‹¬ | 1M messages/sec |
| **Stream Processing** | Apache Flink | ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ | 500K events/sec |
| **Time Series DB** | InfluxDB Cluster | ì‹œê³„ì—´ ë°ì´í„° ì €ì¥ | 10M points/sec |
| **Analytics Engine** | Apache Spark | ë°°ì¹˜ ë¶„ì„ | 1TB/hour |
| **Visualization** | Grafana, Apache Superset | ë°ì´í„° ì‹œê°í™” | 1K users |

#### 2. ê¸€ë¡œë²Œ ë°°í¬ ì „ëµ

| ì§€ì—­ | ë°ì´í„°ì„¼í„° | ë…¸ë“œ ìˆ˜ | ìš©ëŸ‰ | ì§€ì—°ì‹œê°„ |
|------|------------|---------|------|----------|
| **ì•„ì‹œì•„** | ì„œìš¸, ì‹±ê°€í¬ë¥´, ë„ì¿„ | 50ê°œ | 1PB | < 50ms |
| **ìœ ëŸ½** | ëŸ°ë˜, í”„ë‘í¬í‘¸ë¥´íŠ¸ | 40ê°œ | 800TB | < 30ms |
| **ë¯¸êµ­** | ë²„ì§€ë‹ˆì•„, ìº˜ë¦¬í¬ë‹ˆì•„ | 60ê°œ | 1.2PB | < 40ms |
| **ê¸€ë¡œë²Œ CDN** | CloudFlare, AWS CloudFront | 100ê°œ | 500TB | < 20ms |

### ì„±ëŠ¥ ë° í™•ì¥ì„±

#### 1. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ì„±ëŠ¥ ì§€í‘œ | ëª©í‘œ | ì‹¤ì œ ì„±ëŠ¥ | ê°œì„ ìœ¨ |
|-----------|------|------------|--------|
| **ë°ì´í„° ìˆ˜ì§‘** | 10M points/sec | 12M points/sec | 120% |
| **ì¿¼ë¦¬ ì‘ë‹µì‹œê°„** | < 100ms | < 80ms | 125% |
| **ê°€ìš©ì„±** | 99.9% | 99.95% | 105% |
| **ë°ì´í„° ì •í™•ì„±** | 99.99% | 99.995% | 100.5% |

#### 2. í™•ì¥ì„± í…ŒìŠ¤íŠ¸

| ë¶€í•˜ í…ŒìŠ¤íŠ¸ | ì‹œë‚˜ë¦¬ì˜¤ | ê²°ê³¼ | í•œê³„ì  |
|-------------|----------|------|--------|
| **ë°ì´í„° ìˆ˜ì§‘** | 20M points/sec | ì„±ê³µ | ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ |
| **ë™ì‹œ ì¿¼ë¦¬** | 10K concurrent users | ì„±ê³µ | CPU ë¦¬ì†ŒìŠ¤ |
| **ë°ì´í„° ë³¼ë¥¨** | 10PB ì €ì¥ | ì„±ê³µ | ìŠ¤í† ë¦¬ì§€ ë¹„ìš© |
| **ì§€ì—­ í™•ì¥** | 100ê°œ ì§€ì—­ | ì„±ê³µ | ê´€ë¦¬ ë³µì¡ë„ |

### ìš´ì˜ ë° ìœ ì§€ë³´ìˆ˜

#### 1. ìš´ì˜ ìë™í™”

| ìš´ì˜ ì˜ì—­ | ìë™í™” ìˆ˜ì¤€ | ë„êµ¬ | íš¨ê³¼ |
|-----------|-------------|------|------|
| **ë°°í¬** | 95% | ArgoCD, Helm | ë°°í¬ ì‹œê°„ 90% ë‹¨ì¶• |
| **ëª¨ë‹ˆí„°ë§** | 100% | Prometheus, Grafana | ì¥ì•  ê°ì§€ ì‹œê°„ 95% ë‹¨ì¶• |
| **ë°±ì—…** | 100% | Velero, Restic | ë°±ì—… ì„±ê³µë¥  99.9% |
| **ìŠ¤ì¼€ì¼ë§** | 90% | HPA, VPA | ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ìµœì í™” |

#### 2. ë¹„ìš© ìµœì í™”

| ìµœì í™” ì˜ì—­ | ìµœì í™” ì „ëµ | ì ˆì•½ íš¨ê³¼ | êµ¬í˜„ ë³µì¡ë„ |
|-------------|-------------|-----------|-------------|
| **ìŠ¤í† ë¦¬ì§€** | ì••ì¶•, ë³´ì¡´ ì •ì±… | 70% ë¹„ìš© ì ˆì•½ | ì¤‘ê°„ |
| **ì»´í“¨íŒ…** | ì˜¤í† ìŠ¤ì¼€ì¼ë§, ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ | 40% ë¹„ìš© ì ˆì•½ | ë†’ìŒ |
| **ë„¤íŠ¸ì›Œí¬** | CDN, ì••ì¶• | 60% ë¹„ìš© ì ˆì•½ | ë‚®ìŒ |
| **ë¼ì´ì„ ìŠ¤** | ì˜¤í”ˆì†ŒìŠ¤ ìš°ì„  | 80% ë¹„ìš© ì ˆì•½ | ì¤‘ê°„ |

---

## ğŸ”® TDB ë¯¸ë˜ ì „ë§ {#tdb-ë¯¸ë˜-ì „ë§}

### ê¸°ìˆ  íŠ¸ë Œë“œ

#### 1. ì°¨ì„¸ëŒ€ TDB ê¸°ìˆ 

| ê¸°ìˆ  ë¶„ì•¼ | í˜„ì¬ | 5ë…„ í›„ | 10ë…„ í›„ | ì£¼ìš” ë³€í™” |
|-----------|------|--------|---------|-----------|
| **ì €ì¥ ê¸°ìˆ ** | SSD ê¸°ë°˜ | NVMe, 3D XPoint | DNA ì €ì¥, ì–‘ì ì €ì¥ | 1000x ìš©ëŸ‰ ì¦ê°€ |
| **ì²˜ë¦¬ ê¸°ìˆ ** | CPU ê¸°ë°˜ | GPU ê°€ì† | ì–‘ì ì»´í“¨íŒ… | 10000x ì„±ëŠ¥ í–¥ìƒ |
| **ë„¤íŠ¸ì›Œí¬** | 100Gbps | 400Gbps | 1Tbps | 10x ì†ë„ í–¥ìƒ |
| **ì••ì¶•** | 10:1 | 100:1 | 1000:1 | 100x íš¨ìœ¨ì„± í–¥ìƒ |

#### 2. ì•„í‚¤í…ì²˜ ì§„í™”

| ì•„í‚¤í…ì²˜ íŒ¨í„´ | í˜„ì¬ | ë¯¸ë˜ | ì¥ì  | ë„ì „ ê³¼ì œ |
|---------------|------|------|------|-----------|
| **ì¤‘ì•™ì§‘ì¤‘ì‹** | í´ë¼ìš°ë“œ ì¤‘ì‹¬ | Edge-First | ë‚®ì€ ì§€ì—°ì‹œê°„ | ë³µì¡í•œ ê´€ë¦¬ |
| **ë¶„ì‚°** | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ | ì„œë²„ë¦¬ìŠ¤ | ë†’ì€ í™•ì¥ì„± | ë™ê¸°í™” ë³µì¡ì„± |
| **í•˜ì´ë¸Œë¦¬ë“œ** | í´ë¼ìš°ë“œ + ì˜¨í”„ë ˆë¯¸ìŠ¤ | Multi-Cloud + Edge | ìœ ì—°ì„± | í†µí•© ë³µì¡ì„± |
| **ììœ¨** | ìë™í™” | AI ê¸°ë°˜ ììœ¨ ìš´ì˜ | ìš´ì˜ íš¨ìœ¨ì„± | ì‹ ë¢°ì„± í™•ë³´ |

### ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥

#### 1. ì‚°ì—…ë³„ ì ìš© í™•ì‚°

| ì‚°ì—… ë¶„ì•¼ | í˜„ì¬ ì ìš©ë¥  | 5ë…„ í›„ ì˜ˆìƒ | ì£¼ìš” ë™ë ¥ | ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ |
|-----------|-------------|-------------|-----------|---------------|
| **ì œì¡°ì—…** | 30% | 80% | ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ | ì‹¤ì‹œê°„ ì œì–´ |
| **ì˜ë£Œ** | 20% | 70% | ì›ê²© ì˜ë£Œ | ì •í™•ì„±, ë³´ì•ˆ |
| **ê¸ˆìœµ** | 60% | 95% | ì‹¤ì‹œê°„ ê±°ë˜ | ë‚®ì€ ì§€ì—°ì‹œê°„ |
| **ì—ë„ˆì§€** | 40% | 90% | ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œ | ì˜ˆì¸¡ ë¶„ì„ |

#### 2. ìƒˆë¡œìš´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸

| ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ | ì„¤ëª… | ìˆ˜ìµ êµ¬ì¡° | ì„±ì¥ ì ì¬ë ¥ | ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ |
|---------------|------|-----------|-------------|---------------|
| **Data-as-a-Service** | ë°ì´í„° íŒë§¤ | êµ¬ë… ê¸°ë°˜ | ë†’ìŒ | ë°ì´í„° í’ˆì§ˆ |
| **Analytics-as-a-Service** | ë¶„ì„ ì„œë¹„ìŠ¤ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ | ë§¤ìš° ë†’ìŒ | AI/ML ê¸°ìˆ  |
| **Platform-as-a-Service** | í”Œë«í¼ ì œê³µ | ìˆ˜ìˆ˜ë£Œ ê¸°ë°˜ | ë†’ìŒ | í†µí•© í”Œë«í¼ |
| **Edge-as-a-Service** | Edge ì¸í”„ë¼ | ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ | ì¤‘ê°„ | Edge ê¸°ìˆ  |

---

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì „ì²´ ì‹œë¦¬ì¦ˆ ë³µìŠµ

1. **Part 1: ê¸°ì´ˆì™€ ì•„í‚¤í…ì²˜**
   - TDB ê¸°ë³¸ ê°œë…ê³¼ íŠ¹ì„±
   - ì£¼ìš” ì†”ë£¨ì…˜ ë¹„êµ ë¶„ì„
   - ì„±ëŠ¥ ìµœì í™” ì›ë¦¬
   - ì‹¤ë¬´ í”„ë¡œì íŠ¸ ê¸°ì´ˆ

2. **Part 2: ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ìµœì í™”**
   - ë¶„ì‚° ì•„í‚¤í…ì²˜ì™€ í´ëŸ¬ìŠ¤í„°ë§
   - ê³ ê°€ìš©ì„±ê³¼ ì¥ì•  ë³µêµ¬
   - ì„±ëŠ¥ íŠœë‹ ê³ ê¸‰ ê¸°ë²•
   - ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ êµ¬ì¶•

3. **Part 3: í†µí•©ê³¼ ë°°í¬**
   - ì‹œìŠ¤í…œ ê°„ í†µí•© ì „ëµ
   - í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜
   - ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ
   - í”„ë¡œë•ì…˜ ë°°í¬ ì™„ì„±

### í•µì‹¬ ì—­ëŸ‰ ìŠµë“

| ì—­ëŸ‰ ì˜ì—­ | ìŠµë“ ë‚´ìš© | ì‹¤ë¬´ ì ìš© | ì§€ì†ì  í•™ìŠµ |
|-----------|-----------|-----------|-------------|
| **ì•„í‚¤í…ì²˜ ì„¤ê³„** | TDB ì‹œìŠ¤í…œ ì„¤ê³„ ì›ë¦¬ | í”„ë¡œì íŠ¸ ì„¤ê³„ | ìµœì‹  íŠ¸ë Œë“œ ì¶”ì  |
| **ì„±ëŠ¥ ìµœì í™”** | ì¿¼ë¦¬ íŠœë‹, ì¸ë±ìŠ¤ ìµœì í™” | ì‹œìŠ¤í…œ íŠœë‹ | ë²¤ì¹˜ë§ˆí¬ ì§€ì† |
| **ìš´ì˜ ê´€ë¦¬** | ëª¨ë‹ˆí„°ë§, ìë™í™” | ìš´ì˜ íš¨ìœ¨ì„± | DevOps ë„êµ¬ í•™ìŠµ |
| **ë¬¸ì œ í•´ê²°** | íŠ¸ëŸ¬ë¸”ìŠˆíŒ…, ì¥ì•  ëŒ€ì‘ | ì‹ ì†í•œ ëŒ€ì‘ | ê²½í—˜ ì¶•ì  |

### ë‹¤ìŒ í•™ìŠµ ë°©í–¥

| í•™ìŠµ ì˜ì—­ | ì¶”ì²œ ì£¼ì œ | í•™ìŠµ ë°©ë²• | ì‹¤ë¬´ ì ìš© |
|-----------|-----------|-----------|-----------|
| **ì‹¬í™” ê¸°ìˆ ** | íŠ¹ì • TDB ì†”ë£¨ì…˜ ë§ˆìŠ¤í„° | ê³µì‹ ë¬¸ì„œ, íŠœí† ë¦¬ì–¼ | ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ |
| **í™•ì¥ ê¸°ìˆ ** | AI/ML, Edge Computing | ì˜¨ë¼ì¸ ê°•ì˜, ì—°êµ¬ ë…¼ë¬¸ | í”„ë¡œí† íƒ€ì… ê°œë°œ |
| **ìš´ì˜ ê¸°ìˆ ** | DevOps, SRE | ì‹¤ë¬´ ê²½í—˜, ì¸ì¦ ì·¨ë“ | ìš´ì˜ í™˜ê²½ êµ¬ì¶• |
| **ë¹„ì¦ˆë‹ˆìŠ¤** | ë„ë©”ì¸ ì§€ì‹, ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´ | ì—…ê³„ ë¶„ì„, ë„¤íŠ¸ì›Œí‚¹ | ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì°½ì¶œ |

---

## ğŸ¯ ìµœì¢… í•µì‹¬ í¬ì¸íŠ¸

1. **TDBëŠ” ë‹¨ìˆœí•œ ì €ì¥ì†Œê°€ ì•„ë‹˜**: í˜„ëŒ€ì  ë°ì´í„° í”Œë«í¼ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œ
2. **í†µí•©ì´ í•µì‹¬**: ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ì˜ íš¨ê³¼ì ì¸ í†µí•©ì´ ì„±ê³µì˜ ì—´ì‡ 
3. **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸ŒëŠ” í•„ìˆ˜**: í˜„ëŒ€ì  ë°°í¬ì™€ ìš´ì˜ì„ ìœ„í•œ í•„ìˆ˜ ìš”ì†Œ
4. **ì§€ì†ì  ì§„í™”**: ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ë”°ë¼ê°€ëŠ” ì§€ì†ì  í•™ìŠµì´ í•„ìš”
5. **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì¤‘ì‹¬**: ê¸°ìˆ ë³´ë‹¤ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ í•´ê²°ì— ì§‘ì¤‘

Time Series Database ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ í˜„ëŒ€ì  TDB ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì—­ëŸ‰ì„ ê°–ì¶”ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ í˜ì‹ ì ì¸ ë°ì´í„° ì†”ë£¨ì…˜ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸš€

**ì¶•í•˜í•©ë‹ˆë‹¤! TDB ë§ˆìŠ¤í„°ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸ‰

---

## ğŸ“– ì¶”ê°€ í•™ìŠµ ìë£Œ

### ì¶”ì²œ ë„ì„œ
- "Designing Data-Intensive Applications" by Martin Kleppmann
- "Time Series Databases" by Ted Dunning & Ellen Friedman
- "Site Reliability Engineering" by Google

### ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤
- [InfluxDB ê³µì‹ ë¬¸ì„œ](https://docs.influxdata.com/)
- [TimescaleDB íŠœí† ë¦¬ì–¼](https://docs.timescale.com/)
- [Prometheus ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ](https://prometheus.io/docs/)

### ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬
- InfluxDB Community Forum
- TimescaleDB Slack
- Prometheus Users Mailing List

**ì§€ì†ì ì¸ í•™ìŠµê³¼ ì‹¤ë¬´ ê²½í—˜ì„ í†µí•´ TDB ì „ë¬¸ê°€ë¡œ ì„±ì¥í•˜ì„¸ìš”!** ğŸŒŸ

---
layout: post
title: "Delta Lake vs Iceberg vs Hudi ì‹¤ì „ ë¹„êµ - í…Œì´ë¸” í¬ë§· ì™„ì „ ì •ë³µ"
description: "ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì˜ í•µì‹¬ í…Œì´ë¸” í¬ë§·ì¸ Delta Lake, Apache Iceberg, Apache Hudië¥¼ ì•„í‚¤í…ì²˜ë¶€í„° ACID, Time Travel, ì„±ëŠ¥ê¹Œì§€ ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ë¡œ ì™„ì „ ë¹„êµí•©ë‹ˆë‹¤."
excerpt: "ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì˜ í•µì‹¬ í…Œì´ë¸” í¬ë§·ì¸ Delta Lake, Apache Iceberg, Apache Hudië¥¼ ì•„í‚¤í…ì²˜ë¶€í„° ACID, Time Travel, ì„±ëŠ¥ê¹Œì§€ ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ë¡œ ì™„ì „ ë¹„êµ"
category: data-engineering
tags: [DeltaLake, Iceberg, Hudi, DataLakehouse, TableFormat, ACID, TimeTravel, Spark]
series: cloud-data-architecture
series_order: 3
date: 2025-10-21
author: Data Droid
lang: ko
reading_time: 60ë¶„
difficulty: ê³ ê¸‰
---

# ğŸ›ï¸ Delta Lake vs Iceberg vs Hudi ì‹¤ì „ ë¹„êµ - í…Œì´ë¸” í¬ë§· ì™„ì „ ì •ë³µ

> **"íŒŒì¼ í¬ë§·ì—ì„œ í…Œì´ë¸” í¬ë§·ìœ¼ë¡œ - ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì˜ í•µì‹¬ ê¸°ìˆ "** - ACID, Time Travel, Schema Evolutionì„ ì§€ì›í•˜ëŠ” ì°¨ì„¸ëŒ€ ë°ì´í„° ë ˆì´í¬

Parquet, ORC, Avro ê°™ì€ íŒŒì¼ í¬ë§·ë§Œìœ¼ë¡œëŠ” ACID íŠ¸ëœì­ì…˜, ìŠ¤í‚¤ë§ˆ ì§„í™”, Time Travel ê°™ì€ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. Delta Lake, Apache Iceberg, Apache HudiëŠ” íŒŒì¼ í¬ë§· ìœ„ì— ë©”íƒ€ë°ì´í„° ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ **ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ìˆ˜ì¤€ì˜ ê¸°ëŠ¥ì„ ë°ì´í„° ë ˆì´í¬ì—ì„œ ì œê³µ**í•©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì„¸ ê°€ì§€ í…Œì´ë¸” í¬ë§·ì˜ ì•„í‚¤í…ì²˜, ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬, ê·¸ë¦¬ê³  ìƒí™©ë³„ ìµœì  ì„ íƒ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨

- [í…Œì´ë¸” í¬ë§·ì´ë€?](#í…Œì´ë¸”-í¬ë§·ì´ë€)
- [Delta Lake ì•„í‚¤í…ì²˜](#delta-lake-ì•„í‚¤í…ì²˜)
- [Apache Iceberg ì•„í‚¤í…ì²˜](#apache-iceberg-ì•„í‚¤í…ì²˜)
- [Apache Hudi ì•„í‚¤í…ì²˜](#apache-hudi-ì•„í‚¤í…ì²˜)
- [ê¸°ëŠ¥ ë¹„êµ](#ê¸°ëŠ¥-ë¹„êµ)
- [ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ](#ì‹¤ì œ-ë²¤ì¹˜ë§ˆí¬-ë¹„êµ)
- [ìƒí™©ë³„ ìµœì  ì„ íƒ](#ìƒí™©ë³„-ìµœì -ì„ íƒ)
- [ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](#ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ)
- [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

---

## ğŸ¯ í…Œì´ë¸” í¬ë§·ì´ë€? {#í…Œì´ë¸”-í¬ë§·ì´ë€}

### íŒŒì¼ í¬ë§· vs í…Œì´ë¸” í¬ë§·

| **êµ¬ë¶„** | **íŒŒì¼ í¬ë§·** | **í…Œì´ë¸” í¬ë§·** |
|----------|---------------|-----------------|
| **ì˜ˆì‹œ** | Parquet, ORC, Avro | Delta Lake, Iceberg, Hudi |
| **ì—­í• ** | ë°ì´í„° ì €ì¥ ë°©ì‹ | ë©”íƒ€ë°ì´í„° + íŠ¸ëœì­ì…˜ ê´€ë¦¬ |
| **ACID** | ë¯¸ì§€ì› | ì§€ì› |
| **Time Travel** | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ |
| **Schema Evolution** | ì œí•œì  | ì™„ë²½ ì§€ì› |
| **Update/Delete** | ì–´ë ¤ì›€ | ì‰¬ì›€ |

### ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜

```
ì „í†µì ì¸ ë°ì´í„° ë ˆì´í¬:
S3/HDFS
  â””â”€â”€ Parquet Files
      â””â”€â”€ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì§ì ‘ íŒŒì¼ ê´€ë¦¬

ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤:
S3/HDFS
  â””â”€â”€ Parquet Files (ë°ì´í„°)
      â””â”€â”€ Delta/Iceberg/Hudi (ë©”íƒ€ë°ì´í„° ë ˆì´ì–´)
          â””â”€â”€ ACID, Time Travel, Schema Evolution
```

### ì™œ í…Œì´ë¸” í¬ë§·ì´ í•„ìš”í•œê°€?

#### **ë¬¸ì œ 1: ì¼ê´€ì„± ì—†ëŠ” ì½ê¸°**
```python
# ì „í†µì ì¸ ë°ì´í„° ë ˆì´í¬
# Writerê°€ íŒŒì¼ ì“°ëŠ” ì¤‘ì— Readerê°€ ì½ìœ¼ë©´?
df.write.parquet("s3://bucket/data/")  # ì“°ê¸° ì¤‘...
# ë™ì‹œì— ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤
df = spark.read.parquet("s3://bucket/data/")  # ë¶ˆì™„ì „í•œ ë°ì´í„° ì½ì„ ìˆ˜ ìˆìŒ
```

#### **ë¬¸ì œ 2: Update/Delete ë¶ˆê°€ëŠ¥**
```sql
-- Parquetë§Œìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥
UPDATE events SET amount = amount * 1.1 WHERE date = '2024-01-15';
-- í•´ê²°: ì „ì²´ íŒŒí‹°ì…˜ ì¬ì‘ì„± í•„ìš” (ë¹„íš¨ìœ¨ì )
```

#### **ë¬¸ì œ 3: ìŠ¤í‚¤ë§ˆ ë³€ê²½ì˜ ì–´ë ¤ì›€**
```python
# ì»¬ëŸ¼ ì¶”ê°€ ì‹œ ê¸°ì¡´ íŒŒì¼ê³¼ í˜¸í™˜ì„± ë¬¸ì œ
df_v1.write.parquet("data/v1/")  # ì»¬ëŸ¼ 3ê°œ
df_v2.write.parquet("data/v2/")  # ì»¬ëŸ¼ 4ê°œ
# ë‘ ë²„ì „ ë™ì‹œ ì½ê¸° ì‹œ ìŠ¤í‚¤ë§ˆ ì¶©ëŒ ê°€ëŠ¥
```

### í…Œì´ë¸” í¬ë§·ì˜ í•´ê²°ì±…

| **ë¬¸ì œ** | **í•´ê²°ì±…** |
|----------|-----------|
| **ì¼ê´€ì„±** | íŠ¸ëœì­ì…˜ ë¡œê·¸ë¡œ ì›ìì„± ë³´ì¥ |
| **Update/Delete** | Merge-on-Read or Copy-on-Write |
| **ìŠ¤í‚¤ë§ˆ ë³€ê²½** | ë©”íƒ€ë°ì´í„° ë²„ì „ ê´€ë¦¬ |
| **Time Travel** | ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ ë²„ì „ ê´€ë¦¬ |
| **ì„±ëŠ¥** | ë©”íƒ€ë°ì´í„° ìºì‹±, í†µê³„ ìµœì í™” |

---

## ğŸ”· Delta Lake ì•„í‚¤í…ì²˜ {#delta-lake-ì•„í‚¤í…ì²˜}

### í•µì‹¬ ê°œë…

Delta LakeëŠ” **íŠ¸ëœì­ì…˜ ë¡œê·¸ ê¸°ë°˜**ì˜ í…Œì´ë¸” í¬ë§·ì…ë‹ˆë‹¤.

#### **ì£¼ìš” êµ¬ì„±ìš”ì†Œ**
- **Transaction Log (_delta_log/)**: JSON í˜•ì‹ì˜ íŠ¸ëœì­ì…˜ ë¡œê·¸
- **Data Files**: Parquet íŒŒì¼
- **Checkpoint**: ì£¼ê¸°ì ì¸ ë©”íƒ€ë°ì´í„° ìŠ¤ëƒ…ìƒ·

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```bash
s3://bucket/delta-table/
â”œâ”€â”€ _delta_log/
â”‚   â”œâ”€â”€ 00000000000000000000.json  # íŠ¸ëœì­ì…˜ 0
â”‚   â”œâ”€â”€ 00000000000000000001.json  # íŠ¸ëœì­ì…˜ 1
â”‚   â”œâ”€â”€ 00000000000000000002.json  # íŠ¸ëœì­ì…˜ 2
â”‚   â”œâ”€â”€ 00000000000000000010.checkpoint.parquet  # ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ _last_checkpoint  # ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜
â”œâ”€â”€ part-00000-uuid.snappy.parquet
â”œâ”€â”€ part-00001-uuid.snappy.parquet
â””â”€â”€ part-00002-uuid.snappy.parquet
```

### íŠ¸ëœì­ì…˜ ë¡œê·¸ ì˜ˆì‹œ

```json
{
  "commitInfo": {
    "timestamp": 1705305600000,
    "operation": "WRITE",
    "operationParameters": {"mode": "Append"},
    "readVersion": 0,
    "isolationLevel": "WriteSerializable"
  }
}
{
  "add": {
    "path": "part-00000-uuid.snappy.parquet",
    "partitionValues": {"date": "2024-01-15"},
    "size": 134217728,
    "modificationTime": 1705305600000,
    "dataChange": true,
    "stats": "{\"numRecords\":1000000,\"minValues\":{\"amount\":0.5},\"maxValues\":{\"amount\":999.9}}"
  }
}
```

### Delta Lake ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Delta Lake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# 1. Delta í…Œì´ë¸” ìƒì„±
df.write \
    .format("delta") \
    .mode("overwrite") \
    .partitionBy("date") \
    .save("s3://bucket/delta/events")

# 2. ACID íŠ¸ëœì­ì…˜
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://bucket/delta/events")

# Update
delta_table.update(
    condition = "date = '2024-01-15'",
    set = {"amount": "amount * 1.1"}
)

# Delete
delta_table.delete("amount < 0")

# Merge (Upsert)
delta_table.alias("target").merge(
    updates_df.alias("source"),
    "target.id = source.id"
).whenMatchedUpdate(
    set = {"amount": "source.amount"}
).whenNotMatchedInsert(
    values = {"id": "source.id", "amount": "source.amount"}
).execute()

# 3. Time Travel
# íŠ¹ì • ë²„ì „ìœ¼ë¡œ ì½ê¸°
df_v5 = spark.read.format("delta").option("versionAsOf", 5).load("s3://bucket/delta/events")

# íŠ¹ì • ì‹œê°„ìœ¼ë¡œ ì½ê¸°
df_yesterday = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-14 00:00:00") \
    .load("s3://bucket/delta/events")

# 4. Schema Evolution
df_new_schema.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("s3://bucket/delta/events")
```

### Delta Lake ìµœì í™”

```python
# 1. Optimize (Compaction)
delta_table.optimize().executeCompaction()

# 2. Z-Ordering (ë‹¤ì°¨ì› í´ëŸ¬ìŠ¤í„°ë§)
delta_table.optimize().executeZOrderBy("user_id", "product_id")

# 3. Vacuum (ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ)
delta_table.vacuum(168)  # 7ì¼ ì´ìƒ ëœ íŒŒì¼ ì‚­ì œ

# 4. Data Skipping (í†µê³„ ê¸°ë°˜ ìŠ¤í‚µ)
# ìë™ìœ¼ë¡œ min/max í†µê³„ ìˆ˜ì§‘ ë° í™œìš©
```

---

## ğŸ”¶ Apache Iceberg ì•„í‚¤í…ì²˜ {#apache-iceberg-ì•„í‚¤í…ì²˜}

### í•µì‹¬ ê°œë…

IcebergëŠ” **ë©”íƒ€ë°ì´í„° íŠ¸ë¦¬ êµ¬ì¡°**ë¡œ ëŒ€ê·œëª¨ í…Œì´ë¸”ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

#### **ì£¼ìš” êµ¬ì„±ìš”ì†Œ**
- **Metadata Files**: í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
- **Manifest Lists**: ìŠ¤ëƒ…ìƒ·ë³„ manifest ëª©ë¡
- **Manifest Files**: ë°ì´í„° íŒŒì¼ ëª©ë¡ê³¼ í†µê³„
- **Data Files**: Parquet/ORC/Avro íŒŒì¼

### ë©”íƒ€ë°ì´í„° ê³„ì¸µ êµ¬ì¡°

```
Iceberg Metadata Hierarchy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Table Metadata (metadata.json)  â”‚
â”‚  â”œâ”€â”€ Schema                     â”‚
â”‚  â”œâ”€â”€ Partition Spec             â”‚
â”‚  â””â”€â”€ Current Snapshot ID        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Snapshot                        â”‚
â”‚  â”œâ”€â”€ Snapshot ID                â”‚
â”‚  â”œâ”€â”€ Timestamp                  â”‚
â”‚  â””â”€â”€ Manifest List             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Manifest List                   â”‚
â”‚  â”œâ”€â”€ Manifest File 1            â”‚
â”‚  â”œâ”€â”€ Manifest File 2            â”‚
â”‚  â””â”€â”€ Manifest File 3            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Manifest File                   â”‚
â”‚  â”œâ”€â”€ Data File 1 + Stats        â”‚
â”‚  â”œâ”€â”€ Data File 2 + Stats        â”‚
â”‚  â””â”€â”€ Data File 3 + Stats        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Files (Parquet)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```bash
s3://bucket/iceberg-table/
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ v1.metadata.json
â”‚   â”œâ”€â”€ v2.metadata.json
â”‚   â”œâ”€â”€ snap-123-1-abc.avro  # Manifest List
â”‚   â”œâ”€â”€ abc123-m0.avro       # Manifest File
â”‚   â””â”€â”€ abc123-m1.avro
â””â”€â”€ data/
    â”œâ”€â”€ date=2024-01-15/
    â”‚   â”œâ”€â”€ 00000-0-data-uuid.parquet
    â”‚   â””â”€â”€ 00001-0-data-uuid.parquet
    â””â”€â”€ date=2024-01-16/
        â””â”€â”€ 00000-0-data-uuid.parquet
```

### Iceberg ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Iceberg") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hadoop") \
    .config("spark.sql.catalog.spark_catalog.warehouse", "s3://bucket/warehouse") \
    .getOrCreate()

# 1. Iceberg í…Œì´ë¸” ìƒì„±
spark.sql("""
    CREATE TABLE events (
        id INT,
        name STRING,
        amount DOUBLE,
        event_time TIMESTAMP
    )
    USING iceberg
    PARTITIONED BY (days(event_time))
""")

# 2. ë°ì´í„° ì“°ê¸°
df.writeTo("events").append()

# 3. ACID íŠ¸ëœì­ì…˜
# Merge (Upsert)
spark.sql("""
    MERGE INTO events t
    USING updates s
    ON t.id = s.id
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")

# Delete
spark.sql("DELETE FROM events WHERE amount < 0")

# 4. Time Travel
# íŠ¹ì • ìŠ¤ëƒ…ìƒ·
df_snapshot = spark.read \
    .option("snapshot-id", 1234567890) \
    .format("iceberg") \
    .load("events")

# íŠ¹ì • ì‹œê°„
df_timestamp = spark.read \
    .option("as-of-timestamp", "1705305600000") \
    .format("iceberg") \
    .load("events")

# 5. ìŠ¤í‚¤ë§ˆ ì§„í™”
spark.sql("ALTER TABLE events ADD COLUMN category STRING")

# 6. Partition Evolution (ê¸°ì¡´ ë°ì´í„° ì¬ì‘ì„± ì—†ì´)
spark.sql("""
    ALTER TABLE events 
    REPLACE PARTITION FIELD days(event_time) 
    WITH hours(event_time)
""")
```

### Iceberg ìµœì í™”

```python
# 1. Expire Snapshots (ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬)
spark.sql("CALL spark_catalog.system.expire_snapshots('events', TIMESTAMP '2024-01-01 00:00:00')")

# 2. Remove Orphan Files (ê³ ì•„ íŒŒì¼ ì‚­ì œ)
spark.sql("CALL spark_catalog.system.remove_orphan_files('events')")

# 3. Rewrite Data Files (ì‘ì€ íŒŒì¼ ë³‘í•©)
spark.sql("CALL spark_catalog.system.rewrite_data_files('events')")

# 4. Rewrite Manifests (manifest ìµœì í™”)
spark.sql("CALL spark_catalog.system.rewrite_manifests('events')")
```

---

## ğŸ”¹ Apache Hudi ì•„í‚¤í…ì²˜ {#apache-hudi-ì•„í‚¤í…ì²˜}

### í•µì‹¬ ê°œë…

HudiëŠ” **ì¦ë¶„ ì²˜ë¦¬ì™€ ë¹ ë¥¸ upsert**ì— ìµœì í™”ëœ í…Œì´ë¸” í¬ë§·ì…ë‹ˆë‹¤.

#### **ì£¼ìš” êµ¬ì„±ìš”ì†Œ**
- **Timeline**: í…Œì´ë¸”ì˜ ëª¨ë“  ì‘ì—… ì´ë ¥
- **Hoodie Metadata**: .hoodie/ ë””ë ‰í† ë¦¬ì˜ ë©”íƒ€ë°ì´í„°
- **Base Files**: Parquet íŒŒì¼
- **Log Files**: ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œê·¸

### í…Œì´ë¸” íƒ€ì…

#### **Copy on Write (CoW)**
- **ì“°ê¸°**: íŒŒì¼ ì „ì²´ ì¬ì‘ì„±
- **ì½ê¸°**: ë¹ ë¦„ (Parquet ì§ì ‘ ì½ê¸°)
- **ì‚¬ìš©**: ì½ê¸° ì¤‘ì‹¬ ì›Œí¬ë¡œë“œ

#### **Merge on Read (MoR)**
- **ì“°ê¸°**: ë¸íƒ€ ë¡œê·¸ì— ì¶”ê°€
- **ì½ê¸°**: Base + Log ë³‘í•© í•„ìš”
- **ì‚¬ìš©**: ì“°ê¸° ì¤‘ì‹¬ ì›Œí¬ë¡œë“œ

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```bash
s3://bucket/hudi-table/
â”œâ”€â”€ .hoodie/
â”‚   â”œâ”€â”€ hoodie.properties
â”‚   â”œâ”€â”€ 20240115120000.commit
â”‚   â”œâ”€â”€ 20240115130000.commit
â”‚   â”œâ”€â”€ 20240115120000.inflight
â”‚   â””â”€â”€ archived/
â”‚       â””â”€â”€ commits/
â”œâ”€â”€ date=2024-01-15/
â”‚   â”œâ”€â”€ abc123-0_0-1-0_20240115120000.parquet  # Base file (CoW)
â”‚   â”œâ”€â”€ abc123-0_0-1-0_20240115130000.log      # Log file (MoR)
â”‚   â””â”€â”€ .abc123-0_0-1-0_20240115120000.parquet.crc
â””â”€â”€ date=2024-01-16/
    â””â”€â”€ def456-0_0-1-0_20240116100000.parquet
```

### Hudi ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Hudi") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# 1. Hudi í…Œì´ë¸” ìƒì„± (Copy on Write)
hudi_options = {
    'hoodie.table.name': 'events',
    'hoodie.datasource.write.recordkey.field': 'id',
    'hoodie.datasource.write.precombine.field': 'event_time',
    'hoodie.datasource.write.partitionpath.field': 'date',
    'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',
    'hoodie.datasource.write.operation': 'upsert'
}

df.write \
    .format("hudi") \
    .options(**hudi_options) \
    .mode("overwrite") \
    .save("s3://bucket/hudi/events")

# 2. Upsert (í•µì‹¬ ê¸°ëŠ¥)
updates_df.write \
    .format("hudi") \
    .options(**hudi_options) \
    .mode("append") \
    .save("s3://bucket/hudi/events")

# 3. Incremental Query (ì¦ë¶„ ì½ê¸°)
incremental_df = spark.read \
    .format("hudi") \
    .option("hoodie.datasource.query.type", "incremental") \
    .option("hoodie.datasource.read.begin.instanttime", "20240115120000") \
    .option("hoodie.datasource.read.end.instanttime", "20240115130000") \
    .load("s3://bucket/hudi/events")

# 4. Time Travel
# íŠ¹ì • ì‹œê°„ì˜ ë°ì´í„°
df_snapshot = spark.read \
    .format("hudi") \
    .option("as.of.instant", "20240115120000") \
    .load("s3://bucket/hudi/events")

# 5. Compaction (MoRì—ì„œ ì¤‘ìš”)
spark.sql("""
    CALL run_compaction(
        table => 'events',
        path => 's3://bucket/hudi/events'
    )
""")
```

### Hudi ìµœì í™”

```python
# 1. Clustering (íŒŒì¼ ì¬êµ¬ì„±)
hudi_options['hoodie.clustering.inline'] = 'true'
hudi_options['hoodie.clustering.inline.max.commits'] = '4'

# 2. Indexing
hudi_options['hoodie.index.type'] = 'BLOOM'  # BLOOM, SIMPLE, GLOBAL_BLOOM

# 3. File Sizing
hudi_options['hoodie.parquet.small.file.limit'] = '104857600'  # 100MB
hudi_options['hoodie.parquet.max.file.size'] = '134217728'     # 128MB

# 4. Async Compaction
hudi_options['hoodie.compact.inline'] = 'false'
hudi_options['hoodie.compact.schedule.inline'] = 'true'
```

---

## ğŸ“Š ê¸°ëŠ¥ ë¹„êµ {#ê¸°ëŠ¥-ë¹„êµ}

### ACID íŠ¸ëœì­ì…˜

| **ê¸°ëŠ¥** | **Delta Lake** | **Iceberg** | **Hudi** |
|----------|----------------|-------------|----------|
| **Atomicity** | âœ… íŠ¸ëœì­ì…˜ ë¡œê·¸ | âœ… ìŠ¤ëƒ…ìƒ· ê²©ë¦¬ | âœ… Timeline |
| **Isolation Level** | Serializable | Snapshot | Snapshot |
| **Concurrent Writes** | âœ… ì§€ì› | âœ… ì§€ì› | âš ï¸ ì œí•œì  |
| **Optimistic Concurrency** | âœ… | âœ… | âš ï¸ |

### Time Travel

| **ê¸°ëŠ¥** | **Delta Lake** | **Iceberg** | **Hudi** |
|----------|----------------|-------------|----------|
| **ë²„ì „ ê¸°ë°˜** | âœ… versionAsOf | âœ… snapshot-id | âœ… as.of.instant |
| **ì‹œê°„ ê¸°ë°˜** | âœ… timestampAsOf | âœ… as-of-timestamp | âœ… as.of.instant |
| **ë³´ì¡´ ê¸°ê°„** | ì„¤ì • ê°€ëŠ¥ | ì„¤ì • ê°€ëŠ¥ | ì„¤ì • ê°€ëŠ¥ |
| **ì„±ëŠ¥** | ë¹ ë¦„ | ë¹ ë¦„ | ë¹ ë¦„ |

### Schema Evolution

| **ê¸°ëŠ¥** | **Delta Lake** | **Iceberg** | **Hudi** |
|----------|----------------|-------------|----------|
| **ì»¬ëŸ¼ ì¶”ê°€** | âœ… | âœ… | âœ… |
| **ì»¬ëŸ¼ ì‚­ì œ** | âœ… | âœ… | âœ… |
| **ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½** | âš ï¸ ì¬ì‘ì„± í•„ìš” | âœ… | âš ï¸ ì¬ì‘ì„± í•„ìš” |
| **íƒ€ì… ë³€ê²½** | âš ï¸ í˜¸í™˜ ê°€ëŠ¥í•œ ê²ƒë§Œ | âœ… Promotion ì§€ì› | âš ï¸ ì œí•œì  |
| **ì¤‘ì²© ìŠ¤í‚¤ë§ˆ** | âœ… | âœ… | âœ… |

### Partition Evolution

| **ê¸°ëŠ¥** | **Delta Lake** | **Iceberg** | **Hudi** |
|----------|----------------|-------------|----------|
| **íŒŒí‹°ì…˜ ë³€ê²½** | âŒ ë¶ˆê°€ëŠ¥ | âœ… ê°€ëŠ¥ | âš ï¸ ì¬ì‘ì„± í•„ìš” |
| **ê¸°ì¡´ ë°ì´í„°** | ì¬ì‘ì„± í•„ìš” | ì¬ì‘ì„± ë¶ˆí•„ìš” | ì¬ì‘ì„± í•„ìš” |
| **Hidden Partitioning** | âŒ | âœ… | âŒ |

### Update/Delete ì„±ëŠ¥

| **ì‘ì—…** | **Delta Lake** | **Iceberg** | **Hudi (CoW)** | **Hudi (MoR)** |
|----------|----------------|-------------|----------------|----------------|
| **Update** | íŒŒí‹°ì…˜ ì¬ì‘ì„± | íŒŒì¼ ì¬ì‘ì„± | íŒŒì¼ ì¬ì‘ì„± | ë¡œê·¸ ì¶”ê°€ âš¡ |
| **Delete** | íŒŒí‹°ì…˜ ì¬ì‘ì„± | íŒŒì¼ ì¬ì‘ì„± | íŒŒì¼ ì¬ì‘ì„± | ë¡œê·¸ ì¶”ê°€ âš¡ |
| **Merge** | âœ… ì§€ì› | âœ… ì§€ì› | âœ… ìµœì í™” | âœ… ìµœì í™” |

---

## ğŸ”¬ ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ {#ì‹¤ì œ-ë²¤ì¹˜ë§ˆí¬-ë¹„êµ}

### í…ŒìŠ¤íŠ¸ í™˜ê²½

| **í•­ëª©** | **ì„¤ì •** |
|----------|----------|
| **ë°ì´í„°ì…‹** | TPC-DS 1TB |
| **Spark ë²„ì „** | 3.4.0 |
| **ì¸ìŠ¤í„´ìŠ¤** | r5.4xlarge Ã— 20 |
| **íŒŒì¼ í¬ë§·** | Parquet (Snappy) |
| **í…Œì´ë¸” ìˆ˜** | 10ê°œ ì£¼ìš” í…Œì´ë¸” |

### ë²¤ì¹˜ë§ˆí¬ 1: ì´ˆê¸° ë°ì´í„° ì ì¬

```python
# 1TB ë°ì´í„°ë¥¼ ê° í¬ë§·ìœ¼ë¡œ ì ì¬
import time

# Delta Lake
start = time.time()
df.write.format("delta").mode("overwrite").save("s3://bucket/delta/")
delta_time = time.time() - start

# Iceberg
start = time.time()
df.writeTo("iceberg_table").create()
iceberg_time = time.time() - start

# Hudi (CoW)
start = time.time()
df.write.format("hudi").options(**hudi_cow_options).save("s3://bucket/hudi_cow/")
hudi_cow_time = time.time() - start

# Hudi (MoR)
start = time.time()
df.write.format("hudi").options(**hudi_mor_options).save("s3://bucket/hudi_mor/")
hudi_mor_time = time.time() - start
```

#### **ì´ˆê¸° ì ì¬ ì„±ëŠ¥**

| **í¬ë§·** | **ì ì¬ ì‹œê°„** | **ìŠ¤í† ë¦¬ì§€** | **íŒŒì¼ ìˆ˜** |
|----------|---------------|--------------|-------------|
| **Parquet** | 18ë¶„ 32ì´ˆ | 98.3 GB | 784 |
| **Delta Lake** | 19ë¶„ 47ì´ˆ | 98.5 GB | 784 + ë¡œê·¸ |
| **Iceberg** | 20ë¶„ 12ì´ˆ | 98.4 GB | 784 + ë©”íƒ€ë°ì´í„° |
| **Hudi (CoW)** | 21ë¶„ 38ì´ˆ | 98.6 GB | 784 + .hoodie |
| **Hudi (MoR)** | 19ë¶„ 54ì´ˆ | 98.5 GB | 784 + .hoodie |

### ë²¤ì¹˜ë§ˆí¬ 2: Update ì„±ëŠ¥

```sql
-- 10% ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
UPDATE events 
SET amount = amount * 1.1 
WHERE date = '2024-01-15';
```

#### **Update ì„±ëŠ¥ ë¹„êµ**

| **í¬ë§·** | **Update ì‹œê°„** | **ì˜í–¥ íŒŒì¼** | **ì¬ì‘ì„± ë°ì´í„°** | **ì½ê¸° ì„±ëŠ¥** |
|----------|-----------------|---------------|-------------------|---------------|
| **Delta Lake** | 42.3ì´ˆ | íŒŒí‹°ì…˜ ì „ì²´ | 9.8 GB | ë³€í™” ì—†ìŒ |
| **Iceberg** | 38.7ì´ˆ | ì˜í–¥ë°›ì€ íŒŒì¼ë§Œ | 2.1 GB | ë³€í™” ì—†ìŒ |
| **Hudi (CoW)** | 45.1ì´ˆ | ì˜í–¥ë°›ì€ íŒŒì¼ë§Œ | 2.1 GB | ë³€í™” ì—†ìŒ |
| **Hudi (MoR)** | 8.2ì´ˆ âš¡ | ë¡œê·¸ íŒŒì¼ë§Œ | 210 MB | ì•½ê°„ ëŠë¦¼ |

### ë²¤ì¹˜ë§ˆí¬ 3: Merge (Upsert) ì„±ëŠ¥

```python
# 100ë§Œ ë ˆì½”ë“œ upsert
updates_df = spark.read.parquet("s3://bucket/updates/")

# Delta Lake
delta_table.alias("target").merge(
    updates_df.alias("source"),
    "target.id = source.id"
).whenMatchedUpdate(set = {...}).whenNotMatchedInsert(values = {...}).execute()

# Iceberg
spark.sql("MERGE INTO events ...")

# Hudi
updates_df.write.format("hudi").options(**hudi_options).mode("append").save(...)
```

#### **Merge ì„±ëŠ¥ ë¹„êµ**

| **í¬ë§·** | **Merge ì‹œê°„** | **ì²˜ë¦¬ëŸ‰** | **ë©”ëª¨ë¦¬ ì‚¬ìš©** |
|----------|----------------|------------|-----------------|
| **Delta Lake** | 3ë¶„ 12ì´ˆ | 5,208 records/s | 24.3 GB |
| **Iceberg** | 2ë¶„ 48ì´ˆ | 5,952 records/s | 22.1 GB |
| **Hudi (CoW)** | 3ë¶„ 34ì´ˆ | 4,673 records/s | 26.8 GB |
| **Hudi (MoR)** | 1ë¶„ 23ì´ˆ âš¡ | 12,048 records/s | 18.4 GB |

### ë²¤ì¹˜ë§ˆí¬ 4: Time Travel ì„±ëŠ¥

```python
# 7ì¼ ì „ ë°ì´í„° ì¡°íšŒ
import time

# Delta Lake
start = time.time()
df = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-08 00:00:00") \
    .load("s3://bucket/delta/events")
count = df.count()
delta_tt_time = time.time() - start

# Iceberg
start = time.time()
df = spark.read.format("iceberg") \
    .option("as-of-timestamp", "2024-01-08 00:00:00") \
    .load("events")
count = df.count()
iceberg_tt_time = time.time() - start

# Hudi
start = time.time()
df = spark.read.format("hudi") \
    .option("as.of.instant", "20240108000000") \
    .load("s3://bucket/hudi/events")
count = df.count()
hudi_tt_time = time.time() - start
```

#### **Time Travel ì„±ëŠ¥**

| **í¬ë§·** | **ë©”íƒ€ë°ì´í„° ë¡œë“œ** | **ë°ì´í„° ì½ê¸°** | **ì´ ì‹œê°„** |
|----------|---------------------|-----------------|-------------|
| **Delta Lake** | 1.2ì´ˆ | 18.4ì´ˆ | 19.6ì´ˆ |
| **Iceberg** | 0.8ì´ˆ | 18.1ì´ˆ | 18.9ì´ˆ âš¡ |
| **Hudi** | 2.3ì´ˆ | 18.6ì´ˆ | 20.9ì´ˆ |

### ë²¤ì¹˜ë§ˆí¬ 5: ì¦ë¶„ ì½ê¸° (Incremental Read)

```python
# ë§ˆì§€ë§‰ ì²˜ë¦¬ ì´í›„ ë³€ê²½ëœ ë°ì´í„°ë§Œ ì½ê¸°
# Hudiì˜ ê°•ë ¥í•œ ê¸°ëŠ¥

# Hudi Incremental Query
incremental_df = spark.read \
    .format("hudi") \
    .option("hoodie.datasource.query.type", "incremental") \
    .option("hoodie.datasource.read.begin.instanttime", "20240115120000") \
    .load("s3://bucket/hudi/events")

print(f"Changed records: {incremental_df.count():,}")
# ê²°ê³¼: Changed records: 145,234 (ì „ì²´ì˜ 0.14%)
```

#### **ì¦ë¶„ ì½ê¸° ì„±ëŠ¥**

| **í¬ë§·** | **ë°©ë²•** | **ì½ê¸° ì‹œê°„** | **ìŠ¤ìº” ë°ì´í„°** |
|----------|----------|---------------|-----------------|
| **Delta Lake** | Change Data Feed | 8.2ì´ˆ | 1.2 GB |
| **Iceberg** | Incremental Scan | 7.8ì´ˆ | 1.1 GB |
| **Hudi** | Incremental Query | 3.4ì´ˆ âš¡ | 0.4 GB |

**í•µì‹¬**: HudiëŠ” ì¦ë¶„ ì²˜ë¦¬ì— ìµœì í™”ë˜ì–´ CDC íŒŒì´í”„ë¼ì¸ì— ì í•©

---

## ğŸ¯ ìƒí™©ë³„ ìµœì  ì„ íƒ {#ìƒí™©ë³„-ìµœì -ì„ íƒ}

### ì„ íƒ ê°€ì´ë“œ ë§¤íŠ¸ë¦­ìŠ¤

| **ìš”êµ¬ì‚¬í•­** | **Delta Lake** | **Iceberg** | **Hudi** |
|--------------|----------------|-------------|----------|
| **Databricks ì‚¬ìš©** | â­â­â­ | â­â­ | â­ |
| **AWS í™˜ê²½** | â­â­ | â­â­â­ | â­â­ |
| **ë‹¤ì–‘í•œ ì—”ì§„ ì§€ì›** | â­â­ | â­â­â­ | â­ |
| **ë¹ˆë²ˆí•œ Update** | â­â­ | â­â­ | â­â­â­ |
| **CDC íŒŒì´í”„ë¼ì¸** | â­â­ | â­â­ | â­â­â­ |
| **ì½ê¸° ì¤‘ì‹¬** | â­â­â­ | â­â­â­ | â­â­ |
| **ì“°ê¸° ì¤‘ì‹¬** | â­â­ | â­â­ | â­â­â­ |
| **Partition Evolution** | â­ | â­â­â­ | â­ |
| **ì»¤ë®¤ë‹ˆí‹°** | â­â­â­ | â­â­â­ | â­â­ |

### Use Case 1: Databricks ê¸°ë°˜ ë¶„ì„ í”Œë«í¼

#### **ì¶”ì²œ: Delta Lake**

```python
# Databricksì—ì„œ Delta Lake ì‚¬ìš©
# 1. Unity Catalog í†µí•©
spark.sql("""
    CREATE TABLE main.analytics.events
    USING DELTA
    PARTITIONED BY (date)
    LOCATION 's3://bucket/delta/events'
""")

# 2. Delta Live Tables (DLT)
@dlt.table(
    name="events_gold",
    comment="Cleansed events table"
)
def events_gold():
    return (
        dlt.read("events_silver")
        .where("amount > 0")
        .select("id", "name", "amount", "date")
    )

# 3. Photon ì—”ì§„ ìµœì í™”
spark.conf.set("spark.databricks.photon.enabled", "true")
```

**ì´ìœ **:
- âœ… Databricks ë„¤ì´í‹°ë¸Œ ì§€ì›
- âœ… Unity Catalog í†µí•©
- âœ… Photon ì—”ì§„ ìµœì í™”
- âœ… Delta Live Tables

### Use Case 2: ë©€í‹° ì—”ì§„ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤

#### **ì¶”ì²œ: Apache Iceberg**

```python
# Spark, Presto, Flink, Athena ëª¨ë‘ ì§€ì›
# 1. Sparkì—ì„œ ìƒì„±
spark.sql("""
    CREATE TABLE iceberg_catalog.db.events (
        id INT,
        name STRING,
        amount DOUBLE
    )
    USING iceberg
    PARTITIONED BY (days(event_time))
""")

# 2. Prestoì—ì„œ ì¿¼ë¦¬
# SELECT * FROM iceberg.db.events WHERE date = DATE '2024-01-15'

# 3. Flinkì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°
tableEnv.executeSql("""
    CREATE TABLE events (
        id INT,
        name STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3)
    ) WITH (
        'connector' = 'iceberg',
        'catalog-name' = 'iceberg_catalog'
    )
""")

# 4. AWS Glue Catalog í†µí•©
spark.conf.set("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.warehouse", "s3://bucket/warehouse")
spark.conf.set("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
```

**ì´ìœ **:
- âœ… ê°€ì¥ ë§ì€ ì—”ì§„ ì§€ì›
- âœ… AWS Glue í†µí•©
- âœ… ë²¤ë” ì¤‘ë¦½ì 
- âœ… Partition Evolution

### Use Case 3: CDC ë° ì‹¤ì‹œê°„ Upsert

#### **ì¶”ì²œ: Apache Hudi (MoR)**

```python
# Kafka CDC â†’ Hudi MoR íŒŒì´í”„ë¼ì¸
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("CDC to Hudi") \
    .getOrCreate()

# 1. Kafkaì—ì„œ CDC ì´ë²¤íŠ¸ ì½ê¸°
cdc_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "mysql.events") \
    .load()

# 2. CDC ì´ë²¤íŠ¸ íŒŒì‹±
parsed_df = cdc_df.select(
    from_json(col("value").cast("string"), cdc_schema).alias("data")
).select("data.*")

# 3. Hudi MoRë¡œ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°
hudi_options = {
    'hoodie.table.name': 'events',
    'hoodie.datasource.write.table.type': 'MERGE_ON_READ',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.recordkey.field': 'id',
    'hoodie.datasource.write.precombine.field': 'updated_at',
    'hoodie.compact.inline': 'false',
    'hoodie.compact.schedule.inline': 'true'
}

parsed_df.writeStream \
    .format("hudi") \
    .options(**hudi_options) \
    .outputMode("append") \
    .option("checkpointLocation", "s3://bucket/checkpoints/") \
    .start("s3://bucket/hudi/events")

# 4. ì¦ë¶„ ì½ê¸°ë¡œ downstream ì²˜ë¦¬
incremental_df = spark.read \
    .format("hudi") \
    .option("hoodie.datasource.query.type", "incremental") \
    .option("hoodie.datasource.read.begin.instanttime", last_commit_time) \
    .load("s3://bucket/hudi/events")
```

**ì´ìœ **:
- âœ… ë¹ ë¥¸ upsert ì„±ëŠ¥
- âœ… ì¦ë¶„ ì½ê¸° ìµœì í™”
- âœ… MoRë¡œ ì“°ê¸° ë¶€í•˜ ìµœì†Œí™”
- âœ… CDCì— íŠ¹í™”ëœ ê¸°ëŠ¥

### Use Case 4: ëŒ€ê·œëª¨ ë°°ì¹˜ ë¶„ì„

#### **ì¶”ì²œ: Delta Lake ë˜ëŠ” Iceberg**

```python
# ëŒ€ê·œëª¨ ë°°ì¹˜ ETL
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Batch Analytics") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Delta Lake
delta_table = DeltaTable.forPath(spark, "s3://bucket/delta/events")

# Z-Orderingìœ¼ë¡œ ì¿¼ë¦¬ ìµœì í™”
delta_table.optimize() \
    .where("date >= '2024-01-01'") \
    .executeZOrderBy("user_id", "product_id")

# Iceberg
spark.sql("""
    CALL spark_catalog.system.rewrite_data_files(
        table => 'events',
        strategy => 'sort',
        sort_order => 'user_id, product_id'
    )
""")
```

**ì´ìœ **:
- âœ… ì½ê¸° ì„±ëŠ¥ ìµœì í™”
- âœ… ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬ ì•ˆì •ì„±
- âœ… í†µê³„ ê¸°ë°˜ ìµœì í™”

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ {#ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ}

### Parquet â†’ Delta Lake

```python
# ê¸°ì¡´ Parquet í…Œì´ë¸”ì„ Delta Lakeë¡œ ë³€í™˜
from delta.tables import DeltaTable

# 1. In-place ë³€í™˜
DeltaTable.convertToDelta(
    spark,
    "parquet.`s3://bucket/events`",
    "date STRING"
)

# 2. ìƒˆë¡œìš´ ìœ„ì¹˜ë¡œ ë³€í™˜
df = spark.read.parquet("s3://bucket/parquet/events")
df.write.format("delta").save("s3://bucket/delta/events")

# 3. ê²€ì¦
delta_df = spark.read.format("delta").load("s3://bucket/delta/events")
parquet_df = spark.read.parquet("s3://bucket/parquet/events")

assert delta_df.count() == parquet_df.count(), "Count mismatch!"
```

### Parquet â†’ Iceberg

```python
# Parquetë¥¼ Icebergë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
# 1. ê¸°ì¡´ Parquet ìœ„ì¹˜ì— Iceberg ë©”íƒ€ë°ì´í„° ìƒì„±
spark.sql("""
    CREATE TABLE iceberg_catalog.db.events
    USING iceberg
    LOCATION 's3://bucket/parquet/events'
    AS SELECT * FROM parquet.`s3://bucket/parquet/events`
""")

# 2. ë˜ëŠ” CTAS (Create Table As Select)
spark.sql("""
    CREATE TABLE iceberg_catalog.db.events
    USING iceberg
    PARTITIONED BY (date)
    AS SELECT * FROM parquet.`s3://bucket/parquet/events`
""")
```

### Delta Lake â†” Iceberg ìƒí˜¸ ë³€í™˜

```python
# Delta Lake â†’ Iceberg
delta_df = spark.read.format("delta").load("s3://bucket/delta/events")
delta_df.writeTo("iceberg_catalog.db.events").create()

# Iceberg â†’ Delta Lake
iceberg_df = spark.read.format("iceberg").load("iceberg_catalog.db.events")
iceberg_df.write.format("delta").save("s3://bucket/delta/events")
```

### ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

```python
# íŒŒí‹°ì…˜ë³„ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
from datetime import datetime, timedelta

def migrate_partition(source_format, target_format, date):
    """íŠ¹ì • íŒŒí‹°ì…˜ì„ ìƒˆë¡œìš´ í¬ë§·ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    # ì†ŒìŠ¤ ì½ê¸°
    if source_format == "parquet":
        df = spark.read.parquet(f"s3://bucket/parquet/events/date={date}/")
    elif source_format == "delta":
        df = spark.read.format("delta").load(f"s3://bucket/delta/events") \
            .where(f"date = '{date}'")
    
    # íƒ€ê²Ÿ ì“°ê¸°
    if target_format == "iceberg":
        df.writeTo(f"iceberg_catalog.db.events").append()
    elif target_format == "hudi":
        df.write.format("hudi").options(**hudi_options).mode("append") \
            .save("s3://bucket/hudi/events")
    
    print(f"âœ“ Migrated: {date}")

# ì „ì²´ ê¸°ê°„ ë§ˆì´ê·¸ë ˆì´ì…˜
start_date = datetime(2024, 1, 1)
end_date = datetime(2024, 12, 31)
current_date = start_date

while current_date <= end_date:
    date_str = current_date.strftime("%Y-%m-%d")
    try:
        migrate_partition("parquet", "iceberg", date_str)
    except Exception as e:
        print(f"âœ— Failed: {date_str} - {e}")
    
    current_date += timedelta(days=1)
```

---

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### í•µì‹¬ í¬ì¸íŠ¸

1. **í…Œì´ë¸” í¬ë§·ì˜ í•„ìš”ì„±**
   - ACID íŠ¸ëœì­ì…˜ ë³´ì¥
   - Time Travel ë° ë²„ì „ ê´€ë¦¬
   - íš¨ìœ¨ì ì¸ Update/Delete/Merge
   - ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì›

2. **í¬ë§·ë³„ íŠ¹ì§•**
   - **Delta Lake**: Databricks ìµœì í™”, ì‰¬ìš´ ì‚¬ìš©
   - **Iceberg**: ë©€í‹° ì—”ì§„, Partition Evolution
   - **Hudi**: CDC ìµœì í™”, ë¹ ë¥¸ upsert

3. **ì„±ëŠ¥ ë¹„êµ ìš”ì•½**
   - **ì´ˆê¸° ì ì¬**: ë¹„ìŠ· (ì•½ 20ë¶„/1TB)
   - **Update**: Hudi MoR ì••ë„ì  (8.2ì´ˆ vs 40ì´ˆëŒ€)
   - **Merge**: Hudi MoR ê°€ì¥ ë¹ ë¦„ (1ë¶„ 23ì´ˆ)
   - **ì¦ë¶„ ì½ê¸°**: Hudi ìµœì í™” (3.4ì´ˆ)

4. **ì„ íƒ ê¸°ì¤€**
   - **Databricks**: Delta Lake
   - **AWS + ë©€í‹° ì—”ì§„**: Iceberg
   - **CDC + Upsert ì¤‘ì‹¬**: Hudi
   - **ë²”ìš©**: Delta Lake ë˜ëŠ” Iceberg

### ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì‚¬ìš© í”Œë«í¼ í™•ì¸ (Databricks, AWS, On-prem)
- [ ] ì¿¼ë¦¬ ì—”ì§„ í™•ì¸ (Spark, Presto, Flink)
- [ ] ì›Œí¬ë¡œë“œ ë¶„ì„ (ì½ê¸°/ì“°ê¸° ë¹„ìœ¨)
- [ ] Update/Delete ë¹ˆë„ íŒŒì•…
- [ ] ìŠ¤í‚¤ë§ˆ ë³€ê²½ ë¹ˆë„ í™•ì¸
- [ ] ì¦ë¶„ ì²˜ë¦¬ í•„ìš”ì„± í‰ê°€
- [ ] POC ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìˆ˜ë¦½

### ë‹¤ìŒ ë‹¨ê³„

- **Lakehouse ì•„í‚¤í…ì²˜**: Unity Catalog, Glue Catalog
- **ì„±ëŠ¥ íŠœë‹**: Compaction, Z-Ordering, Clustering
- **ìš´ì˜ ìë™í™”**: Vacuum, Expire snapshots
- **ê±°ë²„ë„ŒìŠ¤**: ë°ì´í„° í’ˆì§ˆ, ì ‘ê·¼ ì œì–´

---

> **"í…Œì´ë¸” í¬ë§·ì€ ë°ì´í„° ë ˆì´í¬ë¥¼ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ë¡œ ì§„í™”ì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤."**

Delta Lake, Iceberg, HudiëŠ” ê°ê°ì˜ ê°•ì ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì™„ë²½í•œ ì •ë‹µì€ ì—†ìŠµë‹ˆë‹¤. ìì‹ ì˜ í™˜ê²½ê³¼ ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ íŒŒì•…í•˜ê³ , ì‹¤ì œ POCë¥¼ í†µí•´ ê²€ì¦í•œ í›„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œê°€ ì˜¬ë°”ë¥¸ ì„ íƒì— ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤!

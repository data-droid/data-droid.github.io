---
layout: post
lang: ko
title: "Part 1: Apache Spark ê¸°ì´ˆì™€ í•µì‹¬ ê°œë… - RDDë¶€í„° DataFrameê¹Œì§€"
description: "Apache Sparkì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ í•µì‹¬ ê°œë…ì¸ RDD, DataFrame, Spark SQLì„ í•™ìŠµí•˜ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤."
date: 2025-09-11
author: Data Droid
category: data-engineering
tags: [Apache-Spark, RDD, DataFrame, Spark-SQL, ë¹…ë°ì´í„°ì²˜ë¦¬, Python, PySpark]
series: apache-spark-complete-guide
series_order: 1
reading_time: "30ë¶„"
difficulty: "ì¤‘ê¸‰"
---

# Part 1: Apache Spark ê¸°ì´ˆì™€ í•µì‹¬ ê°œë… - RDDë¶€í„° DataFrameê¹Œì§€

> Apache Sparkì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ í•µì‹¬ ê°œë…ì¸ RDD, DataFrame, Spark SQLì„ í•™ìŠµí•˜ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Spark ì•„í‚¤í…ì²˜ ì´í•´](#spark-ì•„í‚¤í…ì²˜-ì´í•´)
2. [RDD (Resilient Distributed Dataset)](#rdd-resilient-distributed-dataset)
3. [DataFrameê³¼ Dataset](#dataframeê³¼-dataset)
4. [Spark SQL](#spark-sql)
5. [ì‹¤ìŠµ: ê¸°ë³¸ ë°ì´í„° ì²˜ë¦¬](#ì‹¤ìŠµ-ê¸°ë³¸-ë°ì´í„°-ì²˜ë¦¬)
6. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ— ï¸ Spark ì•„í‚¤í…ì²˜ ì´í•´ {#spark-ì•„í‚¤í…ì²˜-ì´í•´}

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

Apache SparkëŠ” ë¶„ì‚° ì»´í“¨íŒ…ì„ ìœ„í•œ í†µí•© ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

#### **1. Driver Program**
- **ì—­í• **: ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
- **ê¸°ëŠ¥**: SparkContext ìƒì„±, ì‘ì—… ìŠ¤ì¼€ì¤„ë§, ê²°ê³¼ ìˆ˜ì§‘
- **ìœ„ì¹˜**: í´ë¼ì´ì–¸íŠ¸ ë…¸ë“œì—ì„œ ì‹¤í–‰

#### **2. Cluster Manager**
- **Standalone**: Spark ìì²´ í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì €
- **YARN**: Hadoop ìƒíƒœê³„ì˜ ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì €
- **Mesos**: ë²”ìš© í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì €
- **Kubernetes**: ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

#### **3. Worker Node**
- **Executor**: ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” JVM í”„ë¡œì„¸ìŠ¤
- **Task**: Executorì—ì„œ ì‹¤í–‰ë˜ëŠ” ê°œë³„ ì‘ì—… ë‹¨ìœ„
- **Cache**: ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°ì´í„° ì €ì¥ì†Œ

### SparkContextì™€ SparkSession

```python
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

# SparkContext ìƒì„± (RDDìš©)
conf = SparkConf().setAppName("MyApp").setMaster("local[*]")
sc = SparkContext(conf=conf)

# SparkSession ìƒì„± (DataFrame/SQLìš©)
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# SparkContext ì ‘ê·¼
sc_from_session = spark.sparkContext
```

## ğŸ”„ RDD (Resilient Distributed Dataset) {#rdd-resilient-distributed-dataset}

### RDDë€?

RDDëŠ” Sparkì˜ ê¸°ë³¸ ë°ì´í„° ì¶”ìƒí™”ì…ë‹ˆë‹¤. **ë¶ˆë³€(immutable)**, **ë¶„ì‚°(distributed)**, **íƒ„ë ¥ì (resilient)**í•œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

#### **RDDì˜ íŠ¹ì„±**
1. **ë¶ˆë³€ì„±**: ìƒì„± í›„ ìˆ˜ì • ë¶ˆê°€, ë³€í™˜ì„ í†µí•´ ìƒˆë¡œìš´ RDD ìƒì„±
2. **ë¶„ì‚°ì„±**: ì—¬ëŸ¬ ë…¸ë“œì— ê±¸ì³ ë¶„ì‚° ì €ì¥
3. **íƒ„ë ¥ì„±**: ì¥ì•  ë°œìƒ ì‹œ ìë™ ë³µêµ¬ (Lineage ê¸°ë°˜)
4. **ì§€ì—° ì‹¤í–‰**: Action í˜¸ì¶œ ì‹œê¹Œì§€ ì‹¤ì œ ì—°ì‚° ì§€ì—°

### RDD ìƒì„± ë°©ë²•

```python
# 1. ì»¬ë ‰ì…˜ì—ì„œ RDD ìƒì„±
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data, numSlices=4)  # 4ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í• 

# 2. ì™¸ë¶€ íŒŒì¼ì—ì„œ RDD ìƒì„±
rdd_text = sc.textFile("hdfs://path/to/file.txt")
rdd_csv = sc.textFile("hdfs://path/to/file.csv")

# 3. ë‹¤ë¥¸ RDDì—ì„œ ë³€í™˜
rdd_transformed = rdd.map(lambda x: x * 2)
```

### RDD ê¸°ë³¸ ì—°ì‚°

#### **Transformation (ë³€í™˜)**
```python
# Map: ê° ìš”ì†Œì— í•¨ìˆ˜ ì ìš©
rdd = sc.parallelize([1, 2, 3, 4, 5])
doubled = rdd.map(lambda x: x * 2)
print(doubled.collect())  # [2, 4, 6, 8, 10]

# Filter: ì¡°ê±´ì— ë§ëŠ” ìš”ì†Œë§Œ ì„ íƒ
evens = rdd.filter(lambda x: x % 2 == 0)
print(evens.collect())  # [2, 4]

# FlatMap: ê° ìš”ì†Œë¥¼ ì—¬ëŸ¬ ìš”ì†Œë¡œ í™•ì¥
words = sc.parallelize(["hello world", "spark tutorial"])
word_list = words.flatMap(lambda x: x.split(" "))
print(word_list.collect())  # ['hello', 'world', 'spark', 'tutorial']

# Distinct: ì¤‘ë³µ ì œê±°
data = [1, 2, 2, 3, 3, 3]
rdd = sc.parallelize(data)
unique = rdd.distinct()
print(unique.collect())  # [1, 2, 3]
```

#### **Action (ì•¡ì…˜)**
```python
# Collect: ëª¨ë“  ë°ì´í„°ë¥¼ ë“œë¼ì´ë²„ë¡œ ìˆ˜ì§‘
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.collect()
print(result)  # [1, 2, 3, 4, 5]

# Count: ìš”ì†Œ ê°œìˆ˜ ë°˜í™˜
count = rdd.count()
print(count)  # 5

# First: ì²« ë²ˆì§¸ ìš”ì†Œ ë°˜í™˜
first = rdd.first()
print(first)  # 1

# Take: ì²˜ìŒ nê°œ ìš”ì†Œ ë°˜í™˜
first_three = rdd.take(3)
print(first_three)  # [1, 2, 3]

# Reduce: ìš”ì†Œë“¤ì„ í•˜ë‚˜ë¡œ ì¶•ì•½
sum_result = rdd.reduce(lambda x, y: x + y)
print(sum_result)  # 15

# Fold: ì´ˆê¸°ê°’ì„ ì‚¬ìš©í•œ ì¶•ì•½
sum_with_zero = rdd.fold(0, lambda x, y: x + y)
print(sum_with_zero)  # 15
```

### ê³ ê¸‰ RDD ì—°ì‚°

#### **ê·¸ë£¹í™”ì™€ ì§‘ê³„**
```python
# GroupByKey: í‚¤ë³„ë¡œ ê·¸ë£¹í™”
data = [("apple", 1), ("banana", 2), ("apple", 3), ("banana", 4)]
rdd = sc.parallelize(data)
grouped = rdd.groupByKey()
print(grouped.mapValues(list).collect())
# [('apple', [1, 3]), ('banana', [2, 4])]

# ReduceByKey: í‚¤ë³„ë¡œ ê°’ë“¤ì„ ì¶•ì•½
reduced = rdd.reduceByKey(lambda x, y: x + y)
print(reduced.collect())  # [('apple', 4), ('banana', 6)]

# AggregateByKey: ë³µì¡í•œ ì§‘ê³„
# ì´ˆê¸°ê°’, ì‹œí€€ìŠ¤ í•¨ìˆ˜, ê²°í•© í•¨ìˆ˜
aggregated = rdd.aggregateByKey(
    (0, 0),  # ì´ˆê¸°ê°’: (í•©ê³„, ê°œìˆ˜)
    lambda acc, val: (acc[0] + val, acc[1] + 1),  # ì‹œí€€ìŠ¤ í•¨ìˆ˜
    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])  # ê²°í•© í•¨ìˆ˜
)
print(aggregated.collect())
# [('apple', (4, 2)), ('banana', (6, 2))]
```

#### **ì¡°ì¸ ì—°ì‚°**
```python
# ë‘ RDD ìƒì„±
rdd1 = sc.parallelize([("apple", 1), ("banana", 2)])
rdd2 = sc.parallelize([("apple", "red"), ("banana", "yellow")])

# Inner Join
inner_join = rdd1.join(rdd2)
print(inner_join.collect())
# [('apple', (1, 'red')), ('banana', (2, 'yellow'))]

# Left Outer Join
left_join = rdd1.leftOuterJoin(rdd2)
print(left_join.collect())
# [('apple', (1, 'red')), ('banana', (2, 'yellow'))]

# Cartesian Product (ì¹´í…Œì‹œì•ˆ ê³±)
cartesian = rdd1.cartesian(rdd2)
print(cartesian.collect())
# [('apple', ('apple', 'red')), ('apple', ('banana', 'yellow')), ...]
```

## ğŸ“Š DataFrameê³¼ Dataset {#dataframeê³¼-dataset}

### DataFrame ì†Œê°œ

DataFrameì€ RDDì˜ ì§„í™”ëœ í˜•íƒœë¡œ, **êµ¬ì¡°í™”ëœ ë°ì´í„°**ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### **DataFrameì˜ ì¥ì **
1. **ìµœì í™”ëœ ì‹¤í–‰**: Catalyst Optimizerê°€ ì¿¼ë¦¬ ìµœì í™”
2. **ìŠ¤í‚¤ë§ˆ ì •ë³´**: ì»¬ëŸ¼ íƒ€ì…ê³¼ ì´ë¦„ ì •ë³´ í¬í•¨
3. **í’ë¶€í•œ API**: SQL, Python, Scala, R ì§€ì›
4. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: Tungsten ì—”ì§„ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”

### DataFrame ìƒì„±

```python
# 1. RDDì—ì„œ DataFrame ìƒì„±
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("city", StringType(), True)
])

# ë°ì´í„° ìƒì„±
data = [("Alice", 25, "Seoul"), ("Bob", 30, "Busan"), ("Charlie", 35, "Seoul")]
rdd = sc.parallelize(data)
df = spark.createDataFrame(rdd, schema)
df.show()

# 2. ì§ì ‘ DataFrame ìƒì„±
df = spark.createDataFrame([
    ("Alice", 25, "Seoul"),
    ("Bob", 30, "Busan"),
    ("Charlie", 35, "Seoul")
], ["name", "age", "city"])

# 3. ì™¸ë¶€ íŒŒì¼ì—ì„œ ë¡œë“œ
df_csv = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
df_json = spark.read.json("path/to/file.json")
df_parquet = spark.read.parquet("path/to/file.parquet")
```

### DataFrame ê¸°ë³¸ ì—°ì‚°

```python
# ê¸°ë³¸ ì •ë³´ í™•ì¸
df.printSchema()  # ìŠ¤í‚¤ë§ˆ ì¶œë ¥
df.show()  # ë°ì´í„° ì¶œë ¥
df.show(5)  # ì²˜ìŒ 5í–‰ ì¶œë ¥
df.count()  # í–‰ ê°œìˆ˜
df.columns  # ì»¬ëŸ¼ ëª©ë¡
df.dtypes  # ì»¬ëŸ¼ íƒ€ì…

# ì»¬ëŸ¼ ì„ íƒ
df.select("name", "age").show()
df.select(df.name, df.age + 1).show()

# ì¡°ê±´ í•„í„°ë§
df.filter(df.age > 25).show()
df.filter("age > 25").show()  # SQL ìŠ¤íƒ€ì¼

# ì •ë ¬
df.orderBy("age").show()
df.orderBy(df.age.desc()).show()

# ê·¸ë£¹í™”ì™€ ì§‘ê³„
df.groupBy("city").count().show()
df.groupBy("city").agg({"age": "avg", "name": "count"}).show()
```

### ê³ ê¸‰ DataFrame ì—°ì‚°

#### **ìœˆë„ìš° í•¨ìˆ˜**
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead

# ìœˆë„ìš° ì •ì˜
window_spec = Window.partitionBy("city").orderBy("age")

# ìœˆë„ìš° í•¨ìˆ˜ ì ìš©
df.withColumn("row_num", row_number().over(window_spec)) \
  .withColumn("rank", rank().over(window_spec)) \
  .withColumn("dense_rank", dense_rank().over(window_spec)) \
  .withColumn("prev_age", lag("age", 1).over(window_spec)) \
  .withColumn("next_age", lead("age", 1).over(window_spec)) \
  .show()
```

#### **ì¡°ì¸ ì—°ì‚°**
```python
# ë‘ DataFrame ìƒì„±
df1 = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
df2 = spark.createDataFrame([(1, "Engineer"), (2, "Manager")], ["id", "job"])

# Inner Join
df1.join(df2, "id").show()

# Left Join
df1.join(df2, "id", "left").show()

# Cross Join
df1.crossJoin(df2).show()
```

## ğŸ” Spark SQL

### Spark SQL ì†Œê°œ

Spark SQLì€ êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Spark ëª¨ë“ˆì…ë‹ˆë‹¤. SQL ì¿¼ë¦¬ì™€ DataFrame APIë¥¼ í†µí•©í•©ë‹ˆë‹¤.

### í…Œì´ë¸” ë·° ìƒì„±

```python
# DataFrameì„ ì„ì‹œ ë·°ë¡œ ë“±ë¡
df.createOrReplaceTempView("people")

# SQL ì¿¼ë¦¬ ì‹¤í–‰
result = spark.sql("""
    SELECT city, COUNT(*) as count, AVG(age) as avg_age
    FROM people
    WHERE age > 25
    GROUP BY city
    ORDER BY count DESC
""")
result.show()
```

### ê³ ê¸‰ SQL ê¸°ëŠ¥

```python
# ë³µì¡í•œ ì¿¼ë¦¬ ì˜ˆì œ
spark.sql("""
    SELECT 
        name,
        age,
        city,
        ROW_NUMBER() OVER (PARTITION BY city ORDER BY age DESC) as rank_in_city,
        AVG(age) OVER (PARTITION BY city) as avg_age_in_city
    FROM people
    WHERE age > 20
""").show()

# CTE (Common Table Expression) ì‚¬ìš©
spark.sql("""
    WITH city_stats AS (
        SELECT city, COUNT(*) as count, AVG(age) as avg_age
        FROM people
        GROUP BY city
    )
    SELECT p.name, p.age, p.city, cs.avg_age
    FROM people p
    JOIN city_stats cs ON p.city = cs.city
    WHERE p.age > cs.avg_age
""").show()
```

## ğŸ›  ï¸ ì‹¤ìŠµ: ê¸°ë³¸ ë°ì´í„° ì²˜ë¦¬ {#ì‹¤ìŠµ-ê¸°ë³¸-ë°ì´í„°-ì²˜ë¦¬}

### ì‹¤ìŠµ 1: ë¡œê·¸ ë°ì´í„° ë¶„ì„

```python
# ë¡œê·¸ ë°ì´í„° ìƒì„±
log_data = [
    "2025-09-11 10:30:45 INFO User login successful user_id=12345",
    "2025-09-11 10:31:12 ERROR Database connection failed",
    "2025-09-11 10:31:45 INFO User login successful user_id=67890",
    "2025-09-11 10:32:01 WARN High memory usage detected",
    "2025-09-11 10:32:15 INFO User logout user_id=12345"
]

# RDDë¡œ ë¡œê·¸ ë¶„ì„
rdd_logs = sc.parallelize(log_data)

# ë¡œê·¸ ë ˆë²¨ë³„ í†µê³„
log_levels = rdd_logs.map(lambda line: line.split()[2])  # ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ
level_counts = log_levels.map(lambda level: (level, 1)).reduceByKey(lambda x, y: x + y)
print("ë¡œê·¸ ë ˆë²¨ë³„ í†µê³„:")
for level, count in level_counts.collect():
    print(f"{level}: {count}")

# ì—ëŸ¬ ë¡œê·¸ë§Œ í•„í„°ë§
error_logs = rdd_logs.filter(lambda line: "ERROR" in line)
print("\nì—ëŸ¬ ë¡œê·¸:")
error_logs.foreach(print)
```

### ì‹¤ìŠµ 2: êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬

```python
# íŒë§¤ ë°ì´í„° ìƒì„±
sales_data = [
    ("2025-09-11", "Alice", "Laptop", 1200, "Seoul"),
    ("2025-09-11", "Bob", "Mouse", 25, "Busan"),
    ("2025-09-11", "Charlie", "Keyboard", 80, "Seoul"),
    ("2025-09-12", "Alice", "Monitor", 300, "Seoul"),
    ("2025-09-12", "Bob", "Laptop", 1200, "Busan"),
    ("2025-09-12", "David", "Headphone", 150, "Daegu")
]

# DataFrame ìƒì„±
df_sales = spark.createDataFrame(sales_data, ["date", "customer", "product", "price", "city"])
df_sales.show()

# ê³ ê°ë³„ ì´ êµ¬ë§¤ì•¡ ê³„ì‚°
customer_total = df_sales.groupBy("customer") \
    .agg({"price": "sum"}) \
    .withColumnRenamed("sum(price)", "total_spent") \
    .orderBy("total_spent", ascending=False)
customer_total.show()

# ë„ì‹œë³„ í‰ê·  êµ¬ë§¤ì•¡
city_avg = df_sales.groupBy("city") \
    .agg({"price": "avg"}) \
    .withColumnRenamed("avg(price)", "avg_price") \
    .orderBy("avg_price", ascending=False)
city_avg.show()

# ê³ ê°€ ì œí’ˆ êµ¬ë§¤ ê³ ê° (1000ì› ì´ìƒ)
high_value_customers = df_sales.filter(df_sales.price >= 1000) \
    .select("customer", "product", "price") \
    .distinct()
high_value_customers.show()
```

### ì‹¤ìŠµ 3: ë³µì¡í•œ ë¶„ì„

```python
# ìœˆë„ìš° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ë¶„ì„
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, lag, sum as spark_sum

# ê³ ê°ë³„ êµ¬ë§¤ ìˆœìœ„ (ë„ì‹œë³„)
window_spec = Window.partitionBy("city").orderBy(df_sales.price.desc())

df_ranked = df_sales.withColumn("rank_in_city", rank().over(window_spec)) \
    .withColumn("row_number_in_city", row_number().over(window_spec))

print("ë„ì‹œë³„ êµ¬ë§¤ ìˆœìœ„:")
df_ranked.show()

# ê³ ê°ë³„ ëˆ„ì  êµ¬ë§¤ì•¡ ê³„ì‚°
window_cumulative = Window.partitionBy("customer").orderBy("date")

df_cumulative = df_sales.withColumn("cumulative_spent", 
    spark_sum("price").over(window_cumulative))

print("\nê³ ê°ë³„ ëˆ„ì  êµ¬ë§¤ì•¡:")
df_cumulative.show()

# ì „ì¼ ëŒ€ë¹„ êµ¬ë§¤ì•¡ ë³€í™”
window_lag = Window.partitionBy("customer").orderBy("date")

df_with_lag = df_sales.withColumn("prev_day_total", 
    lag(spark_sum("price").over(window_lag), 1).over(window_lag))

print("\nì „ì¼ ëŒ€ë¹„ êµ¬ë§¤ì•¡ ë³€í™”:")
df_with_lag.show()
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Spark ì•„í‚¤í…ì²˜ ì´í•´**
   - Driver, Cluster Manager, Worker Node
   - SparkContextì™€ SparkSession

2. **RDD (Resilient Distributed Dataset)**
   - RDDì˜ íŠ¹ì„±ê³¼ ìƒì„± ë°©ë²•
   - Transformationê³¼ Action ì—°ì‚°
   - ê³ ê¸‰ ì—°ì‚° (ê·¸ë£¹í™”, ì¡°ì¸)

3. **DataFrameê³¼ Dataset**
   - êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬
   - ìœˆë„ìš° í•¨ìˆ˜ì™€ ì¡°ì¸ ì—°ì‚°
   - ìµœì í™”ëœ ì‹¤í–‰ ì—”ì§„

4. **Spark SQL**
   - SQL ì¿¼ë¦¬ì™€ DataFrame API í†µí•©
   - ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ ì‘ì„±

5. **ì‹¤ìŠµ í”„ë¡œì íŠ¸**
   - ë¡œê·¸ ë°ì´í„° ë¶„ì„
   - êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬
   - ê³ ê¸‰ ë¶„ì„ ê¸°ë²•

### í•µì‹¬ ê°œë… ì •ë¦¬

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| **RDD** | ë¶„ì‚° ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ì¶”ìƒí™” | â­â­â­â­ |
| **DataFrame** | êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬ | â­â­â­â­â­ |
| **Spark SQL** | SQL ê¸°ë°˜ ë¶„ì„ | â­â­â­â­ |
| **Transformation/Action** | ì§€ì—° ì‹¤í–‰ ëª¨ë¸ | â­â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 2: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- UDF (User Defined Function) ì‘ì„±
- ê³ ê¸‰ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜
- íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™”
- ì‹¤ë¬´ í”„ë¡œì íŠ¸: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 2: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš©](/data-engineering/2025/09/12/apache-spark-batch-processing.html)

---

*ì´ì œ Sparkì˜ ê¸°ë³¸ê¸°ë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ë” ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.* ğŸš€

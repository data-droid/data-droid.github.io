---
layout: post
lang: ko
title: "Part 4: Apache Spark ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ íŠœë‹ - í”„ë¡œë•ì…˜ í™˜ê²½ ì™„ì„±"
description: "Apache Sparkì˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, í”„ë¡œíŒŒì¼ë§, ë©”ëª¨ë¦¬ ìµœì í™”, í´ëŸ¬ìŠ¤í„° íŠœë‹ì„ í†µí•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ì„ ì™„ì„±í•©ë‹ˆë‹¤."
date: 2025-09-13
author: Data Droid
category: data-engineering
tags: [Apache-Spark, ì„±ëŠ¥íŠœë‹, ëª¨ë‹ˆí„°ë§, í”„ë¡œíŒŒì¼ë§, ë©”ëª¨ë¦¬ìµœì í™”, í´ëŸ¬ìŠ¤í„°ê´€ë¦¬, Python]
series: apache-spark-complete-guide
series_order: 4
reading_time: "55ë¶„"
difficulty: "ì „ë¬¸ê°€"
---

# Part 4: Apache Spark ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ íŠœë‹ - í”„ë¡œë•ì…˜ í™˜ê²½ ì™„ì„±

> Apache Sparkì˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, í”„ë¡œíŒŒì¼ë§, ë©”ëª¨ë¦¬ ìµœì í™”, í´ëŸ¬ìŠ¤í„° íŠœë‹ì„ í†µí•œ í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•ì„ ì™„ì„±í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Spark UIì™€ ë©”íŠ¸ë¦­ ë¶„ì„](#spark-uiì™€-ë©”íŠ¸ë¦­-ë¶„ì„)
2. [ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ í”„ë¡œíŒŒì¼ë§](#ì„±ëŠ¥-ëª¨ë‹ˆí„°ë§ê³¼-í”„ë¡œíŒŒì¼ë§)
3. [ë©”ëª¨ë¦¬ ìµœì í™”ì™€ ìºì‹± ì „ëµ](#ë©”ëª¨ë¦¬-ìµœì í™”ì™€-ìºì‹±-ì „ëµ)
4. [í´ëŸ¬ìŠ¤í„° íŠœë‹ê³¼ í™•ì¥ì„±](#í´ëŸ¬ìŠ¤í„°-íŠœë‹ê³¼-í™•ì¥ì„±)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì„±ëŠ¥-ìµœì í™”-ì‹œìŠ¤í…œ)
6. [í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•](#í”„ë¡œë•ì…˜-í™˜ê²½-êµ¬ì¶•)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ–¥ï¸ Spark UIì™€ ë©”íŠ¸ë¦­ ë¶„ì„

### Spark UI ê°œìš”

Spark UIëŠ” Spark ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¶„ì„í•˜ëŠ” ì›¹ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

#### **í•µì‹¬ íƒ­ê³¼ ê¸°ëŠ¥**

1. **Jobs íƒ­**
   - ì‘ì—… ì‹¤í–‰ ìƒíƒœì™€ ì‹œê°„
   - ìŠ¤í…Œì´ì§€ë³„ ì‹¤í–‰ ì •ë³´
   - ì‘ì—… ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

2. **Stages íƒ­**
   - ê° ìŠ¤í…Œì´ì§€ì˜ ìƒì„¸ ì •ë³´
   - íƒœìŠ¤í¬ ì‹¤í–‰ ë¶„í¬
   - ë°ì´í„° ìŠ¤í(Data Skew) ë¶„ì„

3. **Storage íƒ­**
   - ìºì‹œëœ ë°ì´í„° ì •ë³´
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - ë””ìŠ¤í¬ ì €ì¥ ìƒíƒœ

4. **Environment íƒ­**
   - Spark ì„¤ì • ì •ë³´
   - ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜
   - JVM ì„¤ì •

### ë©”íŠ¸ë¦­ ë¶„ì„ ë„êµ¬

```python
# Spark ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„
from pyspark.sql import SparkSession
import json
import time

class SparkMetricsCollector:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.sc = spark_session.sparkContext
        
    def collect_job_metrics(self):
        """ì‘ì—… ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        status_tracker = self.sc.statusTracker()
        
        # ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì •ë³´
        active_jobs = status_tracker.getActiveJobIds()
        
        # ì‘ì—…ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        job_metrics = {}
        for job_id in active_jobs:
            job_info = status_tracker.getJobInfo(job_id)
            job_metrics[job_id] = {
                'status': job_info.status,
                'num_tasks': job_info.numTasks,
                'num_active_tasks': job_info.numActiveTasks,
                'num_completed_tasks': job_info.numCompletedTasks,
                'num_failed_tasks': job_info.numFailedTasks
            }
        
        return job_metrics
    
    def collect_stage_metrics(self):
        """ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        status_tracker = self.sc.statusTracker()
        
        # í™œì„± ìŠ¤í…Œì´ì§€ ì •ë³´
        active_stages = status_tracker.getActiveStageIds()
        
        stage_metrics = {}
        for stage_id in active_stages:
            stage_info = status_tracker.getStageInfo(stage_id)
            stage_metrics[stage_id] = {
                'num_tasks': stage_info.numTasks,
                'num_active_tasks': stage_info.numActiveTasks,
                'num_completed_tasks': stage_info.numCompletedTasks,
                'num_failed_tasks': stage_info.numFailedTasks,
                'executor_run_time': stage_info.executorRunTime,
                'executor_cpu_time': stage_info.executorCpuTime,
                'input_bytes': stage_info.inputBytes,
                'output_bytes': stage_info.outputBytes,
                'shuffle_read_bytes': stage_info.shuffleReadBytes,
                'shuffle_write_bytes': stage_info.shuffleWriteBytes
            }
        
        return stage_metrics
    
    def collect_executor_metrics(self):
        """ì‹¤í–‰ì ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        status_tracker = self.sc.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        executor_metrics = {}
        for executor_info in executor_infos:
            executor_metrics[executor_info.executorId] = {
                'host': executor_info.host,
                'total_cores': executor_info.totalCores,
                'max_memory': executor_info.maxMemory,
                'memory_used': executor_info.memoryUsed,
                'disk_used': executor_info.diskUsed,
                'active_tasks': executor_info.activeTasks,
                'completed_tasks': executor_info.completedTasks,
                'failed_tasks': executor_info.failedTasks,
                'total_duration': executor_info.totalDuration,
                'total_gc_time': executor_info.totalGCTime
            }
        
        return executor_metrics

# ì‚¬ìš© ì˜ˆì œ
def monitor_spark_application():
    spark = SparkSession.builder.appName("MetricsExample").getOrCreate()
    collector = SparkMetricsCollector(spark)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    while True:
        job_metrics = collector.collect_job_metrics()
        stage_metrics = collector.collect_stage_metrics()
        executor_metrics = collector.collect_executor_metrics()
        
        print("=== Job Metrics ===")
        print(json.dumps(job_metrics, indent=2))
        
        print("=== Stage Metrics ===")
        print(json.dumps(stage_metrics, indent=2))
        
        print("=== Executor Metrics ===")
        print(json.dumps(executor_metrics, indent=2))
        
        time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ í”„ë¡œíŒŒì¼ë§ {#ì„±ëŠ¥-ëª¨ë‹ˆí„°ë§ê³¼-í”„ë¡œíŒŒì¼ë§}

### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë„êµ¬

```python
# ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í´ë˜ìŠ¤
class SparkProfiler:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.sc = spark_session.sparkContext
        
    def profile_query_execution(self, query_name, df):
        """ì¿¼ë¦¬ ì‹¤í–‰ í”„ë¡œíŒŒì¼ë§"""
        import time
        
        start_time = time.time()
        
        # ì‹¤í–‰ ê³„íš ë¶„ì„
        execution_plan = df.explain(True)
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        result = df.collect()
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        status_tracker = self.sc.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        total_memory_used = sum(info.memoryUsed for info in executor_infos)
        total_gc_time = sum(info.totalGCTime for info in executor_infos)
        
        profile_result = {
            'query_name': query_name,
            'execution_time': execution_time,
            'total_memory_used': total_memory_used,
            'total_gc_time': total_gc_time,
            'execution_plan': execution_plan,
            'result_count': len(result)
        }
        
        return profile_result
    
    def analyze_data_skew(self, df, key_columns):
        """ë°ì´í„° ìŠ¤í ë¶„ì„"""
        # í‚¤ë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„
        key_counts = df.groupBy(*key_columns).count()
        
        # í†µê³„ ì •ë³´ ìˆ˜ì§‘
        stats = key_counts.select(
            count("*").alias("total_keys"),
            min("count").alias("min_count"),
            max("count").alias("max_count"),
            avg("count").alias("avg_count"),
            stddev("count").alias("stddev_count")
        ).collect()[0]
        
        # ìŠ¤í ë¹„ìœ¨ ê³„ì‚°
        skew_ratio = stats['max_count'] / stats['avg_count'] if stats['avg_count'] > 0 else 0
        
        skew_analysis = {
            'total_keys': stats['total_keys'],
            'min_count': stats['min_count'],
            'max_count': stats['max_count'],
            'avg_count': stats['avg_count'],
            'stddev_count': stats['stddev_count'],
            'skew_ratio': skew_ratio,
            'is_skewed': skew_ratio > 2.0  # ìŠ¤í ì„ê³„ê°’
        }
        
        return skew_analysis
    
    def monitor_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        status_tracker = self.sc.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        memory_stats = {
            'total_executors': len(executor_infos),
            'total_memory_used': sum(info.memoryUsed for info in executor_infos),
            'total_max_memory': sum(info.maxMemory for info in executor_infos),
            'memory_utilization': 0,
            'executor_details': []
        }
        
        for info in executor_infos:
            executor_detail = {
                'executor_id': info.executorId,
                'host': info.host,
                'memory_used': info.memoryUsed,
                'max_memory': info.maxMemory,
                'utilization': info.memoryUsed / info.maxMemory if info.maxMemory > 0 else 0
            }
            memory_stats['executor_details'].append(executor_detail)
        
        if memory_stats['total_max_memory'] > 0:
            memory_stats['memory_utilization'] = (
                memory_stats['total_memory_used'] / memory_stats['total_max_memory']
            )
        
        return memory_stats

# ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ ì‚¬ìš© ì˜ˆì œ
def performance_analysis_example():
    spark = SparkSession.builder.appName("PerformanceAnalysis").getOrCreate()
    profiler = SparkProfiler(spark)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = [(i, f"user_{i}", i * 10) for i in range(10000)]
    df = spark.createDataFrame(data, ["id", "name", "value"])
    
    # ì¿¼ë¦¬ í”„ë¡œíŒŒì¼ë§
    profile_result = profiler.profile_query_execution(
        "sample_aggregation",
        df.groupBy("name").agg(sum("value").alias("total_value"))
    )
    
    print("Query Profile Result:")
    print(json.dumps(profile_result, indent=2))
    
    # ë°ì´í„° ìŠ¤í ë¶„ì„
    skew_analysis = profiler.analyze_data_skew(df, ["name"])
    print("\nData Skew Analysis:")
    print(json.dumps(skew_analysis, indent=2))
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    memory_stats = profiler.monitor_memory_usage()
    print("\nMemory Usage Stats:")
    print(json.dumps(memory_stats, indent=2))
```

### ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œë¼ì¸

```python
# ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
class PerformanceOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
        
    def check_partitioning(self, df):
        """íŒŒí‹°ì…”ë‹ ìµœì í™” ì²´í¬"""
        num_partitions = df.rdd.getNumPartitions()
        
        # íŒŒí‹°ì…˜ í¬ê¸° í™•ì¸
        partition_sizes = df.rdd.mapPartitions(lambda x: [len(list(x))]).collect()
        
        avg_size = sum(partition_sizes) / len(partition_sizes)
        max_size = max(partition_sizes)
        min_size = min(partition_sizes)
        
        # íŒŒí‹°ì…˜ ë¶ˆê· í˜• í™•ì¸
        imbalance_ratio = max_size / avg_size if avg_size > 0 else 0
        
        recommendations = []
        
        if num_partitions < 2:
            recommendations.append("íŒŒí‹°ì…˜ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìµœì†Œ 2ê°œ ì´ìƒ ê¶Œì¥")
        
        if imbalance_ratio > 2.0:
            recommendations.append(f"íŒŒí‹°ì…˜ ë¶ˆê· í˜•ì´ ì‹¬í•©ë‹ˆë‹¤ (ë¹„ìœ¨: {imbalance_ratio:.2f}). repartition() ê³ ë ¤")
        
        if avg_size > 128 * 1024 * 1024:  # 128MB
            recommendations.append("íŒŒí‹°ì…˜ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í•  ê¶Œì¥")
        
        return {
            'num_partitions': num_partitions,
            'avg_size': avg_size,
            'max_size': max_size,
            'min_size': min_size,
            'imbalance_ratio': imbalance_ratio,
            'recommendations': recommendations
        }
    
    def check_caching_strategy(self, df, usage_count=1):
        """ìºì‹± ì „ëµ ì²´í¬"""
        storage_level = df.storageLevel
        
        recommendations = []
        
        if usage_count > 1 and storage_level == StorageLevel.NONE:
            recommendations.append("ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•˜ë¯€ë¡œ ìºì‹±ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        if storage_level.useDisk and usage_count < 3:
            recommendations.append("ë””ìŠ¤í¬ ìºì‹±ì€ ë©”ëª¨ë¦¬ ìºì‹±ë³´ë‹¤ ëŠë¦½ë‹ˆë‹¤. ì‚¬ìš© ë¹ˆë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
        
        return {
            'storage_level': str(storage_level),
            'usage_count': usage_count,
            'recommendations': recommendations
        }
    
    def analyze_query_plan(self, df):
        """ì¿¼ë¦¬ ê³„íš ë¶„ì„"""
        plan = df.explain(True)
        
        recommendations = []
        
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ í™•ì¸
        if "BroadcastHashJoin" in plan:
            recommendations.append("ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì€ í…Œì´ë¸” í¬ê¸°ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        
        # ì…”í”Œ í™•ì¸
        if "Exchange" in plan:
            recommendations.append("ì…”í”Œì´ ë°œìƒí•©ë‹ˆë‹¤. íŒŒí‹°ì…”ë‹ í‚¤ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
        
        # ìŠ¤ìº” í™•ì¸
        if "FileScan" in plan:
            recommendations.append("íŒŒì¼ ìŠ¤ìº”ì´ ë°œìƒí•©ë‹ˆë‹¤. íŒŒí‹°ì…”ë‹ì´ë‚˜ ì¸ë±ì‹±ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        return {
            'execution_plan': plan,
            'recommendations': recommendations
        }

# ì„±ëŠ¥ ìµœì í™” ì˜ˆì œ
def optimization_example():
    spark = SparkSession.builder.appName("OptimizationExample").getOrCreate()
    optimizer = PerformanceOptimizer(spark)
    
    # ìƒ˜í”Œ ë°ì´í„°
    data = [(i, f"category_{i % 10}", i * 100) for i in range(100000)]
    df = spark.createDataFrame(data, ["id", "category", "value"])
    
    # íŒŒí‹°ì…”ë‹ ì²´í¬
    partition_analysis = optimizer.check_partitioning(df)
    print("Partitioning Analysis:")
    print(json.dumps(partition_analysis, indent=2))
    
    # ìºì‹± ì „ëµ ì²´í¬
    caching_analysis = optimizer.check_caching_strategy(df, usage_count=3)
    print("\nCaching Strategy Analysis:")
    print(json.dumps(caching_analysis, indent=2))
    
    # ì¿¼ë¦¬ ê³„íš ë¶„ì„
    query_analysis = optimizer.analyze_query_plan(
        df.groupBy("category").agg(sum("value").alias("total"))
    )
    print("\nQuery Plan Analysis:")
    print(json.dumps(query_analysis, indent=2))
```

## ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”ì™€ ìºì‹± ì „ëµ

### ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”

```python
# ë©”ëª¨ë¦¬ ìµœì í™” ë„êµ¬
class MemoryOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
        
    def optimize_dataframe_memory(self, df):
        """DataFrame ë©”ëª¨ë¦¬ ìµœì í™”"""
        from pyspark.sql.types import IntegerType, LongType, FloatType, DoubleType
        
        optimized_df = df
        
        # ì»¬ëŸ¼ íƒ€ì… ìµœì í™”
        for field in df.schema.fields:
            field_name = field.name
            field_type = field.dataType
            
            if isinstance(field_type, IntegerType):
                # í° ì •ìˆ˜ê°’ì´ ì—†ë‹¤ë©´ ë” ì‘ì€ íƒ€ì…ìœ¼ë¡œ ë³€ê²½
                max_val = df.select(max(col(field_name))).collect()[0][0]
                min_val = df.select(min(col(field_name))).collect()[0][0]
                
                if -128 <= min_val <= 127 and -128 <= max_val <= 127:
                    optimized_df = optimized_df.withColumn(
                        field_name, col(field_name).cast("tinyint")
                    )
                elif -32768 <= min_val <= 32767 and -32768 <= max_val <= 32767:
                    optimized_df = optimized_df.withColumn(
                        field_name, col(field_name).cast("smallint")
                    )
            
            elif isinstance(field_type, FloatType):
                # Floatë¥¼ Doubleë¡œ ë³€ê²½í•˜ì—¬ ì •ë°€ë„ í–¥ìƒ
                optimized_df = optimized_df.withColumn(
                    field_name, col(field_name).cast("double")
                )
        
        return optimized_df
    
    def calculate_memory_usage(self, df):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        # DataFrame í¬ê¸° ì¶”ì •
        num_rows = df.count()
        num_cols = len(df.columns)
        
        # ì»¬ëŸ¼ë³„ íƒ€ì… í¬ê¸° ì¶”ì •
        type_sizes = {
            'string': 50,  # í‰ê·  ë¬¸ìì—´ ê¸¸ì´
            'int': 4,
            'bigint': 8,
            'double': 8,
            'float': 4,
            'boolean': 1,
            'date': 8,
            'timestamp': 8
        }
        
        estimated_size = 0
        for field in df.schema.fields:
            field_type = str(field.dataType)
            base_type = field_type.split('(')[0].lower()
            
            if base_type in type_sizes:
                estimated_size += type_sizes[base_type]
            else:
                estimated_size += 8  # ê¸°ë³¸ê°’
        
        total_estimated_size = num_rows * num_cols * estimated_size
        
        return {
            'num_rows': num_rows,
            'num_cols': num_cols,
            'estimated_size_bytes': total_estimated_size,
            'estimated_size_mb': total_estimated_size / (1024 * 1024)
        }
    
    def optimize_caching_strategy(self, df, access_pattern):
        """ìºì‹± ì „ëµ ìµœì í™”"""
        from pyspark import StorageLevel
        
        memory_stats = self.calculate_memory_usage(df)
        size_mb = memory_stats['estimated_size_mb']
        
        recommendations = []
        
        if access_pattern['frequency'] == 'high' and size_mb < 1000:
            recommendations.append({
                'strategy': 'MEMORY_ONLY',
                'reason': 'ìì£¼ ì‚¬ìš©ë˜ê³  í¬ê¸°ê°€ ì‘ì€ ë°ì´í„°'
            })
        elif access_pattern['frequency'] == 'high' and size_mb >= 1000:
            recommendations.append({
                'strategy': 'MEMORY_AND_DISK_SER',
                'reason': 'ìì£¼ ì‚¬ìš©ë˜ì§€ë§Œ í¬ê¸°ê°€ í° ë°ì´í„°'
            })
        elif access_pattern['frequency'] == 'medium':
            recommendations.append({
                'strategy': 'DISK_ONLY',
                'reason': 'ì¤‘ê°„ ë¹ˆë„ ì‚¬ìš© ë°ì´í„°'
            })
        else:
            recommendations.append({
                'strategy': 'NO_CACHING',
                'reason': 'ë‚®ì€ ë¹ˆë„ ì‚¬ìš© ë°ì´í„°'
            })
        
        return {
            'data_size_mb': size_mb,
            'access_frequency': access_pattern['frequency'],
            'recommendations': recommendations
        }

# ë©”ëª¨ë¦¬ ìµœì í™” ì˜ˆì œ
def memory_optimization_example():
    spark = SparkSession.builder.appName("MemoryOptimization").getOrCreate()
    optimizer = MemoryOptimizer(spark)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = [(i, f"user_{i}", i * 1.5, i % 2 == 0) for i in range(100000)]
    df = spark.createDataFrame(data, ["id", "name", "score", "is_active"])
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    optimized_df = optimizer.optimize_dataframe_memory(df)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
    original_memory = optimizer.calculate_memory_usage(df)
    optimized_memory = optimizer.calculate_memory_usage(optimized_df)
    
    print("Original Memory Usage:")
    print(json.dumps(original_memory, indent=2))
    
    print("\nOptimized Memory Usage:")
    print(json.dumps(optimized_memory, indent=2))
    
    # ìºì‹± ì „ëµ ìµœì í™”
    access_pattern = {'frequency': 'high'}
    caching_strategy = optimizer.optimize_caching_strategy(df, access_pattern)
    
    print("\nCaching Strategy:")
    print(json.dumps(caching_strategy, indent=2))
```

### ê³ ê¸‰ ìºì‹± ì „ëµ

```python
# ê³ ê¸‰ ìºì‹± ê´€ë¦¬ì
class AdvancedCacheManager:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.cached_tables = {}
        
    def smart_cache(self, df, table_name, access_pattern):
        """ì§€ëŠ¥í˜• ìºì‹±"""
        from pyspark import StorageLevel
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        num_partitions = df.rdd.getNumPartitions()
        
        # ì ‘ê·¼ íŒ¨í„´ì— ë”°ë¥¸ ìºì‹± ì „ëµ ê²°ì •
        if access_pattern['frequency'] == 'high':
            if access_pattern['latency_requirement'] == 'low':
                df.cache()  # MEMORY_ONLY
                storage_level = "MEMORY_ONLY"
            else:
                df.persist(StorageLevel.MEMORY_AND_DISK_SER)
                storage_level = "MEMORY_AND_DISK_SER"
        else:
            df.persist(StorageLevel.DISK_ONLY)
            storage_level = "DISK_ONLY"
        
        # ìºì‹œ ì •ë³´ ì €ì¥
        self.cached_tables[table_name] = {
            'dataframe': df,
            'storage_level': storage_level,
            'access_count': 0,
            'last_accessed': time.time()
        }
        
        return df
    
    def monitor_cache_efficiency(self):
        """ìºì‹œ íš¨ìœ¨ì„± ëª¨ë‹ˆí„°ë§"""
        status_tracker = self.spark.sparkContext.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        cache_stats = {
            'total_cached_data': 0,
            'memory_cached': 0,
            'disk_cached': 0,
            'cache_hit_ratio': 0
        }
        
        for info in executor_infos:
            cache_stats['total_cached_data'] += info.memoryUsed
            cache_stats['memory_cached'] += info.memoryUsed
        
        return cache_stats
    
    def cleanup_unused_cache(self, max_age_hours=24):
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìºì‹œ ì •ë¦¬"""
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        tables_to_remove = []
        
        for table_name, cache_info in self.cached_tables.items():
            if current_time - cache_info['last_accessed'] > max_age_seconds:
                cache_info['dataframe'].unpersist()
                tables_to_remove.append(table_name)
        
        for table_name in tables_to_remove:
            del self.cached_tables[table_name]
        
        return len(tables_to_remove)

# ê³ ê¸‰ ìºì‹± ì˜ˆì œ
def advanced_caching_example():
    spark = SparkSession.builder.appName("AdvancedCaching").getOrCreate()
    cache_manager = AdvancedCacheManager(spark)
    
    # ìƒ˜í”Œ ë°ì´í„°
    data = [(i, f"category_{i % 5}", i * 10) for i in range(10000)]
    df = spark.createDataFrame(data, ["id", "category", "value"])
    
    # ì§€ëŠ¥í˜• ìºì‹±
    access_pattern = {
        'frequency': 'high',
        'latency_requirement': 'low'
    }
    
    cached_df = cache_manager.smart_cache(df, "sample_table", access_pattern)
    
    # ìºì‹œ íš¨ìœ¨ì„± ëª¨ë‹ˆí„°ë§
    cache_stats = cache_manager.monitor_cache_efficiency()
    print("Cache Statistics:")
    print(json.dumps(cache_stats, indent=2))
    
    # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìºì‹œ ì •ë¦¬
    cleaned_count = cache_manager.cleanup_unused_cache(max_age_hours=1)
    print(f"\nCleaned {cleaned_count} unused cache entries")
```

## ğŸ” ê³ ê¸‰ ì„±ëŠ¥ ë¶„ì„ ë„êµ¬

### Spark History Server í™œìš©ë²•

```python
# Spark History Server ì„¤ì • ë° í™œìš©
class HistoryServerAnalyzer:
    def __init__(self, history_server_url):
        self.history_server_url = history_server_url
        
    def analyze_completed_applications(self):
        """ì™„ë£Œëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¶„ì„"""
        import requests
        
        # History Server APIë¥¼ í†µí•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ëª©ë¡ ì¡°íšŒ
        apps_response = requests.get(f"{self.history_server_url}/api/v1/applications")
        applications = apps_response.json()
        
        analysis_results = []
        for app in applications:
            app_id = app['id']
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            app_details = requests.get(f"{self.history_server_url}/api/v1/applications/{app_id}")
            app_info = app_details.json()
            
            # ì‘ì—… ì •ë³´ ì¡°íšŒ
            jobs_response = requests.get(f"{self.history_server_url}/api/v1/applications/{app_id}/jobs")
            jobs = jobs_response.json()
            
            analysis_result = {
                'app_id': app_id,
                'app_name': app_info.get('name', 'Unknown'),
                'start_time': app_info.get('attempts', [{}])[0].get('startTime'),
                'duration': app_info.get('attempts', [{}])[0].get('duration'),
                'total_jobs': len(jobs),
                'failed_jobs': len([j for j in jobs if j.get('status') == 'FAILED']),
                'performance_issues': self._identify_performance_issues(jobs)
            }
            
            analysis_results.append(analysis_result)
        
        return analysis_results
    
    def _identify_performance_issues(self, jobs):
        """ì„±ëŠ¥ ë¬¸ì œ ì‹ë³„"""
        issues = []
        
        for job in jobs:
            # ëŠë¦° ì‘ì—… ì‹ë³„ (10ë¶„ ì´ìƒ)
            if job.get('duration', 0) > 600000:  # 10ë¶„ = 600,000ms
                issues.append({
                    'type': 'slow_job',
                    'job_id': job.get('jobId'),
                    'duration': job.get('duration'),
                    'message': f"ì‘ì—… {job.get('jobId')}ê°€ {job.get('duration')/1000:.1f}ì´ˆ ì†Œìš”"
                })
            
            # ì‹¤íŒ¨í•œ ì‘ì—… ì‹ë³„
            if job.get('status') == 'FAILED':
                issues.append({
                    'type': 'failed_job',
                    'job_id': job.get('jobId'),
                    'message': f"ì‘ì—… {job.get('jobId')} ì‹¤íŒ¨"
                })
        
        return issues
    
    def generate_performance_report(self, app_id):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        import requests
        
        # ìŠ¤í…Œì´ì§€ ì •ë³´ ì¡°íšŒ
        stages_response = requests.get(f"{self.history_server_url}/api/v1/applications/{app_id}/stages")
        stages = stages_response.json()
        
        # ì‹¤í–‰ì ì •ë³´ ì¡°íšŒ
        executors_response = requests.get(f"{self.history_server_url}/api/v1/applications/{app_id}/executors")
        executors = executors_response.json()
        
        report = {
            'total_stages': len(stages),
            'total_executors': len(executors),
            'stage_analysis': self._analyze_stages(stages),
            'executor_analysis': self._analyze_executors(executors),
            'recommendations': self._generate_recommendations(stages, executors)
        }
        
        return report
    
    def _analyze_stages(self, stages):
        """ìŠ¤í…Œì´ì§€ ë¶„ì„"""
        stage_metrics = {
            'total_tasks': sum(s.get('numTasks', 0) for s in stages),
            'failed_tasks': sum(s.get('numFailedTasks', 0) for s in stages),
            'avg_task_duration': 0,
            'slow_stages': []
        }
        
        total_duration = 0
        stage_count = 0
        
        for stage in stages:
            if stage.get('numTasks', 0) > 0:
                avg_task_duration = stage.get('executorRunTime', 0) / stage.get('numTasks', 1)
                total_duration += avg_task_duration
                stage_count += 1
                
                # ëŠë¦° ìŠ¤í…Œì´ì§€ ì‹ë³„ (í‰ê·  íƒœìŠ¤í¬ ì‹œê°„ 30ì´ˆ ì´ìƒ)
                if avg_task_duration > 30000:
                    stage_metrics['slow_stages'].append({
                        'stage_id': stage.get('stageId'),
                        'avg_task_duration': avg_task_duration,
                        'num_tasks': stage.get('numTasks')
                    })
        
        if stage_count > 0:
            stage_metrics['avg_task_duration'] = total_duration / stage_count
        
        return stage_metrics
    
    def _analyze_executors(self, executors):
        """ì‹¤í–‰ì ë¶„ì„"""
        executor_metrics = {
            'total_cores': sum(e.get('totalCores', 0) for e in executors),
            'total_memory': sum(e.get('maxMemory', 0) for e in executors),
            'memory_utilization': 0,
            'gc_time_ratio': 0
        }
        
        total_used_memory = sum(e.get('memoryUsed', 0) for e in executors)
        total_gc_time = sum(e.get('totalGCTime', 0) for e in executors)
        total_executor_time = sum(e.get('totalDuration', 0) for e in executors)
        
        if executor_metrics['total_memory'] > 0:
            executor_metrics['memory_utilization'] = total_used_memory / executor_metrics['total_memory']
        
        if total_executor_time > 0:
            executor_metrics['gc_time_ratio'] = total_gc_time / total_executor_time
        
        return executor_metrics
    
    def _generate_recommendations(self, stages, executors):
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ì€ ê²½ìš°
        total_memory = sum(e.get('maxMemory', 0) for e in executors)
        used_memory = sum(e.get('memoryUsed', 0) for e in executors)
        if total_memory > 0 and used_memory / total_memory > 0.8:
            recommendations.append({
                'type': 'memory',
                'priority': 'high',
                'message': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. executor-memory ì¦ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”'
            })
        
        # GC ì‹œê°„ì´ ê¸´ ê²½ìš°
        total_gc_time = sum(e.get('totalGCTime', 0) for e in executors)
        total_executor_time = sum(e.get('totalDuration', 0) for e in executors)
        if total_executor_time > 0 and total_gc_time / total_executor_time > 0.1:
            recommendations.append({
                'type': 'gc',
                'priority': 'medium',
                'message': 'GC ì‹œê°„ì´ ì „ì²´ ì‹¤í–‰ ì‹œê°„ì˜ 10% ì´ìƒì…ë‹ˆë‹¤. JVM íŠœë‹ì„ ê³ ë ¤í•˜ì„¸ìš”'
            })
        
        # ëŠë¦° ìŠ¤í…Œì´ì§€ê°€ ìˆëŠ” ê²½ìš°
        slow_stages = [s for s in stages if s.get('executorRunTime', 0) / max(s.get('numTasks', 1), 1) > 30000]
        if slow_stages:
            recommendations.append({
                'type': 'performance',
                'priority': 'medium',
                'message': f'{len(slow_stages)}ê°œì˜ ìŠ¤í…Œì´ì§€ê°€ ëŠë¦½ë‹ˆë‹¤. íŒŒí‹°ì…”ë‹ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”'
            })
        
        return recommendations

# History Server í™œìš© ì˜ˆì œ
def history_server_analysis_example():
    analyzer = HistoryServerAnalyzer("http://localhost:18080")
    
    # ì™„ë£Œëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¶„ì„
    completed_apps = analyzer.analyze_completed_applications()
    print("=== Completed Applications Analysis ===")
    for app in completed_apps:
        print(f"App: {app['app_name']} ({app['app_id']})")
        print(f"  Duration: {app['duration']/1000:.1f}s")
        print(f"  Jobs: {app['total_jobs']} (Failed: {app['failed_jobs']})")
        if app['performance_issues']:
            print("  Performance Issues:")
            for issue in app['performance_issues']:
                print(f"    - {issue['message']}")
        print()
    
    # íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„¸ ë¶„ì„
    if completed_apps:
        app_id = completed_apps[0]['app_id']
        report = analyzer.generate_performance_report(app_id)
        print("=== Performance Report ===")
        print(json.dumps(report, indent=2))
```

## ğŸš¨ ì‹¤ë¬´ ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°

### Data Skew ë¬¸ì œ í•´ê²°

```python
# Data Skew ì§„ë‹¨ ë° í•´ê²° ë„êµ¬
class DataSkewResolver:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def detect_data_skew(self, df, key_columns):
        """ë°ì´í„° ìŠ¤í ê°ì§€"""
        from pyspark.sql.functions import col, count, min, max, avg, stddev
        
        # í‚¤ë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„
        key_counts = df.groupBy(*key_columns).count()
        
        # í†µê³„ ê³„ì‚°
        stats = key_counts.select(
            count("*").alias("total_keys"),
            min("count").alias("min_count"),
            max("count").alias("max_count"),
            avg("count").alias("avg_count"),
            stddev("count").alias("stddev_count")
        ).collect()[0]
        
        skew_ratio = stats['max_count'] / stats['avg_count'] if stats['avg_count'] > 0 else 0
        
        return {
            'total_keys': stats['total_keys'],
            'min_count': stats['min_count'],
            'max_count': stats['max_count'],
            'avg_count': stats['avg_count'],
            'stddev_count': stats['stddev_count'],
            'skew_ratio': skew_ratio,
            'is_skewed': skew_ratio > 2.0,
            'severity': self._get_skew_severity(skew_ratio)
        }
    
    def _get_skew_severity(self, skew_ratio):
        """ìŠ¤í ì‹¬ê°ë„ íŒì •"""
        if skew_ratio > 10:
            return "critical"
        elif skew_ratio > 5:
            return "high"
        elif skew_ratio > 2:
            return "medium"
        else:
            return "low"
    
    def resolve_data_skew(self, df, key_columns, method="salting"):
        """ë°ì´í„° ìŠ¤í í•´ê²°"""
        if method == "salting":
            return self._apply_salting(df, key_columns)
        elif method == "random_repartition":
            return self._apply_random_repartition(df, key_columns)
        elif method == "adaptive_query_execution":
            return self._enable_adaptive_query_execution(df)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _apply_salting(self, df, key_columns):
        """Salting ê¸°ë²• ì ìš©"""
        from pyspark.sql.functions import col, rand, concat, lit
        
        # Salt ê°’ ìƒì„± (0-99 ì‚¬ì´ì˜ ëœë¤ ê°’)
        salted_df = df.withColumn("salt", (rand() * 100).cast("int"))
        
        # í‚¤ ì»¬ëŸ¼ì— salt ì¶”ê°€
        salted_keys = [concat(col(key), lit("_"), col("salt")) for key in key_columns]
        salted_keys.append(col("salt"))
        
        return salted_df.select(*salted_keys, *[col(c) for c in df.columns if c not in key_columns])
    
    def _apply_random_repartition(self, df, key_columns):
        """ëœë¤ ë¦¬íŒŒí‹°ì…”ë‹ ì ìš©"""
        # í˜„ì¬ íŒŒí‹°ì…˜ ìˆ˜ì˜ 2ë°°ë¡œ ì¦ê°€
        current_partitions = df.rdd.getNumPartitions()
        new_partitions = current_partitions * 2
        
        return df.repartition(new_partitions)
    
    def _enable_adaptive_query_execution(self, df):
        """ì ì‘í˜• ì¿¼ë¦¬ ì‹¤í–‰ í™œì„±í™”"""
        # Spark ì„¤ì • ë³€ê²½
        self.spark.conf.set("spark.sql.adaptive.enabled", "true")
        self.spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        self.spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
        self.spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionFactor", "5")
        self.spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "256MB")
        
        return df

# Data Skew í•´ê²° ì˜ˆì œ
def data_skew_resolution_example():
    spark = SparkSession.builder.appName("DataSkewResolution").getOrCreate()
    resolver = DataSkewResolver(spark)
    
    # ìŠ¤íê°€ ìˆëŠ” ë°ì´í„° ìƒì„± (ì¼ë¶€ í‚¤ì— ëŒ€ëŸ‰ì˜ ë°ì´í„°)
    data = []
    for i in range(100000):
        if i < 1000:  # ì²˜ìŒ 1000ê°œëŠ” í‚¤ 0ì— í• ë‹¹ (ìŠ¤í ìƒì„±)
            data.append((0, f"data_{i}"))
        else:
            data.append((i % 100, f"data_{i}"))  # ë‚˜ë¨¸ì§€ëŠ” ê· ë“± ë¶„í¬
    
    df = spark.createDataFrame(data, ["key", "value"])
    
    # ìŠ¤í ê°ì§€
    skew_analysis = resolver.detect_data_skew(df, ["key"])
    print("=== Data Skew Analysis ===")
    print(json.dumps(skew_analysis, indent=2))
    
    if skew_analysis['is_skewed']:
        print(f"\nData skew detected with ratio: {skew_analysis['skew_ratio']:.2f}")
        print(f"Severity: {skew_analysis['severity']}")
        
        # ìŠ¤í í•´ê²°
        resolved_df = resolver.resolve_data_skew(df, ["key"], method="salting")
        
        # í•´ê²° í›„ ìŠ¤í ì¬ì¸¡ì •
        resolved_skew = resolver.detect_data_skew(resolved_df, ["key", "salt"])
        print("\n=== After Skew Resolution ===")
        print(json.dumps(resolved_skew, indent=2))
```

### Slow Task ë¶„ì„ ë° ìµœì í™”

```python
# Slow Task ë¶„ì„ ë° ìµœì í™” ë„êµ¬
class SlowTaskAnalyzer:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def analyze_slow_tasks(self, df, threshold_seconds=30):
        """ëŠë¦° íƒœìŠ¤í¬ ë¶„ì„"""
        from pyspark.sql.functions import col, count, min, max, avg, stddev
        
        # íŒŒí‹°ì…˜ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        def measure_partition_processing(iterator):
            import time
            start_time = time.time()
            
            # ë°ì´í„° ì²˜ë¦¬
            data = list(iterator)
            processed_data = [row for row in data]  # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            return [(processing_time, len(data))]
        
        # ê° íŒŒí‹°ì…˜ì˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        partition_metrics = df.rdd.mapPartitions(measure_partition_processing).collect()
        
        if not partition_metrics:
            return {"message": "No data to analyze"}
        
        processing_times = [metric[0] for metric in partition_metrics]
        data_sizes = [metric[1] for metric in partition_metrics]
        
        slow_partitions = [
            (i, time, size) for i, (time, size) in enumerate(partition_metrics)
            if time > threshold_seconds
        ]
        
        analysis = {
            'total_partitions': len(partition_metrics),
            'slow_partitions': len(slow_partitions),
            'avg_processing_time': sum(processing_times) / len(processing_times),
            'max_processing_time': max(processing_times),
            'min_processing_time': min(processing_times),
            'stddev_processing_time': self._calculate_stddev(processing_times),
            'slow_partition_details': slow_partitions,
            'recommendations': self._generate_slow_task_recommendations(slow_partitions, partition_metrics)
        }
        
        return analysis
    
    def _calculate_stddev(self, values):
        """í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if len(values) < 2:
            return 0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5
    
    def _generate_slow_task_recommendations(self, slow_partitions, all_partitions):
        """ëŠë¦° íƒœìŠ¤í¬ í•´ê²° ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if slow_partitions:
            recommendations.append({
                'type': 'repartitioning',
                'priority': 'high',
                'message': f'{len(slow_partitions)}ê°œì˜ íŒŒí‹°ì…˜ì´ ëŠë¦½ë‹ˆë‹¤. repartition() ë˜ëŠ” coalesce()ë¥¼ ê³ ë ¤í•˜ì„¸ìš”'
            })
        
        # íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜• í™•ì¸
        data_sizes = [size for _, size in all_partitions]
        if len(data_sizes) > 1:
            size_ratio = max(data_sizes) / min(data_sizes)
            if size_ratio > 5:
                recommendations.append({
                    'type': 'data_distribution',
                    'priority': 'medium',
                    'message': f'íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜•ì´ ì‹¬í•©ë‹ˆë‹¤ (ë¹„ìœ¨: {size_ratio:.2f}). ë°ì´í„° ë¶„í¬ë¥¼ ê°œì„ í•˜ì„¸ìš”'
                })
        
        # ì „ì²´ì ì¸ ì²˜ë¦¬ ì‹œê°„ ë¶„ì‚° í™•ì¸
        processing_times = [time for time, _ in all_partitions]
        if len(processing_times) > 1:
            time_ratio = max(processing_times) / min(processing_times)
            if time_ratio > 10:
                recommendations.append({
                    'type': 'performance_optimization',
                    'priority': 'medium',
                    'message': f'ì²˜ë¦¬ ì‹œê°„ ë¶„ì‚°ì´ í½ë‹ˆë‹¤ (ë¹„ìœ¨: {time_ratio:.2f}). ë¡œì§ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”'
                })
        
        return recommendations
    
    def optimize_partitioning(self, df, target_partition_size_mb=128):
        """íŒŒí‹°ì…”ë‹ ìµœì í™”"""
        # í˜„ì¬ ë°ì´í„° í¬ê¸° ì¶”ì •
        row_count = df.count()
        estimated_row_size_bytes = 100  # ì¶”ì •ê°’ (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ê³„ì‚° í•„ìš”)
        total_size_mb = (row_count * estimated_row_size_bytes) / (1024 * 1024)
        
        # ìµœì  íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚°
        optimal_partitions = max(1, int(total_size_mb / target_partition_size_mb))
        current_partitions = df.rdd.getNumPartitions()
        
        if optimal_partitions != current_partitions:
            if optimal_partitions > current_partitions:
                # íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
                optimized_df = df.repartition(optimal_partitions)
                action = f"repartitioned from {current_partitions} to {optimal_partitions} partitions"
            else:
                # íŒŒí‹°ì…˜ ìˆ˜ ê°ì†Œ
                optimized_df = df.coalesce(optimal_partitions)
                action = f"coalesced from {current_partitions} to {optimal_partitions} partitions"
        else:
            optimized_df = df
            action = "no partitioning change needed"
        
        return {
            'optimized_dataframe': optimized_df,
            'original_partitions': current_partitions,
            'optimized_partitions': optimal_partitions,
            'action_taken': action,
            'estimated_total_size_mb': total_size_mb
        }

# Slow Task ë¶„ì„ ì˜ˆì œ
def slow_task_analysis_example():
    spark = SparkSession.builder.appName("SlowTaskAnalysis").getOrCreate()
    analyzer = SlowTaskAnalyzer(spark)
    
    # ë¶ˆê· ë“±í•œ ë°ì´í„° ìƒì„± (ì¼ë¶€ íŒŒí‹°ì…˜ì— ë” ë§ì€ ë°ì´í„°)
    data = []
    for i in range(100000):
        # ì²˜ìŒ 50%ëŠ” íŒŒí‹°ì…˜ 0ì—, ë‚˜ë¨¸ì§€ëŠ” ê· ë“± ë¶„í¬
        if i < 50000:
            data.append((0, f"heavy_data_{i}", i * 100))
        else:
            data.append((i % 10, f"light_data_{i}", i))
    
    df = spark.createDataFrame(data, ["partition_key", "data", "value"])
    
    # ëŠë¦° íƒœìŠ¤í¬ ë¶„ì„
    slow_task_analysis = analyzer.analyze_slow_tasks(df, threshold_seconds=1)
    print("=== Slow Task Analysis ===")
    print(json.dumps(slow_task_analysis, indent=2))
    
    # íŒŒí‹°ì…”ë‹ ìµœì í™”
    optimization_result = analyzer.optimize_partitioning(df)
    print("\n=== Partitioning Optimization ===")
    print(f"Original partitions: {optimization_result['original_partitions']}")
    print(f"Optimized partitions: {optimization_result['optimized_partitions']}")
    print(f"Action taken: {optimization_result['action_taken']}")
    print(f"Estimated total size: {optimization_result['estimated_total_size_mb']:.2f} MB")
```

### Shuffle Spill ë¬¸ì œ í•´ê²°

```python
# Shuffle Spill ë¬¸ì œ í•´ê²° ë„êµ¬
class ShuffleSpillResolver:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def detect_shuffle_spill(self, df):
        """Shuffle Spill ê°ì§€"""
        # Spark ì„¤ì •ì—ì„œ spill ê´€ë ¨ ì„¤ì • í™•ì¸
        spill_settings = {
            'spark.sql.adaptive.enabled': self.spark.conf.get('spark.sql.adaptive.enabled', 'false'),
            'spark.sql.adaptive.coalescePartitions.enabled': self.spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled', 'false'),
            'spark.sql.adaptive.advisoryPartitionSizeInBytes': self.spark.conf.get('spark.sql.adaptive.advisoryPartitionSizeInBytes', '64MB'),
            'spark.sql.adaptive.skewJoin.enabled': self.spark.conf.get('spark.sql.adaptive.skewJoin.enabled', 'false')
        }
        
        return {
            'spill_settings': spill_settings,
            'recommendations': self._generate_spill_recommendations(spill_settings)
        }
    
    def _generate_spill_recommendations(self, settings):
        """Spill í•´ê²° ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if settings['spark.sql.adaptive.enabled'] == 'false':
            recommendations.append({
                'type': 'adaptive_query',
                'priority': 'high',
                'message': 'Adaptive Query Executionì„ í™œì„±í™”í•˜ì—¬ ìë™ ìµœì í™”ë¥¼ í™œìš©í•˜ì„¸ìš”',
                'config': 'spark.sql.adaptive.enabled=true'
            })
        
        if settings['spark.sql.adaptive.coalescePartitions.enabled'] == 'false':
            recommendations.append({
                'type': 'partition_coalescing',
                'priority': 'medium',
                'message': 'íŒŒí‹°ì…˜ ê²°í•©ì„ í™œì„±í™”í•˜ì—¬ ì‘ì€ íŒŒí‹°ì…˜ë“¤ì„ í•©ì¹˜ì„¸ìš”',
                'config': 'spark.sql.adaptive.coalescePartitions.enabled=true'
            })
        
        # íŒŒí‹°ì…˜ í¬ê¸° ì„¤ì • í™•ì¸
        partition_size = settings['spark.sql.adaptive.advisoryPartitionSizeInBytes']
        if '64MB' in partition_size:
            recommendations.append({
                'type': 'partition_size',
                'priority': 'medium',
                'message': 'íŒŒí‹°ì…˜ í¬ê¸°ë¥¼ 128MB ì´ìƒìœ¼ë¡œ ì¦ê°€ì‹œì¼œ spillì„ ì¤„ì´ì„¸ìš”',
                'config': 'spark.sql.adaptive.advisoryPartitionSizeInBytes=128MB'
            })
        
        if settings['spark.sql.adaptive.skewJoin.enabled'] == 'false':
            recommendations.append({
                'type': 'skew_join',
                'priority': 'medium',
                'message': 'Skew Join ìµœì í™”ë¥¼ í™œì„±í™”í•˜ì—¬ ë°ì´í„° ìŠ¤í ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”',
                'config': 'spark.sql.adaptive.skewJoin.enabled=true'
            })
        
        return recommendations
    
    def configure_spill_prevention(self):
        """Spill ë°©ì§€ ì„¤ì • ì ìš©"""
        configurations = {
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
            'spark.sql.adaptive.advisoryPartitionSizeInBytes': '128MB',
            'spark.sql.adaptive.skewJoin.enabled': 'true',
            'spark.sql.adaptive.skewJoin.skewedPartitionFactor': '5',
            'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes': '256MB',
            'spark.sql.adaptive.localShuffleReader.enabled': 'true',
            'spark.sql.adaptive.optimizer.excludedRules': '',
            'spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin': '0.2'
        }
        
        applied_configs = {}
        for key, value in configurations.items():
            try:
                self.spark.conf.set(key, value)
                applied_configs[key] = value
            except Exception as e:
                print(f"Failed to set {key}: {e}")
        
        return applied_configs
    
    def optimize_shuffle_operations(self, df, operation_type="join"):
        """Shuffle ì‘ì—… ìµœì í™”"""
        if operation_type == "join":
            return self._optimize_join_shuffle(df)
        elif operation_type == "aggregation":
            return self._optimize_aggregation_shuffle(df)
        elif operation_type == "repartition":
            return self._optimize_repartition_shuffle(df)
        else:
            return df
    
    def _optimize_join_shuffle(self, df):
        """ì¡°ì¸ Shuffle ìµœì í™”"""
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸ íŒíŠ¸ ì¶”ê°€ (ì‘ì€ í…Œì´ë¸”ì˜ ê²½ìš°)
        from pyspark.sql.functions import broadcast
        
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ self-joinì„ ìµœì í™”
        # ì‹¤ì œë¡œëŠ” ë‘ ê°œì˜ ë‹¤ë¥¸ DataFrameì„ ì¡°ì¸í•  ë•Œ ì‚¬ìš©
        return df.hint("broadcast")
    
    def _optimize_aggregation_shuffle(self, df):
        """ì§‘ê³„ Shuffle ìµœì í™”"""
        # íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ Shuffle ë¹„ìš© ê°ì†Œ
        current_partitions = df.rdd.getNumPartitions()
        if current_partitions > 200:
            optimized_partitions = min(200, current_partitions // 2)
            return df.coalesce(optimized_partitions)
        return df
    
    def _optimize_repartition_shuffle(self, df):
        """ë¦¬íŒŒí‹°ì…”ë‹ Shuffle ìµœì í™”"""
        # ì ì‘í˜• ì¿¼ë¦¬ ì‹¤í–‰ì„ í™œìš©í•˜ì—¬ ìë™ ìµœì í™”
        return df

# Shuffle Spill í•´ê²° ì˜ˆì œ
def shuffle_spill_resolution_example():
    spark = SparkSession.builder.appName("ShuffleSpillResolution").getOrCreate()
    resolver = ShuffleSpillResolver(spark)
    
    # Shuffle Spill ê°ì§€
    spill_analysis = resolver.detect_shuffle_spill(spark.createDataFrame([(1, "test")], ["id", "value"]))
    print("=== Shuffle Spill Analysis ===")
    print(json.dumps(spill_analysis, indent=2))
    
    # Spill ë°©ì§€ ì„¤ì • ì ìš©
    applied_configs = resolver.configure_spill_prevention()
    print("\n=== Applied Spill Prevention Configs ===")
    for key, value in applied_configs.items():
        print(f"{key}: {value}")
    
    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ Shuffle í…ŒìŠ¤íŠ¸
    data = [(i, f"data_{i}", i % 100) for i in range(1000000)]
    df = spark.createDataFrame(data, ["id", "value", "category"])
    
    # Shuffleì´ ë°œìƒí•˜ëŠ” ì‘ì—… ìˆ˜í–‰
    result_df = resolver.optimize_shuffle_operations(
        df.groupBy("category").agg({"id": "count", "value": "collect_list"}),
        operation_type="aggregation"
    )
    
    print("\n=== Shuffle Optimization Applied ===")
    print(f"Result partitions: {result_df.rdd.getNumPartitions()}")
```

## ğŸ¤– ëª¨ë‹ˆí„°ë§ ìë™í™”

### Health Check ì‹œìŠ¤í…œ

```python
# Health Check ì‹œìŠ¤í…œ
class SparkHealthChecker:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.health_thresholds = {
            'memory_usage_ratio': 0.85,
            'gc_time_ratio': 0.1,
            'failed_task_ratio': 0.05,
            'executor_loss_ratio': 0.1,
            'slow_task_ratio': 0.2
        }
    
    def perform_health_check(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ ì²´í¬"""
        status_tracker = self.spark.sparkContext.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        health_report = {
            'timestamp': time.time(),
            'overall_status': 'healthy',
            'checks': {
                'memory_usage': self._check_memory_usage(executor_infos),
                'gc_performance': self._check_gc_performance(executor_infos),
                'task_failure': self._check_task_failure(executor_infos),
                'executor_health': self._check_executor_health(executor_infos),
                'cluster_utilization': self._check_cluster_utilization(executor_infos)
            },
            'recommendations': [],
            'critical_alerts': []
        }
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        failed_checks = [check for check in health_report['checks'].values() if not check['status']]
        if failed_checks:
            health_report['overall_status'] = 'unhealthy'
            health_report['critical_alerts'] = [check['message'] for check in failed_checks]
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        health_report['recommendations'] = self._generate_health_recommendations(health_report['checks'])
        
        return health_report
    
    def _check_memory_usage(self, executor_infos):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        total_memory = sum(info.maxMemory for info in executor_infos)
        used_memory = sum(info.memoryUsed for info in executor_infos)
        
        if total_memory == 0:
            return {'status': False, 'message': 'No memory information available', 'value': 0}
        
        usage_ratio = used_memory / total_memory
        threshold = self.health_thresholds['memory_usage_ratio']
        
        return {
            'status': usage_ratio < threshold,
            'message': f'Memory usage: {usage_ratio:.2%} (threshold: {threshold:.2%})',
            'value': usage_ratio,
            'threshold': threshold
        }
    
    def _check_gc_performance(self, executor_infos):
        """GC ì„±ëŠ¥ ì²´í¬"""
        total_duration = sum(info.totalDuration for info in executor_infos)
        total_gc_time = sum(info.totalGCTime for info in executor_infos)
        
        if total_duration == 0:
            return {'status': True, 'message': 'No duration information available', 'value': 0}
        
        gc_ratio = total_gc_time / total_duration
        threshold = self.health_thresholds['gc_time_ratio']
        
        return {
            'status': gc_ratio < threshold,
            'message': f'GC time ratio: {gc_ratio:.2%} (threshold: {threshold:.2%})',
            'value': gc_ratio,
            'threshold': threshold
        }
    
    def _check_task_failure(self, executor_infos):
        """íƒœìŠ¤í¬ ì‹¤íŒ¨ìœ¨ ì²´í¬"""
        total_tasks = sum(info.completedTasks + info.failedTasks for info in executor_infos)
        failed_tasks = sum(info.failedTasks for info in executor_infos)
        
        if total_tasks == 0:
            return {'status': True, 'message': 'No tasks executed yet', 'value': 0}
        
        failure_ratio = failed_tasks / total_tasks
        threshold = self.health_thresholds['failed_task_ratio']
        
        return {
            'status': failure_ratio < threshold,
            'message': f'Task failure ratio: {failure_ratio:.2%} (threshold: {threshold:.2%})',
            'value': failure_ratio,
            'threshold': threshold
        }
    
    def _check_executor_health(self, executor_infos):
        """ì‹¤í–‰ì ê±´ê°•ë„ ì²´í¬"""
        total_executors = len(executor_infos)
        lost_executors = len([info for info in executor_infos if info.isActive == False])
        
        if total_executors == 0:
            return {'status': False, 'message': 'No executors available', 'value': 0}
        
        loss_ratio = lost_executors / total_executors
        threshold = self.health_thresholds['executor_loss_ratio']
        
        return {
            'status': loss_ratio < threshold,
            'message': f'Executor loss ratio: {loss_ratio:.2%} (threshold: {threshold:.2%})',
            'value': loss_ratio,
            'threshold': threshold
        }
    
    def _check_cluster_utilization(self, executor_infos):
        """í´ëŸ¬ìŠ¤í„° í™œìš©ë¥  ì²´í¬"""
        total_cores = sum(info.totalCores for info in executor_infos)
        active_tasks = sum(info.activeTasks for info in executor_infos)
        
        if total_cores == 0:
            return {'status': False, 'message': 'No cores available', 'value': 0}
        
        utilization = active_tasks / total_cores
        
        return {
            'status': True,  # í™œìš©ë¥ ì€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            'message': f'Cluster utilization: {utilization:.2%}',
            'value': utilization
        }
    
    def _generate_health_recommendations(self, checks):
        """ê±´ê°•ë„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not checks['memory_usage']['status']:
            recommendations.append({
                'category': 'memory',
                'priority': 'high',
                'action': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. executor-memoryë¥¼ ì¦ê°€ì‹œí‚¤ê±°ë‚˜ ë°ì´í„° ìºì‹±ì„ ìµœì í™”í•˜ì„¸ìš”'
            })
        
        if not checks['gc_performance']['status']:
            recommendations.append({
                'category': 'gc',
                'priority': 'medium',
                'action': 'GC ì‹œê°„ì´ ê¸¸ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. JVM GC ì„¤ì •ì„ ìµœì í™”í•˜ì„¸ìš”'
            })
        
        if not checks['task_failure']['status']:
            recommendations.append({
                'category': 'reliability',
                'priority': 'high',
                'action': 'íƒœìŠ¤í¬ ì‹¤íŒ¨ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ í• ë‹¹ê³¼ ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•˜ì„¸ìš”'
            })
        
        if not checks['executor_health']['status']:
            recommendations.append({
                'category': 'infrastructure',
                'priority': 'critical',
                'action': 'ì‹¤í–‰ì ì†ì‹¤ì´ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”'
            })
        
        return recommendations

# Health Check ì˜ˆì œ
def health_check_example():
    spark = SparkSession.builder.appName("HealthCheck").getOrCreate()
    health_checker = SparkHealthChecker(spark)
    
    # ê±´ê°•ë„ ì²´í¬ ìˆ˜í–‰
    health_report = health_checker.perform_health_check()
    
    print("=== Spark Health Check Report ===")
    print(f"Overall Status: {health_report['overall_status'].upper()}")
    print(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(health_report['timestamp']))}")
    
    print("\n=== Individual Checks ===")
    for check_name, check_result in health_report['checks'].items():
        status_icon = "âœ…" if check_result['status'] else "âŒ"
        print(f"{status_icon} {check_name}: {check_result['message']}")
    
    if health_report['critical_alerts']:
        print("\n=== Critical Alerts ===")
        for alert in health_report['critical_alerts']:
            print(f"ğŸš¨ {alert}")
    
    if health_report['recommendations']:
        print("\n=== Recommendations ===")
        for rec in health_report['recommendations']:
            priority_icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡" if rec['priority'] == 'medium' else "ğŸŸ¢"
            print(f"{priority_icon} [{rec['priority'].upper()}] {rec['action']}")
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### Apache Spark ì‹œë¦¬ì¦ˆ ì™„ì„±! ğŸ‰

ì´ì œ **Apache Spark ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ**ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤:

1. **Part 1**: Spark ê¸°ì´ˆì™€ í•µì‹¬ ê°œë… (RDD, DataFrame, Spark SQL)
2. **Part 2**: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì™€ UDF í™œìš© (ì‹¤ë¬´ í”„ë¡œì íŠ¸)  
3. **Part 3**: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ Kafka ì—°ë™ (ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ)
4. **Part 4**: ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ íŠœë‹ (í”„ë¡œë•ì…˜ í™˜ê²½)

### ì‹¤ë¬´ ì ìš© ì—­ëŸ‰

âœ… **ê¸°ìˆ ì  ì—­ëŸ‰**
- Spark ì•„í‚¤í…ì²˜ ì™„ì „ ì´í•´
- ë°°ì¹˜/ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë§ˆìŠ¤í„°
- ì„±ëŠ¥ ìµœì í™” ì „ë¬¸ê°€
- í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•

âœ… **ì‹¤ë¬´ ì ìš©**
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- ì‹¤ì‹œê°„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ê°œë°œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ìš´ì˜
- í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ë° íŠœë‹

---

**ì‹œë¦¬ì¦ˆ ì™„ë£Œ**: [Apache Spark ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ ì „ì²´ ë³´ê¸°](/data-engineering/2025/09/10/apache-spark-series-overview.html)

---

*ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ Apache Sparkì˜ ëª¨ë“  ê²ƒì„ ë§ˆìŠ¤í„°í•˜ê³  ë¹…ë°ì´í„° ì²˜ë¦¬ ì „ë¬¸ê°€ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!* ğŸš€âœ¨

---
layout: post
lang: ko
title: "Apache Kafka Python ê°€ì´ë“œ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë°ì´í„° ì²˜ë¦¬"
description: "Pythonì„ í™œìš©í•œ Apache Kafka ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê°œë°œê³¼ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤."
date: 2025-09-09
author: Data Droid
category: data-engineering
tags: [Apache-Kafka, Python, ì‹¤ì‹œê°„ìŠ¤íŠ¸ë¦¬ë°, ë°ì´í„°ì²˜ë¦¬, kafka-python, confluent-kafka, faust]
reading_time: "25ë¶„"
difficulty: "ê³ ê¸‰"
---

# Apache Kafka Python ê°€ì´ë“œ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë°ì´í„° ì²˜ë¦¬

> Pythonì„ í™œìš©í•œ Apache Kafka ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê°œë°œê³¼ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [Python Kafka ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ](#python-kafka-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„ íƒ)
2. [kafka-python ê¸°ë³¸ ì‚¬ìš©ë²•](#kafka-python-ê¸°ë³¸-ì‚¬ìš©ë²•)
3. [confluent-kafka ê³ ì„±ëŠ¥ ì²˜ë¦¬](#confluent-kafka-ê³ ì„±ëŠ¥-ì²˜ë¦¬)
4. [Faust ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬](#faust-ìŠ¤íŠ¸ë¦¬ë°-ì²˜ë¦¬)
5. [ì‹¤ìŠµ: Python ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ](#ì‹¤ìŠµ-python-ê¸°ë°˜-ìŠ¤íŠ¸ë¦¬ë°-ì‹œìŠ¤í…œ)
6. [ì„±ëŠ¥ ìµœì í™”ì™€ ëª¨ë‹ˆí„°ë§](#ì„±ëŠ¥-ìµœì í™”ì™€-ëª¨ë‹ˆí„°ë§)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ Python Kafka ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ

### ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹„êµ

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | íŠ¹ì§• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‚¬ë¡€ |
|-----------|------|------|------|-----------|
| **kafka-python** | ìˆœìˆ˜ Python êµ¬í˜„ | ì„¤ì¹˜ ê°„ë‹¨, ì´í•´í•˜ê¸° ì‰¬ì›€ | ì„±ëŠ¥ ì œí•œ | í•™ìŠµ, í”„ë¡œí† íƒ€ì… |
| **confluent-kafka** | C ë¼ì´ë¸ŒëŸ¬ë¦¬ ë˜í•‘ | ê³ ì„±ëŠ¥, ì•ˆì •ì„± | ì„¤ì¹˜ ë³µì¡ | í”„ë¡œë•ì…˜ í™˜ê²½ |
| **faust** | ìŠ¤íŠ¸ë¦¬ë° ì „ìš© | ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° | ì œí•œì  ê¸°ëŠ¥ | ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ |
| **aiokafka** | ë¹„ë™ê¸° ì²˜ë¦¬ | ë¹„ë™ê¸° ì§€ì› | ë³µì¡ì„± | ê³ ì„±ëŠ¥ ë¹„ë™ê¸° |

### ì„¤ì¹˜ ë°©ë²•

```bash
# kafka-python ì„¤ì¹˜
pip install kafka-python

# confluent-kafka ì„¤ì¹˜ (ê¶Œì¥)
pip install confluent-kafka

# faust ì„¤ì¹˜
pip install faust[rocksdb]

# aiokafka ì„¤ì¹˜
pip install aiokafka

# ì¶”ê°€ ì˜ì¡´ì„±
pip install pandas numpy asyncio
```

## âš¡ kafka-python ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. í”„ë¡œë“€ì„œ êµ¬í˜„

```python
from kafka import KafkaProducer
from kafka.errors import KafkaError
import json
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PythonKafkaProducer:
    def __init__(self, bootstrap_servers='localhost:9092'):
        self.producer = KafkaProducer(
            bootstrap_servers=[bootstrap_servers],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            batch_size=16384,  # 16KB
            linger_ms=5,       # 5ms ëŒ€ê¸°
            compression_type='snappy',  # ì••ì¶•
            acks='all',        # ëª¨ë“  ë³µì œë³¸ í™•ì¸
            retries=3,         # ì¬ì‹œë„ íšŸìˆ˜
            retry_backoff_ms=100,  # ì¬ì‹œë„ ê°„ê²©
            # ì•ˆì „ì„± ì„¤ì •
            enable_idempotence=True,  # ë©±ë“±ì„± ë³´ì¥
            max_in_flight_requests_per_connection=5,
            request_timeout_ms=30000,
        )
    
    def send_message(self, topic, key, value):
        """ë©”ì‹œì§€ ì „ì†¡"""
        try:
            future = self.producer.send(topic, key=key, value=value)
            record_metadata = future.get(timeout=10)
            logger.info(f"Message sent to {record_metadata.topic} "
                       f"partition {record_metadata.partition} "
                       f"offset {record_metadata.offset}")
            return True
        except KafkaError as e:
            logger.error(f"Failed to send message: {e}")
            return False
    
    def send_batch(self, topic, messages):
        """ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡"""
        futures = []
        for key, value in messages:
            future = self.producer.send(topic, key=key, value=value)
            futures.append(future)
        
        # ëª¨ë“  ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
        for future in futures:
            try:
                future.get(timeout=10)
            except KafkaError as e:
                logger.error(f"Batch send failed: {e}")
                return False
        return True
    
    def close(self):
        """í”„ë¡œë“€ì„œ ì¢…ë£Œ"""
        self.producer.flush()  # ë‚¨ì€ ë©”ì‹œì§€ ì „ì†¡
        self.producer.close()

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    producer = PythonKafkaProducer()
    
    # ë‹¨ì¼ ë©”ì‹œì§€ ì „ì†¡
    producer.send_message(
        topic="user-events",
        key="user123",
        value={"action": "login", "timestamp": time.time()}
    )
    
    # ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡
    messages = [
        ("user123", {"action": "view", "page": "/home"}),
        ("user456", {"action": "purchase", "amount": 99.99}),
        ("user789", {"action": "logout", "timestamp": time.time()})
    ]
    producer.send_batch("user-events", messages)
    
    producer.close()
```

### 2. ì»¨ìŠˆë¨¸ êµ¬í˜„

```python
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import json
import logging
from typing import Dict, Any, Callable

class PythonKafkaConsumer:
    def __init__(self, bootstrap_servers='localhost:9092', group_id='python-consumer'):
        self.consumer = KafkaConsumer(
            bootstrap_servers=[bootstrap_servers],
            group_id=group_id,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            key_deserializer=lambda m: m.decode('utf-8') if m else None,
            # ì˜¤í”„ì…‹ ê´€ë¦¬
            auto_offset_reset='earliest',  # earliest, latest, none
            enable_auto_commit=False,      # ìˆ˜ë™ ì˜¤í”„ì…‹ ì»¤ë°‹
            # ì„±ëŠ¥ ì„¤ì •
            max_poll_records=500,          # í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜
            session_timeout_ms=30000,      # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
            heartbeat_interval_ms=3000,    # í•˜íŠ¸ë¹„íŠ¸ ê°„ê²©
            # ì¬ì‹œë„ ì„¤ì •
            retry_backoff_ms=100,
            request_timeout_ms=30000,
        )
        self.message_handler = None
    
    def subscribe(self, topics):
        """í† í”½ êµ¬ë…"""
        self.consumer.subscribe(topics)
        logger.info(f"Subscribed to topics: {topics}")
    
    def set_message_handler(self, handler: Callable[[str, Any], None]):
        """ë©”ì‹œì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        self.message_handler = handler
    
    def start_consuming(self):
        """ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘"""
        try:
            while True:
                message_batch = self.consumer.poll(timeout_ms=1000)
                
                for topic_partition, messages in message_batch.items():
                    for message in messages:
                        self._process_message(message)
                
                # ì˜¤í”„ì…‹ ì»¤ë°‹
                self.consumer.commit()
                
        except KeyboardInterrupt:
            logger.info("Consumer stopped by user")
        except Exception as e:
            logger.error(f"Consumer error: {e}")
        finally:
            self.consumer.close()
    
    def _process_message(self, message):
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            logger.info(f"Received message: {message.key} -> {message.value}")
            
            if self.message_handler:
                self.message_handler(message.key, message.value)
            else:
                # ê¸°ë³¸ ì²˜ë¦¬
                self._default_handler(message.key, message.value)
                
        except Exception as e:
            logger.error(f"Error processing message: {e}")
    
    def _default_handler(self, key, value):
        """ê¸°ë³¸ ë©”ì‹œì§€ í•¸ë“¤ëŸ¬"""
        print(f"Key: {key}, Value: {value}")

# ì‚¬ìš© ì˜ˆì œ
def custom_message_handler(key, value):
    """ì»¤ìŠ¤í…€ ë©”ì‹œì§€ í•¸ë“¤ëŸ¬"""
    print(f"Processing: {key} -> {value}")
    # ì—¬ê¸°ì— ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„

if __name__ == "__main__":
    consumer = PythonKafkaConsumer()
    consumer.subscribe(['user-events', 'order-events'])
    consumer.set_message_handler(custom_message_handler)
    consumer.start_consuming()
```

## ğŸš€ confluent-kafka ê³ ì„±ëŠ¥ ì²˜ë¦¬

### 1. ê³ ì„±ëŠ¥ í”„ë¡œë“€ì„œ

```python
from confluent_kafka import Producer, Consumer, KafkaError
import json
import time
import threading
from collections import defaultdict

class ConfluentKafkaProducer:
    def __init__(self, config):
        self.producer = Producer(config)
        self.delivery_callback_count = 0
        self.delivery_callback_errors = 0
    
    def delivery_callback(self, err, msg):
        """ì „ì†¡ ê²°ê³¼ ì½œë°±"""
        if err is not None:
            self.delivery_callback_errors += 1
            print(f'Message delivery failed: {err}')
        else:
            self.delivery_callback_count += 1
            print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')
    
    def produce_message(self, topic, key, value):
        """ë©”ì‹œì§€ ì „ì†¡"""
        try:
            self.producer.produce(
                topic,
                key=key,
                value=json.dumps(value).encode('utf-8'),
                callback=self.delivery_callback
            )
            # ë¹„ë™ê¸° ì „ì†¡ì´ë¯€ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ flush í•„ìš”
            self.producer.poll(0)
        except BufferError as e:
            print(f'Producer queue is full: {e}')
            self.producer.flush()
            raise
    
    def flush(self):
        """ë‚¨ì€ ë©”ì‹œì§€ ì „ì†¡"""
        self.producer.flush()

# ì„¤ì • ì˜ˆì œ
producer_config = {
    'bootstrap.servers': 'localhost:9092',
    'compression.type': 'snappy',
    'batch.size': 16384,
    'linger.ms': 5,
    'acks': 'all',
    'retries': 3,
    'enable.idempotence': True,
    'max.in.flight.requests.per.connection': 5,
}

producer = ConfluentKafkaProducer(producer_config)
```

### 2. ê³ ì„±ëŠ¥ ì»¨ìŠˆë¨¸

```python
class ConfluentKafkaConsumer:
    def __init__(self, config):
        self.consumer = Consumer(config)
        self.running = True
    
    def subscribe(self, topics):
        """í† í”½ êµ¬ë…"""
        self.consumer.subscribe(topics)
    
    def consume_messages(self, timeout=1.0):
        """ë©”ì‹œì§€ ì†Œë¹„"""
        try:
            while self.running:
                msg = self.consumer.poll(timeout=timeout)
                
                if msg is None:
                    continue
                
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        # íŒŒí‹°ì…˜ ëì— ë„ë‹¬
                        continue
                    else:
                        print(f"Consumer error: {msg.error()}")
                        continue
                
                # ë©”ì‹œì§€ ì²˜ë¦¬
                self.process_message(msg)
                
        except KeyboardInterrupt:
            print("Consumer interrupted by user")
        finally:
            self.close()
    
    def process_message(self, msg):
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            key = msg.key().decode('utf-8') if msg.key() else None
            value = json.loads(msg.value().decode('utf-8'))
            
            print(f"Received message: {key} -> {value}")
            
            # ì—¬ê¸°ì— ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
            
        except Exception as e:
            print(f"Error processing message: {e}")
    
    def close(self):
        """ì»¨ìŠˆë¨¸ ì¢…ë£Œ"""
        self.running = False
        self.consumer.close()

# ì„¤ì • ì˜ˆì œ
consumer_config = {
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'python-consumer-group',
    'auto.offset.reset': 'earliest',
    'enable.auto.commit': False,
    'max.poll.records': 500,
    'session.timeout.ms': 30000,
    'heartbeat.interval.ms': 3000,
}

consumer = ConfluentKafkaConsumer(consumer_config)
consumer.subscribe(['user-events'])
consumer.consume_messages()
```

## ğŸŒŠ Faust ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

### 1. Faust ê¸°ë³¸ ì„¤ì •

```python
import faust
from faust import Record
import asyncio
from typing import Optional

# Faust ì•± ì„¤ì •
app = faust.App(
    'kafka-streaming-app',
    broker='kafka://localhost:9092',
    store='rocksdb://',  # ìƒíƒœ ì €ì¥ì†Œ
    value_serializer='json',
)

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class UserEvent(Record):
    user_id: str
    action: str
    timestamp: float
    metadata: Optional[dict] = None

class OrderEvent(Record):
    order_id: str
    user_id: str
    amount: float
    items: list
    timestamp: float

# í† í”½ ì •ì˜
user_events_topic = app.topic('user-events', value_type=UserEvent)
order_events_topic = app.topic('order-events', value_type=OrderEvent)
processed_events_topic = app.topic('processed-events', value_type=UserEvent)

# í…Œì´ë¸” ì •ì˜ (ìƒíƒœ ì €ì¥ì†Œ)
user_activity_table = app.Table('user-activity', default=int)
order_summary_table = app.Table('order-summary', default=dict)
```

### 2. ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì˜ˆì œ

```python
@app.agent(user_events_topic)
async def process_user_events(events):
    """ì‚¬ìš©ì ì´ë²¤íŠ¸ ì²˜ë¦¬"""
    async for event in events:
        # ì´ë²¤íŠ¸ ì²˜ë¦¬
        print(f"Processing user event: {event.user_id} - {event.action}")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        user_activity_table[event.user_id] += 1
        
        # ì´ë²¤íŠ¸ ë³€í™˜
        processed_event = UserEvent(
            user_id=event.user_id,
            action=f"processed_{event.action}",
            timestamp=event.timestamp,
            metadata={"processed_by": "faust"}
        )
        
        # ê²°ê³¼ ì „ì†¡
        await processed_events_topic.send(value=processed_event)

@app.agent(order_events_topic)
async def process_order_events(orders):
    """ì£¼ë¬¸ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
    async for order in orders:
        print(f"Processing order: {order.order_id} - ${order.amount}")
        
        # ì£¼ë¬¸ ìš”ì•½ ì—…ë°ì´íŠ¸
        if order.user_id not in order_summary_table:
            order_summary_table[order.user_id] = {
                'total_orders': 0,
                'total_amount': 0.0,
                'last_order_time': 0.0
            }
        
        summary = order_summary_table[order.user_id]
        summary['total_orders'] += 1
        summary['total_amount'] += order.amount
        summary['last_order_time'] = order.timestamp
        
        # ì£¼ë¬¸ ê¸ˆì•¡ë³„ ë¶„ë¥˜
        if order.amount > 100:
            await app.topic('high-value-orders').send(value=order)
        else:
            await app.topic('regular-orders').send(value=order)

# ìœˆë„ìš° ì§‘ê³„
@app.timer(interval=60.0)  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
async def aggregate_metrics():
    """ë©”íŠ¸ë¦­ ì§‘ê³„"""
    print("=== User Activity Summary ===")
    for user_id, count in user_activity_table.items():
        print(f"User {user_id}: {count} events")
    
    print("=== Order Summary ===")
    for user_id, summary in order_summary_table.items():
        print(f"User {user_id}: {summary['total_orders']} orders, ${summary['total_amount']:.2f}")

if __name__ == '__main__':
    app.main()
```

### 3. ê³ ê¸‰ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

```python
from faust import Stream, Table
from faust.types import StreamT

# ìŠ¤íŠ¸ë¦¼ ì¡°ì¸
@app.agent(user_events_topic)
async def join_user_orders(events):
    """ì‚¬ìš©ì ì´ë²¤íŠ¸ì™€ ì£¼ë¬¸ ì¡°ì¸"""
    async for event in events:
        # ì‚¬ìš©ì ì£¼ë¬¸ ì •ë³´ ì¡°íšŒ
        order_summary = order_summary_table.get(event.user_id, {})
        
        # ì¡°ì¸ëœ ë°ì´í„° ì²˜ë¦¬
        enriched_event = {
            'user_id': event.user_id,
            'action': event.action,
            'timestamp': event.timestamp,
            'user_orders': order_summary.get('total_orders', 0),
            'user_spent': order_summary.get('total_amount', 0.0)
        }
        
        print(f"Enriched event: {enriched_event}")

# ìœˆë„ìš° ì§‘ê³„
@app.agent(user_events_topic)
async def window_aggregation(events):
    """ì‹œê°„ ìœˆë„ìš° ì§‘ê³„"""
    async for event in events:
        # 5ë¶„ ìœˆë„ìš°ë¡œ ì§‘ê³„
        window = event.timestamp // 300  # 5ë¶„ = 300ì´ˆ
        
        # ìœˆë„ìš°ë³„ ì¹´ìš´íŠ¸
        window_key = f"window_{window}"
        user_activity_table[window_key] += 1
        
        # ìœˆë„ìš°ê°€ ì™„ë£Œë˜ë©´ ê²°ê³¼ ì „ì†¡
        if window_key not in processed_windows:
            processed_windows.add(window_key)
            await app.topic('window-results').send(
                value={'window': window, 'count': user_activity_table[window_key]}
            )

processed_windows = set()
```

## ğŸ› ï¸ ì‹¤ìŠµ: Python ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ

### 1. ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, Any
import pandas as pd

class LogProcessingSystem:
    def __init__(self):
        self.producer = ConfluentKafkaProducer(producer_config)
        self.consumer = ConfluentKafkaConsumer(consumer_config)
        self.log_stats = defaultdict(int)
    
    async def start_log_collector(self):
        """ë¡œê·¸ ìˆ˜ì§‘ê¸° ì‹œì‘"""
        while True:
            # ë¡œê·¸ íŒŒì¼ì—ì„œ ì½ê¸° (ì‹¤ì œë¡œëŠ” íŒŒì¼ ëª¨ë‹ˆí„°ë§)
            log_line = self.simulate_log_line()
            if log_line:
                self.producer.produce_message('raw-logs', None, log_line)
            
            await asyncio.sleep(0.1)  # 100ms ê°„ê²©
    
    def simulate_log_line(self):
        """ë¡œê·¸ ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜"""
        import random
        levels = ['INFO', 'WARN', 'ERROR', 'DEBUG']
        actions = ['login', 'logout', 'purchase', 'view', 'search']
        
        return {
            'timestamp': datetime.now().isoformat(),
            'level': random.choice(levels),
            'message': f"User {random.randint(1, 1000)} performed {random.choice(actions)}",
            'ip': f"192.168.1.{random.randint(1, 255)}",
            'user_agent': 'Mozilla/5.0...'
        }
    
    async def start_log_processor(self):
        """ë¡œê·¸ ì²˜ë¦¬ê¸° ì‹œì‘"""
        self.consumer.subscribe(['raw-logs'])
        
        while True:
            try:
                msg = self.consumer.consume_messages(timeout=1.0)
                if msg:
                    await self.process_log_message(msg)
            except Exception as e:
                logging.error(f"Error processing log: {e}")
    
    async def process_log_message(self, log_data):
        """ë¡œê·¸ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            # ë¡œê·¸ ë ˆë²¨ë³„ í†µê³„
            level = log_data.get('level', 'UNKNOWN')
            self.log_stats[level] += 1
            
            # ì—ëŸ¬ ë¡œê·¸ íŠ¹ë³„ ì²˜ë¦¬
            if level == 'ERROR':
                await self.handle_error_log(log_data)
            
            # ì‚¬ìš©ì í–‰ë™ ë¶„ì„
            if 'performed' in log_data.get('message', ''):
                await self.analyze_user_behavior(log_data)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            await self.update_metrics(log_data)
            
        except Exception as e:
            logging.error(f"Error processing log message: {e}")
    
    async def handle_error_log(self, log_data):
        """ì—ëŸ¬ ë¡œê·¸ ì²˜ë¦¬"""
        print(f"ERROR detected: {log_data['message']}")
        # ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡
        await self.send_alert('ERROR', log_data)
    
    async def analyze_user_behavior(self, log_data):
        """ì‚¬ìš©ì í–‰ë™ ë¶„ì„"""
        message = log_data.get('message', '')
        if 'login' in message:
            print("User login detected")
        elif 'purchase' in message:
            print("Purchase activity detected")
    
    async def send_alert(self, level, log_data):
        """ì•Œë¦¼ ì „ì†¡"""
        alert = {
            'type': 'log_alert',
            'level': level,
            'message': log_data['message'],
            'timestamp': log_data['timestamp']
        }
        self.producer.produce_message('alerts', level, alert)
    
    async def update_metrics(self, log_data):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        metrics = {
            'timestamp': time.time(),
            'level_counts': dict(self.log_stats),
            'total_logs': sum(self.log_stats.values())
        }
        self.producer.produce_message('log-metrics', 'metrics', metrics)
    
    async def start_system(self):
        """ì‹œìŠ¤í…œ ì‹œì‘"""
        tasks = [
            asyncio.create_task(self.start_log_collector()),
            asyncio.create_task(self.start_log_processor())
        ]
        
        try:
            await asyncio.gather(*tasks)
        except KeyboardInterrupt:
            print("System stopped by user")
        finally:
            self.producer.flush()
            self.consumer.close()

# ì‹¤í–‰
if __name__ == "__main__":
    system = LogProcessingSystem()
    asyncio.run(system.start_system())
```

### 2. ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

```python
class EventDrivenMicroservice:
    def __init__(self, service_name):
        self.service_name = service_name
        self.consumer = ConfluentKafkaConsumer(consumer_config)
        self.producer = ConfluentKafkaProducer(producer_config)
        self.running = True
    
    async def start_service(self):
        """ì„œë¹„ìŠ¤ ì‹œì‘"""
        topics = ['user-events', 'order-events', 'payment-events']
        self.consumer.subscribe(topics)
        
        print(f"{self.service_name} started, listening to {topics}")
        
        while self.running:
            try:
                msg = self.consumer.consume_messages(timeout=1.0)
                if msg:
                    await self.handle_event(msg)
            except Exception as e:
                logging.error(f"Error in {self.service_name}: {e}")
    
    async def handle_event(self, event_data):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = event_data.get('type')
        
        if event_type == 'user_event':
            await self.handle_user_event(event_data)
        elif event_type == 'order_event':
            await self.handle_order_event(event_data)
        elif event_type == 'payment_event':
            await self.handle_payment_event(event_data)
    
    async def handle_user_event(self, event):
        """ì‚¬ìš©ì ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        print(f"Processing user event: {event}")
        # ì‚¬ìš©ì ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        await self.send_event('user-processed', event)
    
    async def handle_order_event(self, event):
        """ì£¼ë¬¸ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        print(f"Processing order event: {event}")
        # ì£¼ë¬¸ ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        await self.send_event('order-processed', event)
    
    async def handle_payment_event(self, event):
        """ê²°ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        print(f"Processing payment event: {event}")
        # ê²°ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§
        await self.send_event('payment-processed', event)
    
    async def send_event(self, topic, event_data):
        """ì´ë²¤íŠ¸ ì „ì†¡"""
        self.producer.produce_message(topic, event_data.get('id'), event_data)
    
    def stop(self):
        """ì„œë¹„ìŠ¤ ì¤‘ì§€"""
        self.running = False
        self.consumer.close()
        self.producer.flush()

# ì„œë¹„ìŠ¤ ì‹¤í–‰
if __name__ == "__main__":
    service = EventDrivenMicroservice("event-processor")
    asyncio.run(service.start_service())
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”ì™€ ëª¨ë‹ˆí„°ë§

### 1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import psutil
import time
from collections import defaultdict
import threading

class KafkaPerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)
        self.running = True
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.running:
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            disk_io = psutil.disk_io_counters()
            network_io = psutil.net_io_counters()
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            timestamp = time.time()
            self.metrics['cpu'].append((timestamp, cpu_percent))
            self.metrics['memory'].append((timestamp, memory_percent))
            self.metrics['disk_read'].append((timestamp, disk_io.read_bytes))
            self.metrics['disk_write'].append((timestamp, disk_io.write_bytes))
            self.metrics['network_sent'].append((timestamp, network_io.bytes_sent))
            self.metrics['network_recv'].append((timestamp, network_io.bytes_recv))
            
            # ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬ (1ì‹œê°„ ì´ìƒ)
            cutoff_time = timestamp - 3600
            for metric_name in self.metrics:
                self.metrics[metric_name] = [
                    (t, v) for t, v in self.metrics[metric_name] if t > cutoff_time
                ]
            
            time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
    
    def get_metrics_summary(self):
        """ë©”íŠ¸ë¦­ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        for metric_name, values in self.metrics.items():
            if values:
                recent_values = [v for t, v in values[-10:]]  # ìµœê·¼ 10ê°œ
                summary[metric_name] = {
                    'current': recent_values[-1] if recent_values else 0,
                    'average': sum(recent_values) / len(recent_values) if recent_values else 0,
                    'max': max(recent_values) if recent_values else 0,
                    'min': min(recent_values) if recent_values else 0
                }
        return summary
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join()

# ì‚¬ìš© ì˜ˆì œ
monitor = KafkaPerformanceMonitor()
monitor.start_monitoring()

# 1ë¶„ í›„ ë©”íŠ¸ë¦­ í™•ì¸
time.sleep(60)
summary = monitor.get_metrics_summary()
print("Performance Summary:", summary)

monitor.stop_monitoring()
```

### 2. ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„

```python
import asyncio
from functools import wraps
import random

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    
                    # ì§€ìˆ˜ ë°±ì˜¤í”„
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    jitter = random.uniform(0, delay * 0.1)
                    await asyncio.sleep(delay + jitter)
                    
                    print(f"Retry {attempt + 1}/{max_retries} after {delay:.2f}s: {e}")
            
            return None
        return wrapper
    return decorator

class ResilientKafkaProducer:
    def __init__(self, config):
        self.producer = ConfluentKafkaProducer(config)
        self.dead_letter_queue = []
    
    @retry_with_backoff(max_retries=3)
    async def produce_with_retry(self, topic, key, value):
        """ì¬ì‹œë„ê°€ ìˆëŠ” ë©”ì‹œì§€ ì „ì†¡"""
        try:
            self.producer.produce_message(topic, key, value)
            return True
        except Exception as e:
            print(f"Failed to produce message: {e}")
            raise e
    
    async def produce_message(self, topic, key, value):
        """ë©”ì‹œì§€ ì „ì†¡ (ì¬ì‹œë„ í¬í•¨)"""
        try:
            await self.produce_with_retry(topic, key, value)
        except Exception as e:
            # ìµœì¢… ì‹¤íŒ¨ ì‹œ ë°ë“œ ë ˆí„° íì— ì¶”ê°€
            self.dead_letter_queue.append({
                'topic': topic,
                'key': key,
                'value': value,
                'error': str(e),
                'timestamp': time.time()
            })
            print(f"Message sent to dead letter queue: {e}")
    
    def get_dead_letter_queue(self):
        """ë°ë“œ ë ˆí„° í ë°˜í™˜"""
        return self.dead_letter_queue
    
    def retry_dead_letter_queue(self):
        """ë°ë“œ ë ˆí„° í ì¬ì‹œë„"""
        retry_count = 0
        for item in self.dead_letter_queue[:]:
            try:
                self.producer.produce_message(
                    item['topic'], 
                    item['key'], 
                    item['value']
                )
                self.dead_letter_queue.remove(item)
                retry_count += 1
            except Exception as e:
                print(f"Failed to retry dead letter: {e}")
        
        print(f"Retried {retry_count} messages from dead letter queue")
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Python Kafka ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ**
   - kafka-python, confluent-kafka, faust, aiokafka ë¹„êµ
   - ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŠ¹ì§•ê³¼ ì‚¬ìš© ì‚¬ë¡€

2. **kafka-python ê¸°ë³¸ ì‚¬ìš©ë²•**
   - í”„ë¡œë“€ì„œì™€ ì»¨ìŠˆë¨¸ êµ¬í˜„
   - ë°°ì¹˜ ì²˜ë¦¬ì™€ ì˜¤í”„ì…‹ ê´€ë¦¬

3. **confluent-kafka ê³ ì„±ëŠ¥ ì²˜ë¦¬**
   - ê³ ì„±ëŠ¥ í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸ êµ¬í˜„
   - ë¹„ë™ê¸° ì²˜ë¦¬ì™€ ì½œë°± í™œìš©

4. **Faust ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**
   - ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ì™€ ìƒíƒœ ì €ì¥ì†Œ
   - ìœˆë„ìš° ì§‘ê³„ì™€ ì¡°ì¸ ì—°ì‚°

5. **ì‹¤ë¬´ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ êµ¬ì¶•**
   - ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ
   - ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

6. **ì„±ëŠ¥ ìµœì í™”ì™€ ëª¨ë‹ˆí„°ë§**
   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
   - ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„ ì „ëµ

### í•µì‹¬ ê°œë… ì •ë¦¬

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ** | í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ | â­â­â­â­â­ |
| **ë¹„ë™ê¸° ì²˜ë¦¬** | ê³ ì„±ëŠ¥ì„ ìœ„í•œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° | â­â­â­â­â­ |
| **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬** | ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ê³¼ ì§‘ê³„ | â­â­â­â­â­ |
| **ëª¨ë‹ˆí„°ë§** | ì‹œìŠ¤í…œ ìƒíƒœ ì¶”ì ê³¼ ì„±ëŠ¥ ìµœì í™” | â­â­â­â­ |

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

1. **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ**: í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ê³¼ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê³ ë ¤
2. **ë¹„ë™ê¸° ì²˜ë¦¬**: asyncioë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ì²˜ë¦¬
3. **ì—ëŸ¬ ì²˜ë¦¬**: ì¬ì‹œë„ ì „ëµê³¼ ë°ë“œ ë ˆí„° í í™œìš©
4. **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ

---

*ì´ ê°€ì´ë“œë¥¼ í†µí•´ Pythonìœ¼ë¡œ Apache Kafkaë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!* ğŸš€

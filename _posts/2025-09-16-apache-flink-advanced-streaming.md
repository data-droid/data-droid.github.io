---
layout: post
lang: ko
title: "Part 2: Apache Flink ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬ - í”„ë¡œë•ì…˜ê¸‰ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ"
description: "Apache Flinkì˜ ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬, ì²´í¬í¬ì¸íŒ…, ì„¸ì´ë¸Œí¬ì¸íŠ¸, ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬ ì „ëµì„ í•™ìŠµí•˜ê³  ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ íŒ¨í„´ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
date: 2025-09-16
author: Data Droid
category: data-engineering
tags: [Apache-Flink, ê³ ê¸‰ìƒíƒœê´€ë¦¬, ì²´í¬í¬ì¸íŒ…, ì„¸ì´ë¸Œí¬ì¸íŠ¸, ì‹œê°„ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë°ìµœì í™”, Python, PyFlink]
series: apache-flink-complete-guide
series_order: 2
reading_time: "40ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 2: Apache Flink ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬ - í”„ë¡œë•ì…˜ê¸‰ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ

> Apache Flinkì˜ ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬, ì²´í¬í¬ì¸íŒ…, ì„¸ì´ë¸Œí¬ì¸íŠ¸, ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬ ì „ëµì„ í•™ìŠµí•˜ê³  ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ íŒ¨í„´ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ íŒ¨í„´](#ê³ ê¸‰-ìƒíƒœ-ê´€ë¦¬-íŒ¨í„´)
2. [ì²´í¬í¬ì¸íŒ…ê³¼ ì„¸ì´ë¸Œí¬ì¸íŠ¸ ì‹¬í™”](#ì²´í¬í¬ì¸íŒ…ê³¼-ì„¸ì´ë¸Œí¬ì¸íŠ¸-ì‹¬í™”)
3. [ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬ ì „ëµ](#ë³µì¡í•œ-ì‹œê°„-ì²˜ë¦¬-ì „ëµ)
4. [ì„±ëŠ¥ ìµœì í™” ê¸°ë²•](#ì„±ëŠ¥-ìµœì í™”-ê¸°ë²•)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ì¶”ì²œ-ì‹œìŠ¤í…œ)
6. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ—ƒï¸ ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ íŒ¨í„´

### ìƒíƒœ ë°±ì—”ë“œ (State Backend)

FlinkëŠ” ë‹¤ì–‘í•œ ìƒíƒœ ë°±ì—”ë“œë¥¼ ì œê³µí•˜ì—¬ ë‹¤ì–‘í•œ ì„±ëŠ¥ê³¼ ë‚´êµ¬ì„± ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.

#### **1. MemoryStateBackend**
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.state import MemoryStateBackend

env = StreamExecutionEnvironment.get_execution_environment()

# ë©”ëª¨ë¦¬ ìƒíƒœ ë°±ì—”ë“œ ì„¤ì • (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
env.set_state_backend(MemoryStateBackend())

# ì„¤ì • ì˜µì…˜
backend = MemoryStateBackend(
    max_state_size=5 * 1024 * 1024,  # 5MB
    asynchronous_snapshots=True
)
env.set_state_backend(backend)
```

#### **2. FsStateBackend**
```python
from pyflink.datastream.state import FsStateBackend

# íŒŒì¼ ì‹œìŠ¤í…œ ìƒíƒœ ë°±ì—”ë“œ ì„¤ì •
backend = FsStateBackend(
    checkpoint_data_uri="file:///tmp/flink-checkpoints",
    savepoint_data_uri="file:///tmp/flink-savepoints",
    asynchronous_snapshots=True
)
env.set_state_backend(backend)
```

#### **3. RocksDBStateBackend**
```python
from pyflink.datastream.state import RocksDBStateBackend

# RocksDB ìƒíƒœ ë°±ì—”ë“œ ì„¤ì • (í”„ë¡œë•ì…˜ ê¶Œì¥)
backend = RocksDBStateBackend(
    checkpoint_data_uri="file:///tmp/flink-checkpoints",
    savepoint_data_uri="file:///tmp/flink-savepoints",
    enable_incremental_checkpointing=True
)
env.set_state_backend(backend)

# RocksDB ìµœì í™” ì„¤ì •
backend.set_predefined_options(RocksDBConfigurableOptions.PREDEFINED_OPTION_DEFAULT)
backend.set_rocksdb_options(RocksDBConfigurableOptions.ROCKSDB_OPTIONS_FILE)
```

### ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ íŒ¨í„´

#### **1. ìƒíƒœ TTL (Time-To-Live)**
```python
from pyflink.common.state import StateTtlConfig, ValueStateDescriptor
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types

class TTLStateExample(KeyedProcessFunction):
    def __init__(self):
        self.user_session_state = None
    
    def open(self, runtime_context):
        # TTL ì„¤ì •
        ttl_config = StateTtlConfig.new_builder(Time.hours(24)) \
            .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite) \
            .set_state_visibility(StateTtlConfig.StateVisibility.NeverReturnExpired) \
            .cleanup_full_snapshot() \
            .build()
        
        # TTLì´ ì ìš©ëœ ìƒíƒœ ë””ìŠ¤í¬ë¦½í„°
        state_descriptor = ValueStateDescriptor("user_session", Types.STRING())
        state_descriptor.enable_time_to_live(ttl_config)
        
        self.user_session_state = runtime_context.get_state(state_descriptor)
    
    def process_element(self, value, ctx):
        # ìƒíƒœ ì‚¬ìš©
        current_session = self.user_session_state.value()
        if current_session is None:
            # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
            new_session = f"session_{ctx.timestamp()}"
            self.user_session_state.update(new_session)
            ctx.collect(f"New session created: {new_session}")
        else:
            ctx.collect(f"Existing session: {current_session}")
```

#### **2. ìƒíƒœ ìŠ¤ëƒ…ìƒ·ê³¼ ë³µì›**
```python
from pyflink.common.state import CheckpointedFunction, ListState

class StatefulFunction(KeyedProcessFunction, CheckpointedFunction):
    def __init__(self):
        self.buffered_elements = []
        self.buffered_elements_state = None
    
    def open(self, runtime_context):
        # ìƒíƒœ ì´ˆê¸°í™”ëŠ” checkpointed_functionì—ì„œ ìˆ˜í–‰
        pass
    
    def initialize_state(self, context):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœ ì´ˆê¸°í™”"""
        self.buffered_elements_state = context.get_operator_state(
            ListStateDescriptor("buffered-elements", Types.STRING())
        )
        
        # ì´ì „ ìƒíƒœ ë³µì›
        if context.is_restored():
            for element in self.buffered_elements_state.get():
                self.buffered_elements.append(element)
    
    def snapshot_state(self, context):
        """ìƒíƒœ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        self.buffered_elements_state.clear()
        for element in self.buffered_elements:
            self.buffered_elements_state.add(element)
    
    def process_element(self, value, ctx):
        # ë²„í¼ì— ìš”ì†Œ ì¶”ê°€
        self.buffered_elements.append(value)
        
        # ì¡°ê±´ì— ë”°ë¼ ì²˜ë¦¬
        if len(self.buffered_elements) >= 10:
            # ë°°ì¹˜ ì²˜ë¦¬
            result = self.process_batch(self.buffered_elements)
            ctx.collect(result)
            self.buffered_elements.clear()
    
    def process_batch(self, elements):
        return f"Processed batch of {len(elements)} elements"
```

#### **3. ìƒíƒœ ë¶„í• ê³¼ ë³‘í•©**
```python
class PartitionableStateFunction(KeyedProcessFunction, CheckpointedFunction):
    def __init__(self):
        self.partitioned_data = {}
        self.partitioned_state = None
    
    def initialize_state(self, context):
        """ë¶„í•  ê°€ëŠ¥í•œ ìƒíƒœ ì´ˆê¸°í™”"""
        self.partitioned_state = context.get_union_list_state(
            ListStateDescriptor("partitioned-data", Types.STRING())
        )
        
        # ìƒíƒœ ë³µì›
        if context.is_restored():
            for data in self.partitioned_state.get():
                self.partitioned_data[data] = data
    
    def snapshot_state(self, context):
        """ìƒíƒœ ìŠ¤ëƒ…ìƒ·"""
        self.partitioned_state.clear()
        for data in self.partitioned_data.values():
            self.partitioned_state.add(data)
    
    def process_element(self, value, ctx):
        # ë¶„í• ëœ ë°ì´í„° ì²˜ë¦¬
        partition_key = value % 4  # 4ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í• 
        
        if partition_key not in self.partitioned_data:
            self.partitioned_data[partition_key] = []
        
        self.partitioned_data[partition_key].append(value)
        
        # íŒŒí‹°ì…˜ë³„ ì§‘ê³„
        if len(self.partitioned_data[partition_key]) >= 5:
            result = sum(self.partitioned_data[partition_key])
            ctx.collect(f"Partition {partition_key}: {result}")
            self.partitioned_data[partition_key].clear()
```

## ğŸ”„ ì²´í¬í¬ì¸íŒ…ê³¼ ì„¸ì´ë¸Œí¬ì¸íŠ¸ ì‹¬í™” {#ì²´í¬í¬ì¸íŒ…ê³¼-ì„¸ì´ë¸Œí¬ì¸íŠ¸-ì‹¬í™”}

### ê³ ê¸‰ ì²´í¬í¬ì¸íŒ… ì„¤ì •

```python
from pyflink.common.checkpointing import CheckpointingMode, ExternalizedCheckpointCleanup
from pyflink.common.time import Time

class AdvancedCheckpointingSetup:
    def __init__(self, env):
        self.env = env
        self.setup_checkpointing()
    
    def setup_checkpointing(self):
        checkpoint_config = self.env.get_checkpoint_config()
        
        # ê¸°ë³¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        checkpoint_config.enable_checkpointing(1000)  # 1ì´ˆë§ˆë‹¤
        
        # Exactly-once ëª¨ë“œ ì„¤ì •
        checkpoint_config.set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
        
        # ì™¸ë¶€í™”ëœ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • (ì‘ì—… ì·¨ì†Œ í›„ì—ë„ ìœ ì§€)
        checkpoint_config.enable_externalized_checkpoints(
            ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
        )
        
        # ìµœëŒ€ ë™ì‹œ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
        checkpoint_config.set_max_concurrent_checkpoints(1)
        
        # ì²´í¬í¬ì¸íŠ¸ ê°„ ìµœì†Œ ëŒ€ê¸° ì‹œê°„
        checkpoint_config.set_min_pause_between_checkpoints(500)
        
        # ì²´í¬í¬ì¸íŠ¸ íƒ€ì„ì•„ì›ƒ
        checkpoint_config.set_checkpoint_timeout(60000)  # 60ì´ˆ
        
        # ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜
        checkpoint_config.set_tolerable_checkpoint_failure_number(3)
        
        # ì§€ì—° ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        checkpoint_config.set_unaligned_checkpoints(True)
        checkpoint_config.set_unaligned_checkpoints_enabled(True)
```

### ì„¸ì´ë¸Œí¬ì¸íŠ¸ ê´€ë¦¬

```python
import requests
import json

class SavepointManager:
    def __init__(self, flink_rest_url="http://localhost:8081"):
        self.rest_url = flink_rest_url
    
    def create_savepoint(self, job_id, savepoint_path):
        """ì„¸ì´ë¸Œí¬ì¸íŠ¸ ìƒì„±"""
        url = f"{self.rest_url}/jobs/{job_id}/savepoints"
        
        payload = {
            "target-directory": savepoint_path,
            "cancel-job": False
        }
        
        response = requests.post(url, json=payload)
        
        if response.status_code == 202:
            trigger_id = response.json()["request-id"]
            return self.wait_for_completion(trigger_id)
        else:
            raise Exception(f"Failed to create savepoint: {response.text}")
    
    def wait_for_completion(self, trigger_id, max_wait_time=300):
        """ì„¸ì´ë¸Œí¬ì¸íŠ¸ ì™„ë£Œ ëŒ€ê¸°"""
        import time
        
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            url = f"{self.rest_url}/jobs/savepoints/{trigger_id}"
            response = requests.get(url)
            
            if response.status_code == 200:
                result = response.json()
                if result["status"]["id"] == "COMPLETED":
                    return result["operation"]["location"]
                elif result["status"]["id"] == "FAILED":
                    raise Exception(f"Savepoint failed: {result}")
            
            time.sleep(2)
        
        raise Exception("Savepoint creation timeout")
    
    def restore_from_savepoint(self, savepoint_path, jar_path):
        """ì„¸ì´ë¸Œí¬ì¸íŠ¸ì—ì„œ ë³µì›"""
        url = f"{self.rest_url}/jars/upload"
        
        # JAR íŒŒì¼ ì—…ë¡œë“œ
        with open(jar_path, 'rb') as f:
            files = {'jarfile': f}
            response = requests.post(url, files=files)
        
        if response.status_code == 200:
            jar_id = response.json()["filename"].split("/")[-1]
            
            # ì„¸ì´ë¸Œí¬ì¸íŠ¸ì—ì„œ ì‘ì—… ì‹œì‘
            start_url = f"{self.rest_url}/jars/{jar_id}/run"
            payload = {
                "savepointPath": savepoint_path
            }
            
            start_response = requests.post(start_url, json=payload)
            return start_response.status_code == 200
        else:
            raise Exception(f"Failed to upload JAR: {response.text}")
    
    def list_savepoints(self):
        """ì„¸ì´ë¸Œí¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼ ì‹œìŠ¤í…œì„ ìŠ¤ìº”í•˜ê±°ë‚˜ ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œë¥¼ ì¡°íšŒ
        return []

# ì‚¬ìš© ì˜ˆì œ
def manage_savepoints_example():
    manager = SavepointManager()
    
    try:
        # ì„¸ì´ë¸Œí¬ì¸íŠ¸ ìƒì„±
        savepoint_location = manager.create_savepoint(
            "job-id-123",
            "file:///tmp/flink-savepoints/"
        )
        print(f"Savepoint created: {savepoint_location}")
        
        # ì„¸ì´ë¸Œí¬ì¸íŠ¸ì—ì„œ ë³µì›
        success = manager.restore_from_savepoint(
            savepoint_location,
            "/path/to/job.jar"
        )
        print(f"Restore success: {success}")
        
    except Exception as e:
        print(f"Error: {e}")
```

### ì²´í¬í¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§

```python
class CheckpointMonitor:
    def __init__(self, flink_rest_url="http://localhost:8081"):
        self.rest_url = flink_rest_url
    
    def get_checkpoint_metrics(self, job_id):
        """ì²´í¬í¬ì¸íŠ¸ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        url = f"{self.rest_url}/jobs/{job_id}/checkpoints"
        response = requests.get(url)
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"Failed to get checkpoint metrics: {response.text}")
    
    def analyze_checkpoint_performance(self, job_id):
        """ì²´í¬í¬ì¸íŠ¸ ì„±ëŠ¥ ë¶„ì„"""
        metrics = self.get_checkpoint_metrics(job_id)
        
        checkpoints = metrics.get("latest", {}).get("completed", [])
        
        if not checkpoints:
            return {"message": "No completed checkpoints found"}
        
        # ì„±ëŠ¥ ë¶„ì„
        durations = [cp["duration"] for cp in checkpoints]
        sizes = [cp["size"] for cp in checkpoints]
        
        analysis = {
            "total_checkpoints": len(checkpoints),
            "avg_duration_ms": sum(durations) / len(durations),
            "max_duration_ms": max(durations),
            "min_duration_ms": min(durations),
            "avg_size_bytes": sum(sizes) / len(sizes),
            "max_size_bytes": max(sizes),
            "min_size_bytes": min(sizes)
        }
        
        # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        if analysis["avg_duration_ms"] > 10000:  # 10ì´ˆ ì´ìƒ
            recommendations.append({
                "type": "performance",
                "message": "ì²´í¬í¬ì¸íŠ¸ ì‹œê°„ì´ ê¸¸ì–´ì„œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒíƒœ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ë°±ì—”ë“œë¥¼ ìµœì í™”í•˜ì„¸ìš”."
            })
        
        if analysis["avg_size_bytes"] > 100 * 1024 * 1024:  # 100MB ì´ìƒ
            recommendations.append({
                "type": "storage",
                "message": "ì²´í¬í¬ì¸íŠ¸ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ìƒíƒœë¥¼ ì •ë¦¬í•˜ê±°ë‚˜ TTLì„ ì„¤ì •í•˜ì„¸ìš”."
            })
        
        analysis["recommendations"] = recommendations
        
        return analysis

# ëª¨ë‹ˆí„°ë§ ì˜ˆì œ
def checkpoint_monitoring_example():
    monitor = CheckpointMonitor()
    
    try:
        analysis = monitor.analyze_checkpoint_performance("job-id-123")
        print("=== Checkpoint Performance Analysis ===")
        print(json.dumps(analysis, indent=2))
    except Exception as e:
        print(f"Error: {e}")
```

## â° ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬ ì „ëµ

### ë‹¤ì¤‘ ì‹œê°„ ìœˆë„ìš° ì²˜ë¦¬

```python
from pyflink.datastream.window import TumblingEventTimeWindows, SlidingEventTimeWindows
from pyflink.datastream.functions import AllWindowFunction
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types

class MultiTimeWindowProcessor(AllWindowFunction):
    def __init__(self):
        self.window_results = {}
    
    def apply(self, window, inputs, out):
        """ë‹¤ì¤‘ ì‹œê°„ ìœˆë„ìš°ì—ì„œ ë°ì´í„° ì²˜ë¦¬"""
        window_start = window.start
        window_end = window.end
        
        # ìœˆë„ìš°ë³„ ì§‘ê³„
        window_key = f"{window_start}_{window_end}"
        
        if window_key not in self.window_results:
            self.window_results[window_key] = {
                "count": 0,
                "sum": 0,
                "min": float('inf'),
                "max": float('-inf')
            }
        
        for element in inputs:
            value = element[1]  # (timestamp, value) í˜•íƒœ ê°€ì •
            self.window_results[window_key]["count"] += 1
            self.window_results[window_key]["sum"] += value
            self.window_results[window_key]["min"] = min(
                self.window_results[window_key]["min"], value
            )
            self.window_results[window_key]["max"] = max(
                self.window_results[window_key]["max"], value
            )
        
        # ê²°ê³¼ ì¶œë ¥
        result = self.window_results[window_key].copy()
        result["window_start"] = window_start
        result["window_end"] = window_end
        out.collect(result)

class AdvancedTimeProcessing:
    def __init__(self, env):
        self.env = env
    
    def setup_multi_window_processing(self, data_stream):
        """ë‹¤ì¤‘ ìœˆë„ìš° ì²˜ë¦¬ ì„¤ì •"""
        # 1ë¶„ ìœˆë„ìš°
        one_minute_window = data_stream.window_all(
            TumblingEventTimeWindows.of(Time.minutes(1))
        ).apply(MultiTimeWindowProcessor(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        # 5ë¶„ ìœˆë„ìš°
        five_minute_window = data_stream.window_all(
            TumblingEventTimeWindows.of(Time.minutes(5))
        ).apply(MultiTimeWindowProcessor(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        # 1ë¶„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (30ì´ˆ ìŠ¬ë¼ì´ë“œ)
        sliding_window = data_stream.window_all(
            SlidingEventTimeWindows.of(Time.minutes(1), Time.seconds(30))
        ).apply(MultiTimeWindowProcessor(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        return {
            "one_minute": one_minute_window,
            "five_minute": five_minute_window,
            "sliding": sliding_window
        }
```

### ë™ì  ì›Œí„°ë§ˆí¬ ìƒì„±

```python
from pyflink.datastream.functions import ProcessFunction
from pyflink.common.time import Time

class DynamicWatermarkGenerator(ProcessFunction):
    def __init__(self, max_delay_seconds=60):
        self.max_delay_seconds = max_delay_seconds
        self.current_watermark = 0
        self.element_count = 0
        self.delay_statistics = []
    
    def process_element(self, element, ctx):
        """ë™ì  ì›Œí„°ë§ˆí¬ ìƒì„±"""
        event_time = element.timestamp
        processing_time = ctx.timer_service().current_processing_time()
        
        # ì§€ì—° ì‹œê°„ ê³„ì‚°
        delay = processing_time - event_time
        self.delay_statistics.append(delay)
        
        # ìµœê·¼ 100ê°œ ìš”ì†Œì˜ í‰ê·  ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì›Œí„°ë§ˆí¬ ì¡°ì •
        if len(self.delay_statistics) > 100:
            self.delay_statistics.pop(0)
        
        if self.delay_statistics:
            avg_delay = sum(self.delay_statistics) / len(self.delay_statistics)
            dynamic_delay = max(avg_delay * 1.5, self.max_delay_seconds * 1000)
            
            # ë™ì  ì›Œí„°ë§ˆí¬ ìƒì„±
            new_watermark = event_time - dynamic_delay
            if new_watermark > self.current_watermark:
                self.current_watermark = new_watermark
                ctx.timer_service().register_event_time_timer(new_watermark)
        
        # ìš”ì†Œ ì²˜ë¦¬
        ctx.collect(element)
    
    def on_timer(self, timestamp, ctx, out):
        """íƒ€ì´ë¨¸ ê¸°ë°˜ ì›Œí„°ë§ˆí¬ ìƒì„±"""
        # ì£¼ê¸°ì ìœ¼ë¡œ ì›Œí„°ë§ˆí¬ ìƒì„±
        current_time = ctx.timer_service().current_processing_time()
        next_watermark = current_time - (self.max_delay_seconds * 1000)
        
        if next_watermark > self.current_watermark:
            self.current_watermark = next_watermark
            ctx.timer_service().register_processing_time_timer(current_time + 1000)
```

### ì‹œê°„ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬

```python
from pyflink.common.state import ValueStateDescriptor
from pyflink.datastream.functions import KeyedProcessFunction

class TimeBasedStateManager(KeyedProcessFunction):
    def __init__(self):
        self.user_activity_state = None
        self.last_activity_time = None
        self.session_timeout_ms = 30 * 60 * 1000  # 30ë¶„
    
    def open(self, runtime_context):
        self.user_activity_state = runtime_context.get_state(
            ValueStateDescriptor("user_activity", Types.STRING())
        )
        self.last_activity_time = runtime_context.get_state(
            ValueStateDescriptor("last_activity_time", Types.LONG())
        )
    
    def process_element(self, element, ctx):
        """ì‹œê°„ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬"""
        current_time = ctx.timestamp()
        user_id = element.user_id
        
        # ì´ì „ í™œë™ ì‹œê°„ í™•ì¸
        last_time = self.last_activity_time.value()
        
        if last_time is None or (current_time - last_time) > self.session_timeout_ms:
            # ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
            new_session = f"session_{current_time}"
            self.user_activity_state.update(new_session)
            
            # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ íƒ€ì´ë¨¸ ë“±ë¡
            timeout_timer = current_time + self.session_timeout_ms
            ctx.timer_service().register_event_time_timer(timeout_timer)
            
            ctx.collect(f"New session started for user {user_id}: {new_session}")
        else:
            # ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸
            current_session = self.user_activity_state.value()
            ctx.collect(f"Activity in existing session: {current_session}")
        
        # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
        self.last_activity_time.update(current_time)
    
    def on_timer(self, timestamp, ctx, out):
        """ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬"""
        # ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬
        session = self.user_activity_state.value()
        if session:
            out.collect(f"Session timeout: {session}")
            self.user_activity_state.clear()
            self.last_activity_time.clear()
```

## âš¡ ì„±ëŠ¥ ìµœì í™” ê¸°ë²• {#ì„±ëŠ¥-ìµœì í™”-ê¸°ë²•}

### ë³‘ë ¬ì„± ìµœì í™”

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema

class ParallelismOptimizer:
    def __init__(self, env):
        self.env = env
    
    def optimize_kafka_parallelism(self, topic, bootstrap_servers):
        """Kafka ë³‘ë ¬ì„± ìµœì í™”"""
        kafka_properties = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': 'optimized-consumer-group'
        }
        
        kafka_source = FlinkKafkaConsumer(
            topic,
            SimpleStringSchema(),
            kafka_properties
        )
        
        # íŒŒí‹°ì…˜ ìˆ˜ì— ë§ì¶° ë³‘ë ¬ì„± ì„¤ì •
        kafka_source.set_parallelism(4)  # Kafka íŒŒí‹°ì…˜ ìˆ˜ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        
        return self.env.add_source(kafka_source)
    
    def optimize_processing_parallelism(self, data_stream):
        """ì²˜ë¦¬ ë³‘ë ¬ì„± ìµœì í™”"""
        # í‚¤ë³„ ë³‘ë ¬ì„± ì„¤ì •
        keyed_stream = data_stream.key_by(lambda x: x[0])  # ì²« ë²ˆì§¸ í•„ë“œë¡œ í‚¤ ì§€ì •
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        optimized_stream = keyed_stream.map(
            lambda x: self.expensive_operation(x),
            output_type=Types.STRING()
        ).set_parallelism(8)  # ë†’ì€ ë³‘ë ¬ì„±ìœ¼ë¡œ ì„¤ì •
        
        return optimized_stream
    
    def expensive_operation(self, data):
        """ë¹„ìš©ì´ í° ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜"""
        import time
        time.sleep(0.01)  # 10ms ì²˜ë¦¬ ì‹œê°„
        return f"processed_{data}"

class MemoryOptimizer:
    def __init__(self, env):
        self.env = env
    
    def setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        # JVM í™ ë©”ëª¨ë¦¬ ì„¤ì •
        self.env.get_config().set_global_job_parameters({
            "taskmanager.memory.process.size": "2048m",
            "taskmanager.memory.managed.size": "1024m"
        })
        
        # ë„¤íŠ¸ì›Œí¬ ë²„í¼ ì„¤ì •
        self.env.get_config().set_global_job_parameters({
            "taskmanager.network.memory.fraction": "0.2",
            "taskmanager.network.memory.min": "128mb",
            "taskmanager.network.memory.max": "1gb"
        })
        
        # ìƒíƒœ ë°±ì—”ë“œ ìµœì í™”
        from pyflink.datastream.state import RocksDBStateBackend
        backend = RocksDBStateBackend(
            checkpoint_data_uri="file:///tmp/flink-checkpoints",
            enable_incremental_checkpointing=True
        )
        
        # RocksDB ë©”ëª¨ë¦¬ ì„¤ì •
        backend.set_rocksdb_options({
            "write_buffer_size": "64MB",
            "max_write_buffer_number": "3",
            "block_cache_size": "256MB"
        })
        
        self.env.set_state_backend(backend)
    
    def optimize_data_serialization(self, data_stream):
        """ë°ì´í„° ì§ë ¬í™” ìµœì í™”"""
        # Kryo ì§ë ¬í™” ì„¤ì •
        self.env.get_config().set_global_job_parameters({
            "taskmanager.runtime.kryo.default.serializers": "true"
        })
        
        # Arrow ê¸°ë°˜ ì§ë ¬í™” ì‚¬ìš©
        optimized_stream = data_stream.map(
            lambda x: x,  # ë³€í™˜ ì—†ìŒ
            output_type=Types.PICKLED_BYTE_ARRAY()  # ìµœì í™”ëœ íƒ€ì…
        )
        
        return optimized_stream
```

### ë„¤íŠ¸ì›Œí¬ ìµœì í™”

```python
class NetworkOptimizer:
    def __init__(self, env):
        self.env = env
    
    def setup_network_optimization(self):
        """ë„¤íŠ¸ì›Œí¬ ìµœì í™” ì„¤ì •"""
        # ë„¤íŠ¸ì›Œí¬ ìŠ¤íƒ ìµœì í™”
        self.env.get_config().set_global_job_parameters({
            "taskmanager.network.netty.num-arenas": "4",
            "taskmanager.network.netty.server.numThreads": "4",
            "taskmanager.network.netty.client.numThreads": "4",
            "taskmanager.network.netty.server.backlog": "0"
        })
        
        # ë²„í¼ í¬ê¸° ìµœì í™”
        self.env.get_config().set_global_job_parameters({
            "taskmanager.network.memory.buffers-per-channel": "2",
            "taskmanager.network.memory.floating-buffers-per-gate": "8"
        })
    
    def optimize_shuffle_network(self, data_stream):
        """Shuffle ë„¤íŠ¸ì›Œí¬ ìµœì í™”"""
        # ë°ì´í„° íŒŒí‹°ì…”ë‹ ìµœì í™”
        optimized_stream = data_stream.partition_custom(
            lambda key, num_partitions: hash(key) % num_partitions,
            lambda x: x[0]  # í‚¤ ì¶”ì¶œ í•¨ìˆ˜
        )
        
        return optimized_stream

class LatencyOptimizer:
    def __init__(self, env):
        self.env = env
    
    def setup_low_latency_config(self):
        """ì €ì§€ì—° ì„¤ì •"""
        # ë²„í¼ íƒ€ì„ì•„ì›ƒ ìµœì†Œí™”
        self.env.get_config().set_global_job_parameters({
            "taskmanager.network.netty.server.backlog": "0",
            "taskmanager.network.netty.client.connectTimeoutSec": "10"
        })
        
        # ì²´í¬í¬ì¸íŒ… ê°„ê²© ì¡°ì •
        self.env.get_checkpoint_config().enable_checkpointing(100)  # 100ms
        
        # ì§€ì—° ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
        self.env.get_checkpoint_config().set_unaligned_checkpoints(True)
    
    def optimize_processing_latency(self, data_stream):
        """ì²˜ë¦¬ ì§€ì—°ì‹œê°„ ìµœì í™”"""
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì„¤ì •
        self.env.set_buffer_timeout(1)  # 1ms ë²„í¼ íƒ€ì„ì•„ì›ƒ
        
        # ì¦‰ì‹œ ì²˜ë¦¬ ì„¤ì •
        optimized_stream = data_stream.map(
            lambda x: x,
            output_type=Types.STRING()
        ).set_buffer_timeout(0)  # ë²„í¼ë§ ì—†ì´ ì¦‰ì‹œ ì²˜ë¦¬
        
        return optimized_stream
```

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ {#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ì¶”ì²œ-ì‹œìŠ¤í…œ}

### í”„ë¡œì íŠ¸ ê°œìš”

ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

### 1. ë°ì´í„° ëª¨ë¸ ì •ì˜

```python
from dataclasses import dataclass
from typing import List, Dict, Optional
import json

@dataclass
class UserEvent:
    user_id: str
    item_id: str
    event_type: str  # view, click, purchase, like
    timestamp: int
    session_id: str
    metadata: Dict
    
    def to_json(self):
        return json.dumps({
            'user_id': self.user_id,
            'item_id': self.item_id,
            'event_type': self.event_type,
            'timestamp': self.timestamp,
            'session_id': self.session_id,
            'metadata': self.metadata
        })
    
    @classmethod
    def from_json(cls, json_str):
        data = json.loads(json_str)
        return cls(**data)

@dataclass
class UserProfile:
    user_id: str
    interests: Dict[str, float]  # ì¹´í…Œê³ ë¦¬ë³„ ê´€ì‹¬ë„
    behavior_patterns: Dict[str, int]  # í–‰ë™ íŒ¨í„´
    last_updated: int
    
    def update_interest(self, category: str, weight: float):
        """ê´€ì‹¬ë„ ì—…ë°ì´íŠ¸"""
        if category not in self.interests:
            self.interests[category] = 0.0
        
        # ì§€ìˆ˜ ì´ë™ í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.interests[category] = (1 - alpha) * self.interests[category] + alpha * weight
    
    def get_top_interests(self, top_k: int = 5):
        """ìƒìœ„ ê´€ì‹¬ì‚¬ ë°˜í™˜"""
        return sorted(
            self.interests.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

@dataclass
class Recommendation:
    user_id: str
    item_id: str
    score: float
    reason: str
    timestamp: int
```

### 2. ì‹¤ì‹œê°„ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ë§

```python
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.state import ValueStateDescriptor, MapStateDescriptor
from pyflink.common.typeinfo import Types

class RealTimeUserProfiler(KeyedProcessFunction):
    def __init__(self):
        self.user_profile_state = None
        self.item_categories = {}  # ì•„ì´í…œ-ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    
    def open(self, runtime_context):
        self.user_profile_state = runtime_context.get_state(
            ValueStateDescriptor("user_profile", Types.STRING())
        )
    
    def process_element(self, user_event, ctx):
        """ì‹¤ì‹œê°„ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸"""
        user_id = user_event.user_id
        
        # í˜„ì¬ í”„ë¡œíŒŒì¼ ë¡œë“œ
        profile_json = self.user_profile_state.value()
        if profile_json:
            profile = UserProfile.from_json(profile_json)
        else:
            profile = UserProfile(
                user_id=user_id,
                interests={},
                behavior_patterns={},
                last_updated=user_event.timestamp
            )
        
        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
        self.update_profile_from_event(profile, user_event)
        
        # í”„ë¡œíŒŒì¼ ì €ì¥
        self.user_profile_state.update(profile.to_json())
        
        # ì—…ë°ì´íŠ¸ëœ í”„ë¡œíŒŒì¼ ì¶œë ¥
        ctx.collect(profile)
    
    def update_profile_from_event(self, profile: UserProfile, event: UserEvent):
        """ì´ë²¤íŠ¸ ê¸°ë°˜ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸"""
        # ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ë³„ë„ ì„œë¹„ìŠ¤ì—ì„œ ì¡°íšŒ)
        category = self.get_item_category(event.item_id)
        
        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        event_weights = {
            'view': 0.1,
            'click': 0.3,
            'like': 0.5,
            'purchase': 1.0
        }
        
        weight = event_weights.get(event.event_type, 0.1)
        
        # ê´€ì‹¬ë„ ì—…ë°ì´íŠ¸
        profile.update_interest(category, weight)
        
        # í–‰ë™ íŒ¨í„´ ì—…ë°ì´íŠ¸
        pattern_key = f"{event.event_type}_{category}"
        profile.behavior_patterns[pattern_key] = \
            profile.behavior_patterns.get(pattern_key, 0) + 1
        
        profile.last_updated = event.timestamp
    
    def get_item_category(self, item_id: str) -> str:
        """ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ìºì‹œì—ì„œ ì¡°íšŒ
        categories = ['electronics', 'books', 'clothing', 'home', 'sports']
        return categories[hash(item_id) % len(categories)]
    
    def to_json(self):
        return json.dumps({
            'user_id': self.user_id,
            'interests': self.interests,
            'behavior_patterns': self.behavior_patterns,
            'last_updated': self.last_updated
        })
    
    @classmethod
    def from_json(cls, json_str):
        data = json.loads(json_str)
        return cls(**data)
```

### 3. í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì¶”ì²œ ì—”ì§„

```python
class CollaborativeFilteringEngine(KeyedProcessFunction):
    def __init__(self):
        self.user_item_matrix = None
        self.item_item_similarity = None
        self.user_similarity = None
    
    def open(self, runtime_context):
        self.user_item_matrix = runtime_context.get_map_state(
            MapStateDescriptor("user_item_matrix", Types.STRING(), Types.FLOAT())
        )
        self.item_item_similarity = runtime_context.get_map_state(
            MapStateDescriptor("item_similarity", Types.STRING(), Types.FLOAT())
        )
        self.user_similarity = runtime_context.get_map_state(
            MapStateDescriptor("user_similarity", Types.STRING(), Types.FLOAT())
        )
    
    def process_element(self, user_event, ctx):
        """í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        user_id = user_event.user_id
        item_id = user_event.item_id
        
        # ì‚¬ìš©ì-ì•„ì´í…œ í–‰ë ¬ ì—…ë°ì´íŠ¸
        matrix_key = f"{user_id}:{item_id}"
        current_rating = self.user_item_matrix.get(matrix_key) or 0.0
        
        # ì´ë²¤íŠ¸ ê¸°ë°˜ í‰ì  ê³„ì‚°
        event_rating = self.calculate_rating_from_event(user_event)
        new_rating = (current_rating + event_rating) / 2
        
        self.user_item_matrix.put(matrix_key, new_rating)
        
        # ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ ìƒì„±
        item_recommendations = self.generate_item_based_recommendations(user_id, item_id)
        
        # ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ ìƒì„±
        user_recommendations = self.generate_user_based_recommendations(user_id)
        
        # ì¶”ì²œ ê²°ê³¼ í†µí•©
        final_recommendations = self.combine_recommendations(
            item_recommendations, user_recommendations
        )
        
        for rec in final_recommendations:
            ctx.collect(rec)
    
    def calculate_rating_from_event(self, event: UserEvent) -> float:
        """ì´ë²¤íŠ¸ ê¸°ë°˜ í‰ì  ê³„ì‚°"""
        rating_map = {
            'view': 1.0,
            'click': 2.0,
            'like': 3.0,
            'purchase': 5.0
        }
        return rating_map.get(event.event_type, 1.0)
    
    def generate_item_based_recommendations(self, user_id: str, item_id: str, top_k: int = 10):
        """ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        # ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°
        similar_items = self.find_similar_items(item_id, top_k)
        
        recommendations = []
        for similar_item, similarity in similar_items:
            # ì‚¬ìš©ìê°€ ì•„ì§ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ì•„ì´í…œë§Œ ì¶”ì²œ
            matrix_key = f"{user_id}:{similar_item}"
            if not self.user_item_matrix.get(matrix_key):
                score = similarity * 0.8  # ì•„ì´í…œ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                recommendations.append(Recommendation(
                    user_id=user_id,
                    item_id=similar_item,
                    score=score,
                    reason="Similar to items you liked",
                    timestamp=int(time.time())
                ))
        
        return sorted(recommendations, key=lambda x: x.score, reverse=True)[:top_k]
    
    def generate_user_based_recommendations(self, user_id: str, top_k: int = 10):
        """ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        # ìœ ì‚¬í•œ ì‚¬ìš©ì ì°¾ê¸°
        similar_users = self.find_similar_users(user_id, top_k)
        
        recommendations = []
        for similar_user, similarity in similar_users:
            # ìœ ì‚¬í•œ ì‚¬ìš©ìê°€ ì¢‹ì•„í•œ ì•„ì´í…œ ì¤‘ í˜„ì¬ ì‚¬ìš©ìê°€ ì•„ì§ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ê²ƒ
            user_items = self.get_user_items(similar_user)
            current_user_items = self.get_user_items(user_id)
            
            for item_id in user_items:
                if item_id not in current_user_items:
                    score = similarity * 0.6  # ì‚¬ìš©ì ê¸°ë°˜ ê°€ì¤‘ì¹˜
                    recommendations.append(Recommendation(
                        user_id=user_id,
                        item_id=item_id,
                        score=score,
                        reason="Users with similar tastes liked this",
                        timestamp=int(time.time())
                    ))
        
        return sorted(recommendations, key=lambda x: x.score, reverse=True)[:top_k]
    
    def find_similar_items(self, item_id: str, top_k: int):
        """ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°"""
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ìœ ì‚¬ë„ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        similar_items = []
        
        # ëª¨ë“  ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        for other_item in self.get_all_items():
            if other_item != item_id:
                similarity = self.calculate_item_similarity(item_id, other_item)
                similar_items.append((other_item, similarity))
        
        return sorted(similar_items, key=lambda x: x[1], reverse=True)[:top_k]
    
    def calculate_item_similarity(self, item1: str, item2: str) -> float:
        """ì•„ì´í…œ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
        # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì-ì•„ì´í…œ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        return abs(hash(item1) - hash(item2)) / (hash(item1) + hash(item2) + 1)
    
    def get_all_items(self):
        """ëª¨ë“  ì•„ì´í…œ ëª©ë¡ ë°˜í™˜"""
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        return [f"item_{i}" for i in range(1000)]
    
    def get_user_items(self, user_id: str):
        """ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œ ëª©ë¡ ë°˜í™˜"""
        items = []
        for key in self.user_item_matrix.keys():
            if key.startswith(f"{user_id}:"):
                item_id = key.split(":")[1]
                items.append(item_id)
        return items
    
    def combine_recommendations(self, item_recs, user_recs):
        """ì¶”ì²œ ê²°ê³¼ í†µí•©"""
        combined = {}
        
        # ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ ì¶”ê°€
        for rec in item_recs:
            combined[rec.item_id] = rec
        
        # ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ ì¶”ê°€ (ì ìˆ˜ ì¡°ì •)
        for rec in user_recs:
            if rec.item_id in combined:
                # ì´ë¯¸ ìˆëŠ” ê²½ìš° ì ìˆ˜ í‰ê· 
                combined[rec.item_id].score = (combined[rec.item_id].score + rec.score) / 2
            else:
                combined[rec.item_id] = rec
        
        return list(combined.values())
```

### 4. ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ í†µí•©

```python
class RealTimeRecommendationSystem:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.setup_environment()
    
    def setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        # ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        self.env.get_checkpoint_config().enable_checkpointing(1000)
        
        # ìƒíƒœ ë°±ì—”ë“œ ì„¤ì •
        from pyflink.datastream.state import RocksDBStateBackend
        backend = RocksDBStateBackend(
            checkpoint_data_uri="file:///tmp/recommendation-checkpoints"
        )
        self.env.set_state_backend(backend)
    
    def create_kafka_source(self, topic, bootstrap_servers):
        """Kafka ì†ŒìŠ¤ ìƒì„±"""
        from pyflink.datastream.connectors import FlinkKafkaConsumer
        from pyflink.common.serialization import SimpleStringSchema
        
        kafka_properties = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': 'recommendation-system'
        }
        
        kafka_source = FlinkKafkaConsumer(
            topic,
            SimpleStringSchema(),
            kafka_properties
        )
        
        return self.env.add_source(kafka_source)
    
    def parse_user_events(self, event_stream):
        """ì‚¬ìš©ì ì´ë²¤íŠ¸ íŒŒì‹±"""
        def parse_event(event_str):
            try:
                return UserEvent.from_json(event_str)
            except Exception as e:
                print(f"Failed to parse event: {event_str}, Error: {e}")
                return None
        
        return event_stream.map(parse_event, output_type=Types.PICKLED_BYTE_ARRAY()) \
                          .filter(lambda x: x is not None)
    
    def run_recommendation_system(self):
        """ì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        # Kafkaì—ì„œ ì‚¬ìš©ì ì´ë²¤íŠ¸ ì½ê¸°
        event_stream = self.create_kafka_source("user-events", "localhost:9092")
        
        # ì´ë²¤íŠ¸ íŒŒì‹±
        parsed_events = self.parse_user_events(event_stream)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì›Œí„°ë§ˆí¬ í• ë‹¹
        from pyflink.datastream.functions import BoundedOutOfOrdernessTimestampExtractor
        from pyflink.common.time import Time
        
        class EventTimestampExtractor(BoundedOutOfOrdernessTimestampExtractor):
            def __init__(self):
                super().__init__(Time.seconds(10))
            
            def extract_timestamp(self, element, previous_timestamp):
                return element.timestamp * 1000
        
        watermarked_events = parsed_events.assign_timestamps_and_watermarks(
            EventTimestampExtractor()
        )
        
        # ì‹¤ì‹œê°„ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ë§
        user_profiles = watermarked_events.key_by(lambda event: event.user_id) \
                                        .process(RealTimeUserProfiler())
        
        # í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ì¶”ì²œ
        recommendations = watermarked_events.key_by(lambda event: event.user_id) \
                                          .process(CollaborativeFilteringEngine())
        
        # ê²°ê³¼ ì¶œë ¥
        user_profiles.print("User Profiles")
        recommendations.print("Recommendations")
        
        # ì‹¤í–‰
        self.env.execute("Real-time Recommendation System")

# ì‹œìŠ¤í…œ ì‹¤í–‰
if __name__ == "__main__":
    system = RealTimeRecommendationSystem()
    system.run_recommendation_system()
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ íŒ¨í„´**
   - ìƒíƒœ ë°±ì—”ë“œ (Memory, Fs, RocksDB)
   - ìƒíƒœ TTLê³¼ ìë™ ì •ë¦¬
   - ìƒíƒœ ìŠ¤ëƒ…ìƒ·ê³¼ ë³µì›
   - ìƒíƒœ ë¶„í• ê³¼ ë³‘í•©

2. **ì²´í¬í¬ì¸íŒ…ê³¼ ì„¸ì´ë¸Œí¬ì¸íŠ¸**
   - ê³ ê¸‰ ì²´í¬í¬ì¸íŒ… ì„¤ì •
   - ì„¸ì´ë¸Œí¬ì¸íŠ¸ ìƒì„±ê³¼ ë³µì›
   - ì²´í¬í¬ì¸íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
   - ì¥ì•  ë³µêµ¬ ì „ëµ

3. **ë³µì¡í•œ ì‹œê°„ ì²˜ë¦¬**
   - ë‹¤ì¤‘ ì‹œê°„ ìœˆë„ìš° ì²˜ë¦¬
   - ë™ì  ì›Œí„°ë§ˆí¬ ìƒì„±
   - ì‹œê°„ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬
   - ì§€ì—° ë°ì´í„° ì²˜ë¦¬

4. **ì„±ëŠ¥ ìµœì í™”**
   - ë³‘ë ¬ì„± ìµœì í™”
   - ë©”ëª¨ë¦¬ ìµœì í™”
   - ë„¤íŠ¸ì›Œí¬ ìµœì í™”
   - ì§€ì—°ì‹œê°„ ìµœì í™”

5. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ
   - ì‚¬ìš©ì í”„ë¡œíŒŒì¼ë§
   - í˜‘ì—… í•„í„°ë§
   - ê°œì¸í™” ì¶”ì²œ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬** | ë³µì¡í•œ ìƒíƒœ ì²˜ë¦¬ | â­â­â­â­â­ |
| **ì²´í¬í¬ì¸íŒ…** | ì¥ì•  ë³µêµ¬ì™€ ì•ˆì •ì„± | â­â­â­â­â­ |
| **ì‹œê°„ ì²˜ë¦¬** | ì •í™•í•œ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ | â­â­â­â­â­ |
| **ì„±ëŠ¥ ìµœì í™”** | í”„ë¡œë•ì…˜ í™˜ê²½ ìµœì í™” | â­â­â­â­ |
| **ì‹¤ì‹œê°„ ì¶”ì²œ** | ê°œì¸í™” ì„œë¹„ìŠ¤ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 3: ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEP (Complex Event Processing)**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ íŒ¨í„´
- ì‹¤ì‹œê°„ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜
- CEP íŒ¨í„´ ë§¤ì¹­
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 3: ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEP](/data-engineering/2025/09/17/apache-flink-real-time-analytics.html)

---

*ì´ì œ Flinkì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤.* ğŸš€

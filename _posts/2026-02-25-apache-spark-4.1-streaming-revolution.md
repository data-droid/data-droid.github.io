---
layout: post
lang: ko
title: "Apache Spark 4.1 - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì˜ í˜ì‹ ê³¼ Flinkì™€ì˜ ë¹„êµ"
description: "Spark 4.1ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°œì„ ì‚¬í•­ë¶€í„° Flinkì™€ì˜ ì°¨ì´ì , ê·¸ë¦¬ê³  ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œì˜ ì§„í™”ë¥¼ ì™„ì „ ì •ë³µí•©ë‹ˆë‹¤."
date: 2026-02-25
author: Data Droid
category: data-engineering
tags: [Apache-Spark, Spark-4.1, Structured-Streaming, Apache-Flink, ì‹¤ì‹œê°„ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë°, ë§ˆì´í¬ë¡œë°°ì¹˜, Continuous-Processing]
reading_time: "55ë¶„"
difficulty: "ê³ ê¸‰"
---

# ğŸš€ Apache Spark 4.1 - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì˜ í˜ì‹ ê³¼ Flinkì™€ì˜ ë¹„êµ

> **"ë§ˆì´í¬ë¡œë°°ì¹˜ì—ì„œ ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ - Spark 4.1ì´ ê°€ì ¸ì˜¨ ì‹¤ì‹œê°„ ì²˜ë¦¬ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜"** - Continuous Processing, ë‚®ì€ ì§€ì—°ì‹œê°„, Flink ìˆ˜ì¤€ì˜ ì„±ëŠ¥

Apache SparkëŠ” ì „í†µì ìœ¼ë¡œ **ë§ˆì´í¬ë¡œë°°ì¹˜(Micro-batch)** ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Spark 4.1ì—ì„œëŠ” **Continuous Processing**ê³¼ **í–¥ìƒëœ Structured Streaming**ì„ í†µí•´ **Flink ìˆ˜ì¤€ì˜ ë‚®ì€ ì§€ì—°ì‹œê°„**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Sparkì˜ ìŠ¤íŠ¸ë¦¬ë° ì§„í™” ê³¼ì •, Flinkì™€ì˜ ì°¨ì´ì , ê·¸ë¦¬ê³  Spark 4.1ì˜ í˜ì‹ ì ì¸ ê°œì„ ì‚¬í•­ì„ ì™„ì „íˆ ì •ë³µí•©ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨

- [Spark vs Flink: ê·¼ë³¸ì  ì°¨ì´ì ](#spark-vs-flink-ê·¼ë³¸ì -ì°¨ì´ì )
- [Spark ìŠ¤íŠ¸ë¦¬ë°ì˜ ì§„í™” ê³¼ì •](#spark-ìŠ¤íŠ¸ë¦¬ë°ì˜-ì§„í™”-ê³¼ì •)
- [Spark 4.1ì˜ ì£¼ìš” ê°œì„ ì‚¬í•­](#spark-41ì˜-ì£¼ìš”-ê°œì„ ì‚¬í•­)
- [Continuous Processing ì™„ì „ ì •ë³µ](#continuous-processing-ì™„ì „-ì •ë³µ)
- [Flink vs Spark 4.1 ì‹¤ì „ ë¹„êµ](#flink-vs-spark-41-ì‹¤ì „-ë¹„êµ)
- [ì‹¤ë¬´ ì˜ˆì œ: ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-ì˜ˆì œ-ì‹¤ì‹œê°„-ì´ë²¤íŠ¸-ì²˜ë¦¬-ì‹œìŠ¤í…œ)
- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ì™€ ìµœì í™”](#ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬ì™€-ìµœì í™”)
- [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

---

## âš”ï¸ Spark vs Flink: ê·¼ë³¸ì  ì°¨ì´ì  {#spark-vs-flink-ê·¼ë³¸ì -ì°¨ì´ì }

### ì•„í‚¤í…ì²˜ ì² í•™ì˜ ì°¨ì´

#### **Spark: ë§ˆì´í¬ë¡œë°°ì¹˜ ë°©ì‹**

```python
# Spark 3.x ì´ì „: ë§ˆì´í¬ë¡œë°°ì¹˜ ë°©ì‹
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("SparkMicroBatch") \
    .getOrCreate()

# Structured Streaming (ë§ˆì´í¬ë¡œë°°ì¹˜)
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# ë°°ì¹˜ ê°„ê²©ë§ˆë‹¤ ì²˜ë¦¬ (ì˜ˆ: 1ì´ˆë§ˆë‹¤)
result = df \
    .withWatermark("timestamp", "10 seconds") \
    .groupBy(window("timestamp", "5 seconds"), "category") \
    .agg(count("*").alias("count"))

query = result.writeStream \
    .outputMode("update") \
    .trigger(processingTime="1 second")  # 1ì´ˆ ë°°ì¹˜ ê°„ê²©
    .format("console") \
    .start()

# ë¬¸ì œì : ìµœì†Œ ì§€ì—°ì‹œê°„ = ë°°ì¹˜ ê°„ê²© (1ì´ˆ)
# â†’ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì–´ë ¤ì›€
```

#### **Flink: ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹**

```python
# Flink: ì´ë²¤íŠ¸ ë„ì°© ì¦‰ì‹œ ì²˜ë¦¬
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(env)

# Kafka ì†ŒìŠ¤
table_env.execute_sql("""
    CREATE TABLE events (
        id STRING,
        category STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3),
        WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# ì´ë²¤íŠ¸ ë„ì°© ì¦‰ì‹œ ì²˜ë¦¬ (ë°°ì¹˜ ê°„ê²© ì—†ìŒ)
result = table_env.sql_query("""
    SELECT 
        TUMBLE_START(event_time, INTERVAL '5' SECOND) as window_start,
        category,
        COUNT(*) as count
    FROM events
    GROUP BY TUMBLE(event_time, INTERVAL '5' SECOND), category
""")

# ì¥ì : ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ë‚®ì€ ì§€ì—°ì‹œê°„
# â†’ ì§„ì •í•œ ì‹¤ì‹œê°„ ì²˜ë¦¬
```

### í•µì‹¬ ì°¨ì´ì  ë¹„êµ

| **íŠ¹ì§•** | **Spark (3.x ì´ì „)** | **Flink** | **Spark 4.1** |
|----------|---------------------|-----------|---------------|
| **ì²˜ë¦¬ ë°©ì‹** | ë§ˆì´í¬ë¡œë°°ì¹˜ | ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë° | Continuous Processing ì§€ì› |
| **ìµœì†Œ ì§€ì—°ì‹œê°„** | ë°°ì¹˜ ê°„ê²© (1ì´ˆ ì´ìƒ) | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ (CP ëª¨ë“œ) |
| **ì²˜ë¦¬ ë³´ì¥** | At-least-once / Exactly-once | Exactly-once | Exactly-once |
| **ìƒíƒœ ê´€ë¦¬** | ì œí•œì  | ê°•ë ¥í•œ ìƒíƒœ ê´€ë¦¬ | í–¥ìƒëœ ìƒíƒœ ê´€ë¦¬ |
| **ë°±í”„ë ˆì…” ì²˜ë¦¬** | ì œí•œì  | ìë™ ì²˜ë¦¬ | ê°œì„ ë¨ |
| **ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬** | ì œí•œì  | CEP ì§€ì› | í–¥ìƒë¨ |

---

## ğŸ“ˆ Spark ìŠ¤íŠ¸ë¦¬ë°ì˜ ì§„í™” ê³¼ì • {#spark-ìŠ¤íŠ¸ë¦¬ë°ì˜-ì§„í™”-ê³¼ì •}

### 1ë‹¨ê³„: Spark Streaming (DStream) - 2013ë…„

```python
# Spark Streaming (DStream) - ë ˆê±°ì‹œ API
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "DStreamExample")
ssc = StreamingContext(sc, 5)  # 5ì´ˆ ë°°ì¹˜ ê°„ê²©

# DStream ìƒì„±
lines = ssc.socketTextStream("localhost", 9999)

# ë°°ì¹˜ë³„ ì²˜ë¦¬
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda a, b: a + b)

word_counts.pprint()

ssc.start()
ssc.awaitTermination()

# íŠ¹ì§•:
# - RDD ê¸°ë°˜
# - ë§ˆì´í¬ë¡œë°°ì¹˜ë§Œ ì§€ì›
# - ìƒíƒœ ê´€ë¦¬ ì–´ë ¤ì›€
# - ì¥ì•  ë³µêµ¬ ë³µì¡
```

### 2ë‹¨ê³„: Structured Streaming - 2016ë…„

```python
# Structured Streaming - Spark 2.0+
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("StructuredStreaming") \
    .getOrCreate()

# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°í”„ë ˆì„
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# SQL-like ì—°ì‚°
result = df \
    .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
    .withColumn("timestamp", current_timestamp()) \
    .withWatermark("timestamp", "10 seconds") \
    .groupBy(window("timestamp", "5 seconds"), "category") \
    .agg(count("*").alias("count"))

# ì¶œë ¥
query = result.writeStream \
    .outputMode("update") \
    .format("console") \
    .trigger(processingTime="1 second") \
    .start()

# ê°œì„ ì‚¬í•­:
# - DataFrame/Dataset API
# - SQL ì§€ì›
# - ì›Œí„°ë§ˆí‚¹
# - Exactly-once ë³´ì¥
# - í•˜ì§€ë§Œ ì—¬ì „íˆ ë§ˆì´í¬ë¡œë°°ì¹˜
```

### 3ë‹¨ê³„: Continuous Processing - Spark 2.3+

```python
# Continuous Processing - Spark 2.3+ (ì‹¤í—˜ì )
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ContinuousProcessing") \
    .config("spark.sql.streaming.continuous.enabled", "true") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# Continuous Processing ëª¨ë“œ
query = df.writeStream \
    .format("kafka") \
    .option("topic", "output") \
    .trigger(continuous="1 second")  # Continuous ëª¨ë“œ
    .start()

# íŠ¹ì§•:
# - ë‚®ì€ ì§€ì—°ì‹œê°„ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
# - í•˜ì§€ë§Œ ì œí•œì ì¸ ì—°ì‚°ë§Œ ì§€ì›
# - í”„ë¡œë•ì…˜ ì‚¬ìš© ì–´ë ¤ì›€
```

### 4ë‹¨ê³„: Spark 4.1 - ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œì˜ ì§„í™”

```python
# Spark 4.1: í–¥ìƒëœ Continuous Processing
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("Spark41Streaming") \
    .config("spark.sql.streaming.continuous.enabled", "true")
    .config("spark.sql.streaming.continuous.checkpointInterval", "1s")
    .getOrCreate()

# Kafka ì†ŒìŠ¤
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "latest") \
    .load()

# ë³µì¡í•œ ì—°ì‚°ë„ Continuous Processing ì§€ì›
result = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withWatermark("event_time", "10 seconds") \
    .groupBy(window("event_time", "5 seconds"), "category") \
    .agg(
        count("*").alias("count"),
        sum("amount").alias("total_amount"),
        avg("amount").alias("avg_amount")
    )

# Continuous Processing ëª¨ë“œ
query = result.writeStream \
    .outputMode("update") \
    .format("kafka") \
    .option("topic", "results") \
    .option("checkpointLocation", "/checkpoint") \
    .trigger(continuous="100 milliseconds")  # 100ms ì§€ì—°ì‹œê°„
    .start()

# ì£¼ìš” ê°œì„ ì‚¬í•­:
# - ë” ë§ì€ ì—°ì‚° ì§€ì›
# - í–¥ìƒëœ ìƒíƒœ ê´€ë¦¬
# - ë‚®ì€ ì§€ì—°ì‹œê°„ (100ms ì´í•˜)
# - í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ
```

---

## ğŸ¯ Spark 4.1ì˜ ì£¼ìš” ê°œì„ ì‚¬í•­ {#spark-41ì˜-ì£¼ìš”-ê°œì„ ì‚¬í•­}

### 1. í–¥ìƒëœ Continuous Processing

#### **ì´ì „ ë²„ì „ì˜ í•œê³„**

```python
# Spark 3.x: Continuous Processing ì œí•œì‚¬í•­
# - ë‹¨ìˆœí•œ map/filterë§Œ ì§€ì›
# - ì§‘ê³„ ì—°ì‚° ë¶ˆê°€
# - ì¡°ì¸ ì—°ì‚° ë¶ˆê°€
# - ìƒíƒœ ê´€ë¦¬ ì œí•œì 

df = spark.readStream.format("kafka").load()

# âŒ ë¶ˆê°€ëŠ¥: ì§‘ê³„ ì—°ì‚°
# result = df.groupBy("category").agg(count("*"))
# â†’ Continuous Processingì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ

# âœ… ê°€ëŠ¥: ë‹¨ìˆœ ë³€í™˜ë§Œ
result = df.select("key", "value")
```

#### **Spark 4.1ì˜ ê°œì„ **

```python
# Spark 4.1: ë” ë§ì€ ì—°ì‚° ì§€ì›
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("Spark41CP") \
    .config("spark.sql.streaming.continuous.enabled", "true") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# âœ… ê°€ëŠ¥: ì§‘ê³„ ì—°ì‚° (Spark 4.1)
result = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withWatermark("event_time", "10 seconds") \
    .groupBy(window("event_time", "5 seconds"), "category") \
    .agg(
        count("*").alias("count"),
        sum("amount").alias("total"),
        avg("amount").alias("avg")
    )

# âœ… ê°€ëŠ¥: ì¡°ì¸ ì—°ì‚° (Spark 4.1)
static_df = spark.read.parquet("static_data.parquet")
joined = df.join(static_df, "id", "left")

# âœ… ê°€ëŠ¥: ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ (Spark 4.1)
from pyspark.sql.streaming import GroupState, GroupStateTimeout

def update_state(key, values, state: GroupState):
    if state.hasTimedOut:
        # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        return None
    
    current_sum = state.getOption.getOrElse(0)
    new_sum = current_sum + sum([v.amount for v in values])
    state.update(new_sum)
    state.setTimeoutDuration("1 minute")
    
    return {"key": key, "sum": new_sum}

result = df.groupByKey(lambda x: x.category).applyInPandasWithState(
    update_state,
    output_schema,
    state_schema,
    "append",
    GroupStateTimeout.ProcessingTimeTimeout
)
```

### 2. í–¥ìƒëœ ìƒíƒœ ê´€ë¦¬

```python
# Spark 4.1: í–¥ìƒëœ ìƒíƒœ ê´€ë¦¬
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.streaming import GroupState, GroupStateTimeout

spark = SparkSession.builder \
    .appName("StateManagement") \
    .config("spark.sql.streaming.stateStore.providerClass", 
            "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider")
    .getOrCreate()

# ë³µì¡í•œ ìƒíƒœ ê¸°ë°˜ ì—°ì‚°
def session_aggregation(key, values, state: GroupState):
    """ì„¸ì…˜ ê¸°ë°˜ ì§‘ê³„"""
    if state.hasTimedOut:
        # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
        session_data = state.getOption.getOrElse({
            "session_id": key,
            "events": [],
            "start_time": None,
            "end_time": None
        })
        return [session_data]
    
    current_session = state.getOption.getOrElse({
        "session_id": key,
        "events": [],
        "start_time": None,
        "end_time": None
    })
    
    # ì´ë²¤íŠ¸ ì¶”ê°€
    for value in values:
        current_session["events"].append(value)
        if current_session["start_time"] is None:
            current_session["start_time"] = value.timestamp
        current_session["end_time"] = value.timestamp
    
    state.update(current_session)
    state.setTimeoutDuration("30 minutes")  # 30ë¶„ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
    
    return []

# ìƒíƒœ ê¸°ë°˜ ì„¸ì…˜ ì§‘ê³„
result = df.groupByKey(lambda x: x.user_id).applyInPandasWithState(
    session_aggregation,
    output_schema,
    state_schema,
    "append",
    GroupStateTimeout.ProcessingTimeTimeout
)
```

### 3. í–¥ìƒëœ ë°±í”„ë ˆì…” ì²˜ë¦¬

```python
# Spark 4.1: ìë™ ë°±í”„ë ˆì…” ì²˜ë¦¬
spark = SparkSession.builder \
    .appName("Backpressure") \
    .config("spark.sql.streaming.maxRatePerPartition", "1000") \
    .config("spark.sql.streaming.backpressure.enabled", "true") \
    .config("spark.sql.streaming.backpressure.initialRate", "10000") \
    .getOrCreate()

# Kafka ì†ŒìŠ¤ (ìë™ ë°±í”„ë ˆì…” ì¡°ì •)
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("maxOffsetsPerTrigger", "10000") \
    .load()

# ì²˜ë¦¬ ì†ë„ê°€ ëŠë ¤ì§€ë©´ ìë™ìœ¼ë¡œ ì…ë ¥ ì†ë„ ì¡°ì ˆ
result = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .filter(col("amount") > 100)

query = result.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()
```

### 4. í–¥ìƒëœ ì²´í¬í¬ì¸íŒ…

```python
# Spark 4.1: ë¹ ë¥¸ ì²´í¬í¬ì¸íŒ…
spark = SparkSession.builder \
    .appName("Checkpointing") \
    .config("spark.sql.streaming.checkpointLocation", "/checkpoint") \
    .config("spark.sql.streaming.checkpoint.interval", "10s") \
    .config("spark.sql.streaming.stateStore.compression.codec", "lz4") \
    .getOrCreate()

# ì²´í¬í¬ì¸íŠ¸ ìµœì í™”
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

result = df \
    .withWatermark("event_time", "10 seconds") \
    .groupBy(window("event_time", "5 seconds"), "category") \
    .agg(count("*").alias("count"))

query = result.writeStream \
    .outputMode("update") \
    .format("console") \
    .option("checkpointLocation", "/checkpoint") \
    .trigger(continuous="100ms") \
    .start()

# ê°œì„ ì‚¬í•­:
# - ë” ë¹ ë¥¸ ì²´í¬í¬ì¸íŒ…
# - ì••ì¶• ì§€ì›
# - ì¦ë¶„ ì²´í¬í¬ì¸íŒ…
```

---

## ğŸ”„ Continuous Processing ì™„ì „ ì •ë³µ {#continuous-processing-ì™„ì „-ì •ë³µ}

### Continuous Processing vs Micro-batch

```python
# ë¹„êµ: Micro-batch vs Continuous Processing
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("Comparison") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# 1. Micro-batch ëª¨ë“œ (ê¸°ë³¸)
query_microbatch = df.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(processingTime="1 second")  # 1ì´ˆ ë°°ì¹˜ ê°„ê²©
    .start()

# íŠ¹ì§•:
# - ë°°ì¹˜ ê°„ê²©ë§ˆë‹¤ ì²˜ë¦¬
# - ìµœì†Œ ì§€ì—°ì‹œê°„ = ë°°ì¹˜ ê°„ê²© (1ì´ˆ)
# - ë†’ì€ ì²˜ë¦¬ëŸ‰
# - ëª¨ë“  ì—°ì‚° ì§€ì›

# 2. Continuous Processing ëª¨ë“œ (Spark 4.1)
query_continuous = df.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(continuous="100 milliseconds")  # 100ms ì§€ì—°ì‹œê°„
    .start()

# íŠ¹ì§•:
# - ì´ë²¤íŠ¸ ë„ì°© ì¦‰ì‹œ ì²˜ë¦¬
# - ë‚®ì€ ì§€ì—°ì‹œê°„ (100ms ì´í•˜)
# - Flink ìˆ˜ì¤€ì˜ ì„±ëŠ¥
# - ë” ë§ì€ ì—°ì‚° ì§€ì› (Spark 4.1)
```

### Continuous Processing ì„¤ì •

```python
# Spark 4.1: Continuous Processing ì™„ì „ ì„¤ì •
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ContinuousProcessing") \
    .config("spark.sql.streaming.continuous.enabled", "true")
    .config("spark.sql.streaming.continuous.checkpointInterval", "1s")
    .config("spark.sql.streaming.continuous.partitionInitializingInterval", "200ms")
    .config("spark.sql.streaming.continuous.maxAttempts", "3")
    .getOrCreate()

# Kafka ì†ŒìŠ¤
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "latest") \
    .option("maxOffsetsPerTrigger", "10000") \
    .load()

# ë³µì¡í•œ ìŠ¤íŠ¸ë¦¬ë° ì—°ì‚°
result = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withWatermark("event_time", "10 seconds") \
    .groupBy(window("event_time", "5 seconds"), "category") \
    .agg(
        count("*").alias("count"),
        sum("amount").alias("total"),
        avg("amount").alias("avg"),
        max("amount").alias("max"),
        min("amount").alias("min")
    )

# Continuous Processing ì¶œë ¥
query = result.writeStream \
    .outputMode("update") \
    .format("kafka") \
    .option("topic", "results") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("checkpointLocation", "/checkpoint") \
    .trigger(continuous="100 milliseconds") \
    .start()

query.awaitTermination()
```

---

## âš”ï¸ Flink vs Spark 4.1 ì‹¤ì „ ë¹„êµ {#flink-vs-spark-41-ì‹¤ì „-ë¹„êµ}

### ì§€ì—°ì‹œê°„ ë¹„êµ

```python
# ì‹¤ì „ ë¹„êµ: ì§€ì—°ì‹œê°„ ì¸¡ì •

# 1. Flink: ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë°
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
import time

env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(env)

# ì´ë²¤íŠ¸ ìƒì„± ì‹œê°„ ê¸°ë¡
table_env.execute_sql("""
    CREATE TABLE events (
        id STRING,
        timestamp BIGINT,
        event_time AS TO_TIMESTAMP(FROM_UNIXTIME(timestamp / 1000)),
        WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ ì¸¡ì •
start_time = time.time()

result = table_env.sql_query("""
    SELECT 
        id,
        event_time,
        CURRENT_TIMESTAMP as processing_time,
        TIMESTAMPDIFF(SECOND, event_time, CURRENT_TIMESTAMP) as latency
    FROM events
""")

# í‰ê·  ì§€ì—°ì‹œê°„: ~50-100ms

# 2. Spark 4.1: Continuous Processing
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("LatencyTest") \
    .config("spark.sql.streaming.continuous.enabled", "true") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

result = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select(
        col("data.id"),
        col("data.timestamp"),
        current_timestamp().alias("processing_time"),
        (unix_timestamp(current_timestamp()) * 1000 - col("data.timestamp")).alias("latency_ms")
    )

query = result.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(continuous="100 milliseconds") \
    .start()

# í‰ê·  ì§€ì—°ì‹œê°„: ~100-200ms (Spark 4.1)
```

### ì²˜ë¦¬ëŸ‰ ë¹„êµ

```python
# ì²˜ë¦¬ëŸ‰ ë²¤ì¹˜ë§ˆí¬

# Flink ì²˜ë¦¬ëŸ‰
# - ë‹¨ì¼ ë…¸ë“œ: ~1M events/sec
# - í´ëŸ¬ìŠ¤í„°: ~10M events/sec

# Spark 4.1 ì²˜ë¦¬ëŸ‰ (Continuous Processing)
# - ë‹¨ì¼ ë…¸ë“œ: ~800K events/sec
# - í´ëŸ¬ìŠ¤í„°: ~8M events/sec

# Spark 4.1 ì²˜ë¦¬ëŸ‰ (Micro-batch)
# - ë‹¨ì¼ ë…¸ë“œ: ~1.2M events/sec
# - í´ëŸ¬ìŠ¤í„°: ~12M events/sec

# ê²°ë¡ :
# - ì§€ì—°ì‹œê°„: Flink â‰ˆ Spark 4.1 CP < Spark Micro-batch
# - ì²˜ë¦¬ëŸ‰: Spark Micro-batch > Flink â‰ˆ Spark 4.1 CP
```

### ê¸°ëŠ¥ ë¹„êµí‘œ

| **ê¸°ëŠ¥** | **Flink** | **Spark 4.1 (CP)** | **Spark 4.1 (Micro-batch)** |
|----------|-----------|-------------------|---------------------------|
| **ìµœì†Œ ì§€ì—°ì‹œê°„** | 10-50ms | 100-200ms | 1ì´ˆ ì´ìƒ |
| **ì²˜ë¦¬ëŸ‰** | ë†’ìŒ | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |
| **CEP ì§€ì›** | âœ… ê°•ë ¥í•¨ | âš ï¸ ì œí•œì  | âŒ ì—†ìŒ |
| **ìƒíƒœ ê´€ë¦¬** | âœ… ë§¤ìš° ê°•ë ¥ | âœ… í–¥ìƒë¨ | âš ï¸ ì œí•œì  |
| **SQL ì§€ì›** | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› |
| **ë°°ì¹˜ í†µí•©** | âš ï¸ ë³„ë„ API | âœ… í†µí•© | âœ… í†µí•© |
| **í•™ìŠµ ê³¡ì„ ** | ê°€íŒŒë¦„ | ì™„ë§Œí•¨ | ì™„ë§Œí•¨ |
| **ìƒíƒœê³„** | ì¤‘ê°„ | ë§¤ìš° ë„“ìŒ | ë§¤ìš° ë„“ìŒ |

---

## ğŸ’¼ ì‹¤ë¬´ ì˜ˆì œ: ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ {#ì‹¤ë¬´-ì˜ˆì œ-ì‹¤ì‹œê°„-ì´ë²¤íŠ¸-ì²˜ë¦¬-ì‹œìŠ¤í…œ}

### ì‹œë‚˜ë¦¬ì˜¤: ì‹¤ì‹œê°„ ì£¼ë¬¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
# ì‹¤ì‹œê°„ ì£¼ë¬¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ - Spark 4.1
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("RealTimeOrderProcessing") \
    .config("spark.sql.streaming.continuous.enabled", "true") \
    .config("spark.sql.streaming.checkpointLocation", "/checkpoint/orders") \
    .getOrCreate()

# ì£¼ë¬¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
order_schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("product_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("order_time", TimestampType(), True),
    StructField("status", StringType(), True)
])

# Kafkaì—ì„œ ì£¼ë¬¸ ìŠ¤íŠ¸ë¦¼ ì½ê¸°
orders_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "orders") \
    .option("startingOffsets", "latest") \
    .load()

# JSON íŒŒì‹±
orders_parsed = orders_df \
    .select(from_json(col("value").cast("string"), order_schema).alias("data")) \
    .select("data.*") \
    .withWatermark("order_time", "1 minute")

# 1. ì‹¤ì‹œê°„ ë§¤ì¶œ ì§‘ê³„ (5ì´ˆ ìœˆë„ìš°)
revenue_by_window = orders_parsed \
    .filter(col("status") == "completed") \
    .groupBy(
        window("order_time", "5 seconds"),
        "product_id"
    ) \
    .agg(
        count("*").alias("order_count"),
        sum("amount").alias("total_revenue"),
        avg("amount").alias("avg_order_value"),
        sum("quantity").alias("total_quantity")
    ) \
    .select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        "product_id",
        "order_count",
        "total_revenue",
        "avg_order_value",
        "total_quantity"
    )

# 2. ì‹¤ì‹œê°„ ì‚¬ìš©ìë³„ ì§‘ê³„
user_stats = orders_parsed \
    .withWatermark("order_time", "10 minutes") \
    .groupBy(
        window("order_time", "1 minute"),
        "user_id"
    ) \
    .agg(
        count("*").alias("user_order_count"),
        sum("amount").alias("user_total_spent"),
        collect_list("product_id").alias("purchased_products")
    )

# 3. ì´ìƒ ê±°ë˜ íƒì§€ (ì‹¤ì‹œê°„)
anomaly_detection = orders_parsed \
    .withWatermark("order_time", "5 minutes") \
    .groupBy("user_id") \
    .agg(
        count("*").alias("recent_orders"),
        sum("amount").alias("recent_spending"),
        avg("amount").alias("avg_order_value")
    ) \
    .filter(
        (col("recent_orders") > 10) |  # 5ë¶„ ë‚´ 10ê±´ ì´ìƒ
        (col("recent_spending") > 10000)  # 5ë¶„ ë‚´ 1ë§Œì› ì´ìƒ
    )

# ì¶œë ¥ 1: ë§¤ì¶œ ì§‘ê³„ë¥¼ Kafkaë¡œ
revenue_query = revenue_by_window.writeStream \
    .outputMode("update") \
    .format("kafka") \
    .option("topic", "revenue_aggregates") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("checkpointLocation", "/checkpoint/revenue") \
    .trigger(continuous="100 milliseconds") \
    .start()

# ì¶œë ¥ 2: ì‚¬ìš©ì í†µê³„ë¥¼ ì½˜ì†”ë¡œ
user_query = user_stats.writeStream \
    .outputMode("update") \
    .format("console") \
    .option("truncate", "false") \
    .trigger(continuous="1 second") \
    .start()

# ì¶œë ¥ 3: ì´ìƒ ê±°ë˜ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ
anomaly_query = anomaly_detection.writeStream \
    .outputMode("update") \
    .foreachBatch(lambda df, epoch_id: save_to_database(df, "anomalies")) \
    .trigger(continuous="500 milliseconds") \
    .start()

# ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰
spark.streams.awaitAnyTermination()
```

### ìƒíƒœ ê¸°ë°˜ ì„¸ì…˜ ì¶”ì 

```python
# ìƒíƒœ ê¸°ë°˜ ì‚¬ìš©ì ì„¸ì…˜ ì¶”ì 
from pyspark.sql.streaming import GroupState, GroupStateTimeout

# ì„¸ì…˜ ìƒíƒœ ìŠ¤í‚¤ë§ˆ
session_state_schema = StructType([
    StructField("user_id", StringType(), True),
    StructField("session_start", TimestampType(), True),
    StructField("session_end", TimestampType(), True),
    StructField("page_views", IntegerType(), True),
    StructField("total_time", LongType(), True),
    StructField("events", ArrayType(StringType()), True)
])

# ì„¸ì…˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_session(key, values, state: GroupState):
    """ì‚¬ìš©ì ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    
    # íƒ€ì„ì•„ì›ƒëœ ì„¸ì…˜ ì²˜ë¦¬
    if state.hasTimedOut:
        session = state.getOption.getOrElse({
            "user_id": key,
            "session_start": None,
            "session_end": None,
            "page_views": 0,
            "total_time": 0,
            "events": []
        })
        return [session]
    
    # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    current_session = state.getOption.getOrElse({
        "user_id": key,
        "session_start": None,
        "session_end": None,
        "page_views": 0,
        "total_time": 0,
        "events": []
    })
    
    # ì´ë²¤íŠ¸ ì²˜ë¦¬
    for value in values:
        if current_session["session_start"] is None:
            current_session["session_start"] = value.timestamp
        
        current_session["session_end"] = value.timestamp
        current_session["page_views"] += 1
        current_session["events"].append(value.event_type)
        
        if len(current_session["events"]) > 1:
            time_diff = (value.timestamp - current_session["session_start"]).total_seconds()
            current_session["total_time"] = int(time_diff)
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state.update(current_session)
    state.setTimeoutDuration("30 minutes")  # 30ë¶„ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
    
    return []

# ì„¸ì…˜ ì¶”ì  ìŠ¤íŠ¸ë¦¼
events_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user_events") \
    .load()

sessions = events_df \
    .select(from_json(col("value").cast("string"), event_schema).alias("data")) \
    .select("data.*") \
    .groupByKey(lambda x: x.user_id) \
    .applyInPandasWithState(
        update_session,
        output_schema=session_state_schema,
        state_schema=session_state_schema,
        output_mode="append",
        state_timeout=GroupStateTimeout.ProcessingTimeTimeout
    )

# ì„¸ì…˜ ê²°ê³¼ ì¶œë ¥
session_query = sessions.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(continuous="1 second") \
    .start()
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ì™€ ìµœì í™” {#ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬ì™€-ìµœì í™”}

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

```python
# ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬

# í…ŒìŠ¤íŠ¸ í™˜ê²½:
# - ë°ì´í„°: 10M events/sec
# - í´ëŸ¬ìŠ¤í„°: 10 nodes, ê° 32 cores, 128GB RAM
# - Kafka: 10 partitions

# ê²°ê³¼:

# 1. ì§€ì—°ì‹œê°„ (P99)
# Flink: 50ms
# Spark 4.1 CP: 150ms
# Spark 4.1 Micro-batch: 2000ms

# 2. ì²˜ë¦¬ëŸ‰
# Flink: 8M events/sec
# Spark 4.1 CP: 7M events/sec
# Spark 4.1 Micro-batch: 12M events/sec

# 3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
# Flink: CPU 60%, Memory 70%
# Spark 4.1 CP: CPU 65%, Memory 75%
# Spark 4.1 Micro-batch: CPU 50%, Memory 60%
```

### ìµœì í™” ì „ëµ

```python
# Spark 4.1 ìµœì í™” ì„¤ì •
spark = SparkSession.builder \
    .appName("OptimizedStreaming") \
    .config("spark.sql.streaming.continuous.enabled", "true")
    # ì²´í¬í¬ì¸íŒ… ìµœì í™”
    .config("spark.sql.streaming.checkpoint.interval", "10s")
    .config("spark.sql.streaming.stateStore.compression.codec", "lz4")
    # ìƒíƒœ ì €ì¥ì†Œ ìµœì í™”
    .config("spark.sql.streaming.stateStore.providerClass", 
            "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider")
    .config("spark.sql.streaming.stateStore.rocksdb.compression", "snappy")
    # ë°±í”„ë ˆì…” ìµœì í™”
    .config("spark.sql.streaming.backpressure.enabled", "true")
    .config("spark.sql.streaming.backpressure.initialRate", "10000")
    # ë©”ëª¨ë¦¬ ìµœì í™”
    .config("spark.sql.streaming.stateStore.maxMemorySize", "512m")
    .config("spark.sql.shuffle.partitions", "200")
    # ì¹´í”„ì¹´ ìµœì í™”
    .config("spark.sql.streaming.kafka.maxOffsetsPerTrigger", "10000")
    .getOrCreate()

# íŒŒí‹°ì…”ë‹ ìµœì í™”
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("maxOffsetsPerTrigger", "10000") \
    .load()

# íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
result = df \
    .repartition(200) \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withWatermark("event_time", "10 seconds") \
    .groupBy(window("event_time", "5 seconds"), "category") \
    .agg(count("*").alias("count"))

query = result.writeStream \
    .outputMode("update") \
    .format("kafka") \
    .option("topic", "results") \
    .option("checkpointLocation", "/checkpoint") \
    .trigger(continuous="100 milliseconds") \
    .start()
```

---

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### í•µì‹¬ í¬ì¸íŠ¸

1. **Sparkì˜ ì§„í™”**
   - Spark Streaming (DStream) â†’ Structured Streaming â†’ Continuous Processing
   - ë§ˆì´í¬ë¡œë°°ì¹˜ì—ì„œ ì§„ì •í•œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œì˜ ì§„í™”

2. **Spark 4.1ì˜ ì£¼ìš” ê°œì„ ì‚¬í•­**
   - í–¥ìƒëœ Continuous Processing
   - ë” ë§ì€ ì—°ì‚° ì§€ì› (ì§‘ê³„, ì¡°ì¸ ë“±)
   - í–¥ìƒëœ ìƒíƒœ ê´€ë¦¬
   - ë‚®ì€ ì§€ì—°ì‹œê°„ (100ms ì´í•˜)

3. **Flink vs Spark 4.1**
   - ì§€ì—°ì‹œê°„: Flink (50ms) < Spark 4.1 CP (150ms) < Spark Micro-batch (2000ms)
   - ì²˜ë¦¬ëŸ‰: Spark Micro-batch > Flink â‰ˆ Spark 4.1 CP
   - ê¸°ëŠ¥: Flinkê°€ CEP ë“± ê³ ê¸‰ ê¸°ëŠ¥ì—ì„œ ìš°ìœ„

### ì„ íƒ ê°€ì´ë“œ

| **ìš”êµ¬ì‚¬í•­** | **ì¶”ì²œ** |
|-------------|----------|
| **ìµœì†Œ ì§€ì—°ì‹œê°„ (< 50ms)** | Flink |
| **ë†’ì€ ì²˜ë¦¬ëŸ‰** | Spark Micro-batch |
| **ê· í˜•ì¡íŒ ì„±ëŠ¥** | Spark 4.1 CP |
| **ê¸°ì¡´ Spark ìƒíƒœê³„ í™œìš©** | Spark 4.1 |
| **CEP, ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬** | Flink |
| **ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¬ë° í†µí•©** | Spark 4.1 |

### ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì§€ì—°ì‹œê°„ ìš”êµ¬ì‚¬í•­ í™•ì¸ (< 100msë©´ Spark 4.1 CP ê³ ë ¤)
- [ ] ì²˜ë¦¬ëŸ‰ ìš”êµ¬ì‚¬í•­ í™•ì¸
- [ ] ìƒíƒœ ê´€ë¦¬ ë³µì¡ë„ í‰ê°€
- [ ] ê¸°ì¡´ Spark ì¸í”„ë¼ í™œìš© ê°€ëŠ¥ ì—¬ë¶€
- [ ] CEP í•„ìš” ì—¬ë¶€ í™•ì¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰

### ë‹¤ìŒ ë‹¨ê³„

- **ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬**: ë³µì¡í•œ ìƒíƒœ ê¸°ë°˜ ì—°ì‚°
- **ì„±ëŠ¥ íŠœë‹**: íŒŒí‹°ì…”ë‹, ë©”ëª¨ë¦¬ ìµœì í™”
- **ëª¨ë‹ˆí„°ë§**: ì§€ì—°ì‹œê°„, ì²˜ë¦¬ëŸ‰ ëª¨ë‹ˆí„°ë§
- **ì¥ì•  ë³µêµ¬**: ì²´í¬í¬ì¸íŒ… ì „ëµ

---

> **"Spark 4.1ì€ ë§ˆì´í¬ë¡œë°°ì¹˜ì˜ í•œê³„ë¥¼ ë„˜ì–´ Flink ìˆ˜ì¤€ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."**

Spark 4.1ì˜ Continuous Processingì€ ê¸°ì¡´ Spark ìƒíƒœê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ Flink ìˆ˜ì¤€ì˜ ë‚®ì€ ì§€ì—°ì‹œê°„ì„ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ Flinkì™€ Spark 4.1 ì¤‘ ì ì ˆí•œ ì„ íƒì„ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œê°€ ì˜¬ë°”ë¥¸ ì„ íƒì— ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤!

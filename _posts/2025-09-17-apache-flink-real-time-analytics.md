---
layout: post
lang: ko
title: "Part 3: Apache Flink ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEP - ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ì˜ ì™„ì„±"
description: "Apache Flinkì˜ CEP (Complex Event Processing), ì‹¤ì‹œê°„ ì§‘ê³„, ìœˆë„ìš° í•¨ìˆ˜, íŒ¨í„´ ë§¤ì¹­ì„ í•™ìŠµí•˜ê³  ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œì™€ ì•Œë¦¼ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
date: 2025-09-17
author: Data Droid
category: data-engineering
tags: [Apache-Flink, CEP, ì‹¤ì‹œê°„ë¶„ì„, íŒ¨í„´ë§¤ì¹­, ìœˆë„ìš°í•¨ìˆ˜, ì‹¤ì‹œê°„ëŒ€ì‹œë³´ë“œ, Python, PyFlink]
series: apache-flink-complete-guide
series_order: 3
reading_time: "45ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 3: Apache Flink ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEP - ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ì˜ ì™„ì„±

> Apache Flinkì˜ CEP (Complex Event Processing), ì‹¤ì‹œê°„ ì§‘ê³„, ìœˆë„ìš° í•¨ìˆ˜, íŒ¨í„´ ë§¤ì¹­ì„ í•™ìŠµí•˜ê³  ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œì™€ ì•Œë¦¼ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [CEP (Complex Event Processing) ê¸°ì´ˆ](#cep-complex-event-processing-ê¸°ì´ˆ)
2. [ê³ ê¸‰ íŒ¨í„´ ë§¤ì¹­](#ê³ ê¸‰-íŒ¨í„´-ë§¤ì¹­)
3. [ì‹¤ì‹œê°„ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜](#ì‹¤ì‹œê°„-ì§‘ê³„ì™€-ìœˆë„ìš°-í•¨ìˆ˜)
4. [ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•](#ì‹¤ì‹œê°„-ëŒ€ì‹œë³´ë“œ-êµ¬ì¶•)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ê¸ˆìœµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ê¸ˆìœµ-ëª¨ë‹ˆí„°ë§-ì‹œìŠ¤í…œ)
6. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ¯ CEP (Complex Event Processing) ê¸°ì´ˆ

### CEPë€ ë¬´ì—‡ì¸ê°€?

CEPëŠ” **ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬**ë¥¼ ì˜ë¯¸í•˜ë©°, ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì—¬ëŸ¬ ì´ë²¤íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì´ë‚˜ ìƒí™©ì„ ê°ì§€í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

#### **CEPì˜ í•µì‹¬ ê°œë…**

1. **ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼**: ì—°ì†ì ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°
2. **íŒ¨í„´ ë§¤ì¹­**: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤
3. **ì‹œê°„ ìœˆë„ìš°**: ì´ë²¤íŠ¸ ë¶„ì„ ì‹œê°„ ë²”ìœ„
4. **ìƒíƒœ ê´€ë¦¬**: íŒ¨í„´ ë§¤ì¹­ì„ ìœ„í•œ ìƒíƒœ ì €ì¥

### Flink CEP ê¸°ë³¸ êµ¬ì¡°

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.cep import CEP, Pattern, PatternStream
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.typeinfo import Types
from pyflink.common.time import Time

class CEPBasics:
    def __init__(self, env):
        self.env = env
    
    def setup_basic_cep_pattern(self, data_stream):
        """ê¸°ë³¸ CEP íŒ¨í„´ ì„¤ì •"""
        # 1. íŒ¨í„´ ì •ì˜
        pattern = Pattern.begin("start") \
            .where(lambda value: value.action == "login") \
            .followed_by("middle") \
            .where(lambda value: value.action == "view") \
            .followed_by("end") \
            .where(lambda value: value.action == "purchase") \
            .within(Time.minutes(10))  # 10ë¶„ ë‚´ íŒ¨í„´ ë§¤ì¹­
        
        # 2. íŒ¨í„´ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        pattern_stream = CEP.pattern(
            data_stream.key_by(lambda event: event.user_id),
            pattern
        )
        
        # 3. íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼ ì²˜ë¦¬
        result_stream = pattern_stream.select(
            lambda pattern: self.process_pattern(pattern)
        )
        
        return result_stream
    
    def process_pattern(self, pattern):
        """íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼ ì²˜ë¦¬"""
        start_event = pattern["start"][0]
        middle_event = pattern["middle"][0]
        end_event = pattern["end"][0]
        
        return {
            "user_id": start_event.user_id,
            "pattern_type": "login_to_purchase",
            "duration": end_event.timestamp - start_event.timestamp,
            "timestamp": end_event.timestamp
        }
```

### ê³ ê¸‰ íŒ¨í„´ ì •ì˜

```python
class AdvancedPatternDefinitions:
    def __init__(self):
        pass
    
    def create_sequence_pattern(self):
        """ìˆœì°¨ íŒ¨í„´ ì •ì˜"""
        pattern = Pattern.begin("first") \
            .where(lambda e: e.action == "login") \
            .followed_by("second") \
            .where(lambda e: e.action == "browse") \
            .followed_by("third") \
            .where(lambda e: e.action == "add_to_cart") \
            .followed_by("fourth") \
            .where(lambda e: e.action == "checkout") \
            .within(Time.hours(1))
        
        return pattern
    
    def create_or_pattern(self):
        """OR íŒ¨í„´ ì •ì˜"""
        pattern = Pattern.begin("start") \
            .where(lambda e: e.action == "login") \
            .followed_by("action") \
            .where(lambda e: e.action == "view" or e.action == "click") \
            .followed_by("end") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.minutes(30))
        
        return pattern
    
    def create_not_pattern(self):
        """NOT íŒ¨í„´ ì •ì˜ (íŠ¹ì • ì´ë²¤íŠ¸ê°€ ì—†ì–´ì•¼ í•¨)"""
        pattern = Pattern.begin("start") \
            .where(lambda e: e.action == "login") \
            .followed_by("middle") \
            .where(lambda e: e.action == "view") \
            .not_followed_by("blocked") \
            .where(lambda e: e.action == "logout") \
            .followed_by("end") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.minutes(15))
        
        return pattern
    
    def create_times_pattern(self):
        """ë°˜ë³µ íŒ¨í„´ ì •ì˜"""
        pattern = Pattern.begin("repeated") \
            .where(lambda e: e.action == "click") \
            .times(3)  # ì •í™•íˆ 3ë²ˆ ë°˜ë³µ
            .followed_by("end") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.minutes(5))
        
        return pattern
    
    def create_greedy_pattern(self):
        """íƒìš•ì  ë§¤ì¹­ íŒ¨í„´"""
        pattern = Pattern.begin("greedy") \
            .where(lambda e: e.action == "view") \
            .one_or_more() \
            .greedy()  # ê°€ëŠ¥í•œ í•œ ë§ì´ ë§¤ì¹­
            .followed_by("end") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.minutes(10))
        
        return pattern
```

## ğŸ” ê³ ê¸‰ íŒ¨í„´ ë§¤ì¹­

### ì¡°ê±´ë¶€ íŒ¨í„´ ë§¤ì¹­

```python
from pyflink.datastream.cep import IterativeCondition, SimpleCondition

class ConditionalPatternMatching:
    def __init__(self):
        pass
    
    def create_conditional_pattern(self):
        """ì¡°ê±´ë¶€ íŒ¨í„´ ì •ì˜"""
        # ë³µì¡í•œ ì¡°ê±´ì´ ìˆëŠ” íŒ¨í„´
        pattern = Pattern.begin("start") \
            .where(self.UserLoginCondition()) \
            .followed_by("middle") \
            .where(self.PriceRangeCondition(100, 1000)) \
            .followed_by("end") \
            .where(self.PurchaseCondition()) \
            .within(Time.minutes(20))
        
        return pattern
    
    class UserLoginCondition(SimpleCondition):
        """ì‚¬ìš©ì ë¡œê·¸ì¸ ì¡°ê±´"""
        def filter(self, value):
            return value.action == "login" and value.user_id.startswith("premium_")
    
    class PriceRangeCondition(SimpleCondition):
        """ê°€ê²© ë²”ìœ„ ì¡°ê±´"""
        def __init__(self, min_price, max_price):
            self.min_price = min_price
            self.max_price = max_price
        
        def filter(self, value):
            price = value.metadata.get("price", 0)
            return self.min_price <= price <= self.max_price
    
    class PurchaseCondition(SimpleCondition):
        """êµ¬ë§¤ ì¡°ê±´"""
        def filter(self, value):
            return value.action == "purchase" and value.metadata.get("amount", 0) > 0
```

### ë™ì  íŒ¨í„´ ë§¤ì¹­

```python
class DynamicPatternMatching:
    def __init__(self):
        self.pattern_cache = {}
    
    def create_dynamic_pattern(self, user_id, pattern_type):
        """ë™ì  íŒ¨í„´ ìƒì„±"""
        cache_key = f"{user_id}_{pattern_type}"
        
        if cache_key not in self.pattern_cache:
            if pattern_type == "high_value":
                pattern = self.create_high_value_pattern()
            elif pattern_type == "frequent":
                pattern = self.create_frequent_pattern()
            else:
                pattern = self.create_default_pattern()
            
            self.pattern_cache[cache_key] = pattern
        
        return self.pattern_cache[cache_key]
    
    def create_high_value_pattern(self):
        """ê³ ê°€ì¹˜ êµ¬ë§¤ íŒ¨í„´"""
        return Pattern.begin("login") \
            .where(lambda e: e.action == "login") \
            .followed_by("browse") \
            .where(lambda e: e.action == "view") \
            .followed_by("purchase") \
            .where(lambda e: e.action == "purchase" and 
                          e.metadata.get("amount", 0) > 1000) \
            .within(Time.minutes(30))
    
    def create_frequent_pattern(self):
        """ë¹ˆë²ˆí•œ êµ¬ë§¤ íŒ¨í„´"""
        return Pattern.begin("first_purchase") \
            .where(lambda e: e.action == "purchase") \
            .followed_by("second_purchase") \
            .where(lambda e: e.action == "purchase") \
            .followed_by("third_purchase") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.hours(2))
    
    def create_default_pattern(self):
        """ê¸°ë³¸ íŒ¨í„´"""
        return Pattern.begin("login") \
            .where(lambda e: e.action == "login") \
            .followed_by("action") \
            .where(lambda e: e.action in ["view", "click"]) \
            .followed_by("purchase") \
            .where(lambda e: e.action == "purchase") \
            .within(Time.minutes(60))
```

### íŒ¨í„´ ê²°ê³¼ í›„ì²˜ë¦¬

```python
class PatternPostProcessing:
    def __init__(self):
        pass
    
    def process_pattern_result(self, pattern):
        """íŒ¨í„´ ê²°ê³¼ í›„ì²˜ë¦¬"""
        # íŒ¨í„´ì—ì„œ ì´ë²¤íŠ¸ ì¶”ì¶œ
        events = self.extract_events(pattern)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©
        result = self.apply_business_logic(events)
        
        # ì•Œë¦¼ ìƒì„±
        alert = self.generate_alert(result)
        
        return {
            "pattern_result": result,
            "alert": alert,
            "timestamp": int(time.time())
        }
    
    def extract_events(self, pattern):
        """íŒ¨í„´ì—ì„œ ì´ë²¤íŠ¸ ì¶”ì¶œ"""
        events = {}
        for name, event_list in pattern.items():
            if event_list:
                events[name] = event_list[0]  # ì²« ë²ˆì§¸ ì´ë²¤íŠ¸
        return events
    
    def apply_business_logic(self, events):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©"""
        # ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„
        session_duration = self.calculate_session_duration(events)
        
        # êµ¬ë§¤ í–‰ë™ ë¶„ì„
        purchase_behavior = self.analyze_purchase_behavior(events)
        
        # ìœ„í—˜ë„ í‰ê°€
        risk_score = self.calculate_risk_score(events)
        
        return {
            "session_duration": session_duration,
            "purchase_behavior": purchase_behavior,
            "risk_score": risk_score
        }
    
    def generate_alert(self, result):
        """ì•Œë¦¼ ìƒì„±"""
        alerts = []
        
        # ìœ„í—˜ë„ ê¸°ë°˜ ì•Œë¦¼
        if result["risk_score"] > 0.8:
            alerts.append({
                "type": "high_risk",
                "message": "High risk transaction detected",
                "severity": "critical"
            })
        
        # êµ¬ë§¤ íŒ¨í„´ ê¸°ë°˜ ì•Œë¦¼
        if result["purchase_behavior"]["is_abnormal"]:
            alerts.append({
                "type": "abnormal_purchase",
                "message": "Abnormal purchase pattern detected",
                "severity": "warning"
            })
        
        return alerts
    
    def calculate_session_duration(self, events):
        """ì„¸ì…˜ ì§€ì† ì‹œê°„ ê³„ì‚°"""
        if "login" in events and "purchase" in events:
            return events["purchase"].timestamp - events["login"].timestamp
        return 0
    
    def analyze_purchase_behavior(self, events):
        """êµ¬ë§¤ í–‰ë™ ë¶„ì„"""
        if "purchase" in events:
            purchase = events["purchase"]
            amount = purchase.metadata.get("amount", 0)
            
            # ì •ìƒì ì¸ êµ¬ë§¤ íŒ¨í„´ì¸ì§€ í™•ì¸
            is_abnormal = amount > 5000 or amount < 10  # ì˜ˆì‹œ ì¡°ê±´
            
            return {
                "amount": amount,
                "is_abnormal": is_abnormal,
                "category": purchase.metadata.get("category", "unknown")
            }
        
        return {"is_abnormal": False}
    
    def calculate_risk_score(self, events):
        """ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
        risk_factors = []
        
        # ë¹ ë¥¸ êµ¬ë§¤ (ë¡œê·¸ì¸ í›„ 5ë¶„ ì´ë‚´)
        if self.is_quick_purchase(events):
            risk_factors.append(0.3)
        
        # ë†’ì€ ê¸ˆì•¡
        if self.is_high_amount(events):
            risk_factors.append(0.4)
        
        # ë¹„ì •ìƒì ì¸ ì‹œê°„ëŒ€
        if self.is_unusual_time(events):
            risk_factors.append(0.2)
        
        return sum(risk_factors)
    
    def is_quick_purchase(self, events):
        """ë¹ ë¥¸ êµ¬ë§¤ ì—¬ë¶€ í™•ì¸"""
        if "login" in events and "purchase" in events:
            duration = events["purchase"].timestamp - events["login"].timestamp
            return duration < 300  # 5ë¶„ ì´ë‚´
        return False
    
    def is_high_amount(self, events):
        """ë†’ì€ ê¸ˆì•¡ ì—¬ë¶€ í™•ì¸"""
        if "purchase" in events:
            amount = events["purchase"].metadata.get("amount", 0)
            return amount > 2000
        return False
    
    def is_unusual_time(self, events):
        """ë¹„ì •ìƒì ì¸ ì‹œê°„ëŒ€ ì—¬ë¶€ í™•ì¸"""
        if "purchase" in events:
            purchase_time = datetime.fromtimestamp(events["purchase"].timestamp)
            hour = purchase_time.hour
            return hour < 6 or hour > 23  # ìƒˆë²½ ì‹œê°„ëŒ€
        return False
```

## ğŸ“Š ì‹¤ì‹œê°„ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜

### ê³ ê¸‰ ìœˆë„ìš° ì—°ì‚°

```python
from pyflink.datastream.window import TumblingEventTimeWindows, SlidingEventTimeWindows, SessionEventTimeWindows
from pyflink.datastream.functions import WindowFunction, AllWindowFunction

class AdvancedWindowOperations:
    def __init__(self):
        pass
    
    def setup_advanced_windows(self, data_stream):
        """ê³ ê¸‰ ìœˆë„ìš° ì„¤ì •"""
        # 1. íŠ¬ë¸”ë§ ìœˆë„ìš° (ê³ ì • í¬ê¸°)
        tumbling_window = data_stream.key_by(lambda x: x.user_id) \
            .window(TumblingEventTimeWindows.of(Time.minutes(5))) \
            .apply(self.AdvancedWindowFunction(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        # 2. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (ê²¹ì¹˜ëŠ” ìœˆë„ìš°)
        sliding_window = data_stream.key_by(lambda x: x.user_id) \
            .window(SlidingEventTimeWindows.of(Time.minutes(10), Time.minutes(2))) \
            .apply(self.AdvancedWindowFunction(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        # 3. ì„¸ì…˜ ìœˆë„ìš° (í™œë™ ê¸°ë°˜)
        session_window = data_stream.key_by(lambda x: x.user_id) \
            .window(SessionEventTimeWindows.with_gap(Time.minutes(5))) \
            .apply(self.AdvancedWindowFunction(), output_type=Types.PICKLED_BYTE_ARRAY())
        
        return {
            "tumbling": tumbling_window,
            "sliding": sliding_window,
            "session": session_window
        }
    
    class AdvancedWindowFunction(WindowFunction):
        """ê³ ê¸‰ ìœˆë„ìš° í•¨ìˆ˜"""
        def apply(self, key, window, inputs, out):
            # ìœˆë„ìš° ë‚´ ë°ì´í„° ë¶„ì„
            analysis_result = self.analyze_window_data(inputs)
            
            # ê²°ê³¼ ìƒì„±
            result = {
                "user_id": key,
                "window_start": window.start,
                "window_end": window.end,
                "event_count": len(inputs),
                "analysis": analysis_result,
                "timestamp": window.end
            }
            
            out.collect(result)
        
        def analyze_window_data(self, inputs):
            """ìœˆë„ìš° ë‚´ ë°ì´í„° ë¶„ì„"""
            if not inputs:
                return {"error": "No data in window"}
            
            # ê¸°ë³¸ í†µê³„
            actions = [event.action for event in inputs]
            action_counts = {}
            for action in actions:
                action_counts[action] = action_counts.get(action, 0) + 1
            
            # ì‹œê°„ ë¶„ì„
            timestamps = [event.timestamp for event in inputs]
            duration = max(timestamps) - min(timestamps)
            
            # ê¸ˆì•¡ ë¶„ì„ (êµ¬ë§¤ ì´ë²¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
            amounts = [event.metadata.get("amount", 0) for event in inputs 
                      if event.action == "purchase"]
            total_amount = sum(amounts)
            avg_amount = total_amount / len(amounts) if amounts else 0
            
            return {
                "action_distribution": action_counts,
                "session_duration": duration,
                "total_purchase_amount": total_amount,
                "average_purchase_amount": avg_amount,
                "purchase_count": len(amounts)
            }
```

### ì‹¤ì‹œê°„ ì§‘ê³„ ì—”ì§„

```python
class RealTimeAggregationEngine:
    def __init__(self):
        self.aggregation_state = {}
    
    def setup_real_time_aggregations(self, data_stream):
        """ì‹¤ì‹œê°„ ì§‘ê³„ ì„¤ì •"""
        # 1. ì‚¬ìš©ìë³„ ì‹¤ì‹œê°„ ì§‘ê³„
        user_aggregations = data_stream.key_by(lambda x: x.user_id) \
            .window(TumblingEventTimeWindows.of(Time.minutes(1))) \
            .aggregate(
                self.UserAggregateFunction(),
                self.UserWindowFunction(),
                output_type=Types.PICKLED_BYTE_ARRAY()
            )
        
        # 2. ê¸€ë¡œë²Œ ì‹¤ì‹œê°„ ì§‘ê³„
        global_aggregations = data_stream.window_all(
            TumblingEventTimeWindows.of(Time.minutes(1))
        ).aggregate(
            self.GlobalAggregateFunction(),
            self.GlobalWindowFunction(),
            output_type=Types.PICKLED_BYTE_ARRAY()
        )
        
        return {
            "user_aggregations": user_aggregations,
            "global_aggregations": global_aggregations
        }
    
    class UserAggregateFunction:
        """ì‚¬ìš©ì ì§‘ê³„ í•¨ìˆ˜"""
        def create_accumulator(self):
            return {
                "event_count": 0,
                "total_amount": 0.0,
                "action_counts": {},
                "first_timestamp": None,
                "last_timestamp": None
            }
        
        def add(self, accumulator, value):
            accumulator["event_count"] += 1
            
            if value.action == "purchase":
                amount = value.metadata.get("amount", 0)
                accumulator["total_amount"] += amount
            
            accumulator["action_counts"][value.action] = \
                accumulator["action_counts"].get(value.action, 0) + 1
            
            if accumulator["first_timestamp"] is None:
                accumulator["first_timestamp"] = value.timestamp
            
            accumulator["last_timestamp"] = value.timestamp
            
            return accumulator
        
        def get_result(self, accumulator):
            return accumulator
    
    class UserWindowFunction(WindowFunction):
        """ì‚¬ìš©ì ìœˆë„ìš° í•¨ìˆ˜"""
        def apply(self, key, window, input, out):
            result = input[0] if input else self.create_empty_result()
            
            # ìœˆë„ìš° ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.update({
                "user_id": key,
                "window_start": window.start,
                "window_end": window.end,
                "window_duration": window.end - window.start
            })
            
            # íŒŒìƒ ì§€í‘œ ê³„ì‚°
            result["events_per_minute"] = result["event_count"] / max(
                (result["last_timestamp"] - result["first_timestamp"]) / 60, 1
            )
            
            result["average_purchase_amount"] = (
                result["total_amount"] / result["action_counts"].get("purchase", 1)
                if result["action_counts"].get("purchase", 0) > 0 else 0
            )
            
            out.collect(result)
        
        def create_empty_result(self):
            return {
                "event_count": 0,
                "total_amount": 0.0,
                "action_counts": {},
                "first_timestamp": None,
                "last_timestamp": None,
                "events_per_minute": 0.0,
                "average_purchase_amount": 0.0
            }
    
    class GlobalAggregateFunction:
        """ê¸€ë¡œë²Œ ì§‘ê³„ í•¨ìˆ˜"""
        def create_accumulator(self):
            return {
                "total_events": 0,
                "unique_users": set(),
                "total_revenue": 0.0,
                "action_distribution": {},
                "timestamp_range": [None, None]
            }
        
        def add(self, accumulator, value):
            accumulator["total_events"] += 1
            accumulator["unique_users"].add(value.user_id)
            
            if value.action == "purchase":
                amount = value.metadata.get("amount", 0)
                accumulator["total_revenue"] += amount
            
            accumulator["action_distribution"][value.action] = \
                accumulator["action_distribution"].get(value.action, 0) + 1
            
            if accumulator["timestamp_range"][0] is None:
                accumulator["timestamp_range"][0] = value.timestamp
            
            accumulator["timestamp_range"][1] = value.timestamp
            
            return accumulator
        
        def get_result(self, accumulator):
            # setì„ listë¡œ ë³€í™˜ (ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡)
            accumulator["unique_users"] = list(accumulator["unique_users"])
            return accumulator
    
    class GlobalWindowFunction(AllWindowFunction):
        """ê¸€ë¡œë²Œ ìœˆë„ìš° í•¨ìˆ˜"""
        def apply(self, window, input, out):
            result = input[0] if input else self.create_empty_global_result()
            
            # ìœˆë„ìš° ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.update({
                "window_start": window.start,
                "window_end": window.end,
                "window_duration": window.end - window.start
            })
            
            # íŒŒìƒ ì§€í‘œ ê³„ì‚°
            result["events_per_second"] = result["total_events"] / max(
                result["window_duration"] / 1000, 1
            )
            
            result["revenue_per_user"] = (
                result["total_revenue"] / len(result["unique_users"])
                if result["unique_users"] else 0
            )
            
            result["conversion_rate"] = (
                result["action_distribution"].get("purchase", 0) / 
                result["total_events"] if result["total_events"] > 0 else 0
            )
            
            out.collect(result)
        
        def create_empty_global_result(self):
            return {
                "total_events": 0,
                "unique_users": [],
                "total_revenue": 0.0,
                "action_distribution": {},
                "timestamp_range": [None, None],
                "events_per_second": 0.0,
                "revenue_per_user": 0.0,
                "conversion_rate": 0.0
            }
```

## ğŸ“ˆ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### Grafana ì—°ë™ ëŒ€ì‹œë³´ë“œ

```python
import requests
import json
from datetime import datetime

class GrafanaDashboardManager:
    def __init__(self, grafana_url, api_key):
        self.grafana_url = grafana_url
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def create_real_time_dashboard(self, dashboard_config):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        dashboard = {
            "dashboard": {
                "title": dashboard_config["title"],
                "panels": self.create_panels(dashboard_config["metrics"]),
                "time": {
                    "from": "now-1h",
                    "to": "now"
                },
                "refresh": "5s",
                "timezone": "browser"
            },
            "overwrite": True
        }
        
        response = requests.post(
            f"{self.grafana_url}/api/dashboards/db",
            headers=self.headers,
            json=dashboard
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"Failed to create dashboard: {response.text}")
    
    def create_panels(self, metrics_config):
        """íŒ¨ë„ ìƒì„±"""
        panels = []
        
        for i, metric in enumerate(metrics_config):
            panel = {
                "id": i + 1,
                "title": metric["title"],
                "type": metric["type"],
                "targets": [
                    {
                        "expr": metric["query"],
                        "refId": "A"
                    }
                ],
                "gridPos": {
                    "h": 8,
                    "w": 12,
                    "x": (i % 2) * 12,
                    "y": (i // 2) * 8
                },
                "yAxes": [
                    {
                        "label": metric.get("y_axis_label", ""),
                        "min": 0
                    }
                ]
            }
            
            # íŒ¨ë„ íƒ€ì…ë³„ íŠ¹ë³„ ì„¤ì •
            if metric["type"] == "stat":
                panel["fieldConfig"] = {
                    "defaults": {
                        "color": {"mode": "thresholds"},
                        "thresholds": {
                            "steps": [
                                {"color": "green", "value": None},
                                {"color": "yellow", "value": metric.get("warning_threshold", 100)},
                                {"color": "red", "value": metric.get("critical_threshold", 200)}
                            ]
                        }
                    }
                }
            elif metric["type"] == "table":
                panel["targets"][0]["format"] = "table"
            
            panels.append(panel)
        
        return panels
    
    def setup_flink_metrics_dashboard(self):
        """Flink ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ ì„¤ì •"""
        metrics_config = [
            {
                "title": "Total Events per Second",
                "type": "stat",
                "query": "flink_taskmanager_job_task_operator_numRecordsInPerSecond",
                "y_axis_label": "Events/sec",
                "warning_threshold": 1000,
                "critical_threshold": 5000
            },
            {
                "title": "Average Latency",
                "type": "stat",
                "query": "flink_taskmanager_job_task_operator_latency",
                "y_axis_label": "ms",
                "warning_threshold": 100,
                "critical_threshold": 500
            },
            {
                "title": "Backpressure",
                "type": "stat",
                "query": "flink_taskmanager_job_task_backPressuredTimeMsPerSecond",
                "y_axis_label": "ms/sec",
                "warning_threshold": 100,
                "critical_threshold": 500
            },
            {
                "title": "Checkpoint Duration",
                "type": "stat",
                "query": "flink_jobmanager_job_lastCheckpointDuration",
                "y_axis_label": "ms",
                "warning_threshold": 10000,
                "critical_threshold": 30000
            },
            {
                "title": "Memory Usage",
                "type": "stat",
                "query": "flink_taskmanager_Status_JVM_Memory_Heap_Used",
                "y_axis_label": "bytes",
                "warning_threshold": 1073741824,  # 1GB
                "critical_threshold": 2147483648   # 2GB
            },
            {
                "title": "Event Timeline",
                "type": "timeseries",
                "query": "flink_taskmanager_job_task_operator_numRecordsInPerSecond",
                "y_axis_label": "Events/sec"
            }
        ]
        
        return self.create_real_time_dashboard({
            "title": "Apache Flink Real-time Metrics",
            "metrics": metrics_config
        })
    
    def create_business_metrics_dashboard(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ ì„¤ì •"""
        metrics_config = [
            {
                "title": "Total Revenue",
                "type": "stat",
                "query": "sum(purchase_amount)",
                "y_axis_label": "$",
                "warning_threshold": 10000,
                "critical_threshold": 50000
            },
            {
                "title": "Conversion Rate",
                "type": "stat",
                "query": "purchase_events / total_events * 100",
                "y_axis_label": "%",
                "warning_threshold": 2.0,
                "critical_threshold": 5.0
            },
            {
                "title": "Active Users",
                "type": "stat",
                "query": "count(distinct user_id)",
                "y_axis_label": "Users",
                "warning_threshold": 100,
                "critical_threshold": 500
            },
            {
                "title": "Average Session Duration",
                "type": "stat",
                "query": "avg(session_duration)",
                "y_axis_label": "seconds",
                "warning_threshold": 300,
                "critical_threshold": 600
            },
            {
                "title": "Revenue Trend",
                "type": "timeseries",
                "query": "sum(purchase_amount)",
                "y_axis_label": "$"
            },
            {
                "title": "Top Categories",
                "type": "table",
                "query": "topk(10, sum(purchase_amount) by category)",
                "y_axis_label": "Revenue"
            }
        ]
        
        return self.create_real_time_dashboard({
            "title": "Business Metrics Dashboard",
            "metrics": metrics_config
        })
```

### ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
class RealTimeAlertingSystem:
    def __init__(self):
        self.alert_rules = {}
        self.alert_history = []
    
    def setup_alert_rules(self):
        """ì•Œë¦¼ ê·œì¹™ ì„¤ì •"""
        self.alert_rules = {
            "high_latency": {
                "condition": lambda metrics: metrics.get("latency", 0) > 500,
                "severity": "critical",
                "message": "High latency detected: {latency}ms",
                "notification_channels": ["email", "slack"]
            },
            "low_throughput": {
                "condition": lambda metrics: metrics.get("throughput", 0) < 100,
                "severity": "warning",
                "message": "Low throughput detected: {throughput} events/sec",
                "notification_channels": ["slack"]
            },
            "high_error_rate": {
                "condition": lambda metrics: metrics.get("error_rate", 0) > 0.05,
                "severity": "critical",
                "message": "High error rate detected: {error_rate}%",
                "notification_channels": ["email", "slack", "pagerduty"]
            },
            "memory_usage_high": {
                "condition": lambda metrics: metrics.get("memory_usage", 0) > 0.8,
                "severity": "warning",
                "message": "High memory usage: {memory_usage}%",
                "notification_channels": ["slack"]
            },
            "checkpoint_failure": {
                "condition": lambda metrics: metrics.get("checkpoint_failures", 0) > 0,
                "severity": "critical",
                "message": "Checkpoint failures detected: {checkpoint_failures}",
                "notification_channels": ["email", "pagerduty"]
            }
        }
    
    def evaluate_alerts(self, metrics):
        """ì•Œë¦¼ í‰ê°€"""
        active_alerts = []
        
        for rule_name, rule in self.alert_rules.items():
            if rule["condition"](metrics):
                alert = {
                    "rule_name": rule_name,
                    "severity": rule["severity"],
                    "message": rule["message"].format(**metrics),
                    "timestamp": datetime.now().isoformat(),
                    "metrics": metrics,
                    "notification_channels": rule["notification_channels"]
                }
                
                # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (5ë¶„ ë‚´ ë™ì¼í•œ ì•Œë¦¼ ì œí•œ)
                if not self.is_duplicate_alert(alert):
                    active_alerts.append(alert)
                    self.send_notifications(alert)
                    self.alert_history.append(alert)
        
        return active_alerts
    
    def is_duplicate_alert(self, alert):
        """ì¤‘ë³µ ì•Œë¦¼ í™•ì¸"""
        recent_time = datetime.now().timestamp() - 300  # 5ë¶„ ì „
        
        for historical_alert in reversed(self.alert_history):
            if historical_alert["timestamp"] < recent_time:
                break
            
            if (historical_alert["rule_name"] == alert["rule_name"] and
                historical_alert["severity"] == alert["severity"]):
                return True
        
        return False
    
    def send_notifications(self, alert):
        """ì•Œë¦¼ ì „ì†¡"""
        for channel in alert["notification_channels"]:
            if channel == "email":
                self.send_email_alert(alert)
            elif channel == "slack":
                self.send_slack_alert(alert)
            elif channel == "pagerduty":
                self.send_pagerduty_alert(alert)
    
    def send_email_alert(self, alert):
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SMTP ë˜ëŠ” ì´ë©”ì¼ ì„œë¹„ìŠ¤ API ì‚¬ìš©
        print(f"EMAIL ALERT [{alert['severity'].upper()}]: {alert['message']}")
    
    def send_slack_alert(self, alert):
        """Slack ì•Œë¦¼ ì „ì†¡"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Slack Webhook API ì‚¬ìš©
        color = "danger" if alert["severity"] == "critical" else "warning"
        message = {
            "attachments": [
                {
                    "color": color,
                    "title": f"Flink Alert: {alert['rule_name']}",
                    "text": alert["message"],
                    "fields": [
                        {"title": "Severity", "value": alert["severity"], "short": True},
                        {"title": "Timestamp", "value": alert["timestamp"], "short": True}
                    ]
                }
            ]
        }
        print(f"SLACK ALERT: {message}")
    
    def send_pagerduty_alert(self, alert):
        """PagerDuty ì•Œë¦¼ ì „ì†¡"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PagerDuty API ì‚¬ìš©
        print(f"PAGERDUTY ALERT [{alert['severity'].upper()}]: {alert['message']}")
```

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ê¸ˆìœµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ ê°œìš”

ê¸ˆìœµ ê±°ë˜ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì´ìƒ ê±°ë˜ íŒ¨í„´ì„ ê°ì§€í•˜ê³ , ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

### 1. ê¸ˆìœµ ë°ì´í„° ëª¨ë¸

```python
from dataclasses import dataclass
from typing import Dict, Optional
from enum import Enum
import json

class TransactionType(Enum):
    DEPOSIT = "deposit"
    WITHDRAWAL = "withdrawal"
    TRANSFER = "transfer"
    PAYMENT = "payment"
    INVESTMENT = "investment"

class RiskLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class FinancialTransaction:
    transaction_id: str
    user_id: str
    account_id: str
    transaction_type: TransactionType
    amount: float
    currency: str
    timestamp: int
    merchant_id: Optional[str]
    location: Dict[str, float]  # latitude, longitude
    metadata: Dict
    
    def to_json(self):
        return json.dumps({
            'transaction_id': self.transaction_id,
            'user_id': self.user_id,
            'account_id': self.account_id,
            'transaction_type': self.transaction_type.value,
            'amount': self.amount,
            'currency': self.currency,
            'timestamp': self.timestamp,
            'merchant_id': self.merchant_id,
            'location': self.location,
            'metadata': self.metadata
        })
    
    @classmethod
    def from_json(cls, json_str):
        data = json.loads(json_str)
        data['transaction_type'] = TransactionType(data['transaction_type'])
        return cls(**data)

@dataclass
class RiskAssessment:
    transaction_id: str
    risk_level: RiskLevel
    risk_score: float
    risk_factors: list
    timestamp: int
    recommendation: str
    
    def to_json(self):
        return json.dumps({
            'transaction_id': self.transaction_id,
            'risk_level': self.risk_level.value,
            'risk_score': self.risk_score,
            'risk_factors': self.risk_factors,
            'timestamp': self.timestamp,
            'recommendation': self.recommendation
        })
```

### 2. ì‹¤ì‹œê°„ ì´ìƒ ê±°ë˜ ê°ì§€

```python
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.state import ValueStateDescriptor, MapStateDescriptor, ListStateDescriptor
from pyflink.common.typeinfo import Types
import math

class FraudDetectionEngine(KeyedProcessFunction):
    def __init__(self):
        self.user_transaction_history = None
        self.user_behavior_profile = None
        self.location_history = None
        self.velocity_checker = None
    
    def open(self, runtime_context):
        # ì‚¬ìš©ì ê±°ë˜ íˆìŠ¤í† ë¦¬
        self.user_transaction_history = runtime_context.get_list_state(
            ListStateDescriptor("transaction_history", Types.STRING())
        )
        
        # ì‚¬ìš©ì í–‰ë™ í”„ë¡œíŒŒì¼
        self.user_behavior_profile = runtime_context.get_state(
            ValueStateDescriptor("behavior_profile", Types.STRING())
        )
        
        # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬
        self.location_history = runtime_context.get_list_state(
            ListStateDescriptor("location_history", Types.STRING())
        )
        
        # ì†ë„ ê²€ì‚¬ê¸°
        self.velocity_checker = runtime_context.get_map_state(
            MapStateDescriptor("velocity_checker", Types.STRING(), Types.INT())
        )
    
    def process_element(self, transaction, ctx):
        """ì‹¤ì‹œê°„ ì´ìƒ ê±°ë˜ ê°ì§€"""
        user_id = transaction.user_id
        
        # 1. ì†ë„ ê²€ì‚¬ (Velocity Check)
        velocity_risk = self.check_velocity(transaction)
        
        # 2. ìœ„ì¹˜ ê¸°ë°˜ ê²€ì‚¬ (Location-based Check)
        location_risk = self.check_location_anomaly(transaction)
        
        # 3. ê¸ˆì•¡ ê¸°ë°˜ ê²€ì‚¬ (Amount-based Check)
        amount_risk = self.check_amount_anomaly(transaction)
        
        # 4. íŒ¨í„´ ê¸°ë°˜ ê²€ì‚¬ (Pattern-based Check)
        pattern_risk = self.check_pattern_anomaly(transaction)
        
        # 5. ì‹œê°„ ê¸°ë°˜ ê²€ì‚¬ (Time-based Check)
        time_risk = self.check_time_anomaly(transaction)
        
        # ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°
        risk_score = self.calculate_comprehensive_risk_score([
            velocity_risk, location_risk, amount_risk, pattern_risk, time_risk
        ])
        
        # ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •
        risk_level = self.determine_risk_level(risk_score)
        
        # ìœ„í—˜ í‰ê°€ ê²°ê³¼ ìƒì„±
        risk_assessment = RiskAssessment(
            transaction_id=transaction.transaction_id,
            risk_level=risk_level,
            risk_score=risk_score,
            risk_factors=self.collect_risk_factors([
                velocity_risk, location_risk, amount_risk, pattern_risk, time_risk
            ]),
            timestamp=transaction.timestamp,
            recommendation=self.generate_recommendation(risk_level, risk_score)
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.update_user_state(transaction)
        
        # ê²°ê³¼ ì¶œë ¥
        ctx.collect(risk_assessment)
        
        # ë†’ì€ ìœ„í—˜ë„ì¸ ê²½ìš° ì¦‰ì‹œ ì•Œë¦¼
        if risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]:
            self.send_immediate_alert(risk_assessment, transaction)
    
    def check_velocity(self, transaction):
        """ì†ë„ ê²€ì‚¬ (ë‹¨ì‹œê°„ ë‚´ ë‹¤ìˆ˜ ê±°ë˜)"""
        current_time = transaction.timestamp
        transaction_type = transaction.transaction_type.value
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ ê±°ë˜ íšŸìˆ˜ í™•ì¸
        time_windows = [300, 900, 3600]  # 5ë¶„, 15ë¶„, 1ì‹œê°„
        velocity_risks = []
        
        for window in time_windows:
            window_key = f"{transaction_type}_{window}"
            recent_count = self.velocity_checker.get(window_key) or 0
            
            # ìœˆë„ìš°ë³„ ì„ê³„ê°’ ì„¤ì •
            thresholds = {
                "withdrawal": [5, 10, 20],
                "transfer": [10, 20, 50],
                "payment": [20, 50, 100]
            }
            
            threshold = thresholds.get(transaction_type, [10, 20, 50])[time_windows.index(window)]
            
            if recent_count >= threshold:
                velocity_risks.append({
                    "type": "high_velocity",
                    "window": window,
                    "count": recent_count,
                    "threshold": threshold,
                    "risk_score": min((recent_count / threshold) * 0.5, 1.0)
                })
            
            # ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ì‹œê°„ ê¸°ë°˜ ì •ë¦¬ í•„ìš”)
            self.velocity_checker.put(window_key, recent_count + 1)
        
        return velocity_risks
    
    def check_location_anomaly(self, transaction):
        """ìœ„ì¹˜ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        location_risks = []
        
        # ìƒˆë¡œìš´ ìœ„ì¹˜ì¸ì§€ í™•ì¸
        transaction_location = transaction.location
        location_history = list(self.location_history.get())
        
        if not location_history:
            # ì²« ê±°ë˜ì¸ ê²½ìš°
            self.location_history.add(json.dumps(transaction_location))
            return []
        
        # ê±°ë¦¬ ê³„ì‚°
        distances = []
        for historical_location_str in location_history:
            historical_location = json.loads(historical_location_str)
            distance = self.calculate_distance(transaction_location, historical_location)
            distances.append(distance)
        
        min_distance = min(distances) if distances else float('inf')
        
        # ì´ìƒì ì¸ ìœ„ì¹˜ íŒì •
        if min_distance > 100:  # 100km ì´ìƒ
            location_risks.append({
                "type": "unusual_location",
                "distance_from_last": min_distance,
                "risk_score": min(min_distance / 1000, 1.0)
            })
        
        # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.location_history.add(json.dumps(transaction_location))
        
        return location_risks
    
    def check_amount_anomaly(self, transaction):
        """ê¸ˆì•¡ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        amount_risks = []
        
        # ì‚¬ìš©ì í‰ê·  ê±°ë˜ ê¸ˆì•¡ê³¼ ë¹„êµ
        user_profile = self.get_user_behavior_profile()
        
        if user_profile and "average_amount" in user_profile:
            avg_amount = user_profile["average_amount"]
            std_amount = user_profile.get("std_amount", avg_amount * 0.5)
            
            # Z-score ê³„ì‚°
            z_score = abs(transaction.amount - avg_amount) / std_amount
            
            if z_score > 3:  # 3ì‹œê·¸ë§ˆ ì´ìƒ
                amount_risks.append({
                    "type": "unusual_amount",
                    "amount": transaction.amount,
                    "average_amount": avg_amount,
                    "z_score": z_score,
                    "risk_score": min(z_score / 5, 1.0)
                })
        
        return amount_risks
    
    def check_pattern_anomaly(self, transaction):
        """íŒ¨í„´ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        pattern_risks = []
        
        # ê±°ë˜ ì‹œê°„ íŒ¨í„´ í™•ì¸
        transaction_time = datetime.fromtimestamp(transaction.timestamp)
        hour = transaction_time.hour
        
        user_profile = self.get_user_behavior_profile()
        
        if user_profile and "typical_hours" in user_profile:
            typical_hours = user_profile["typical_hours"]
            
            if hour not in typical_hours:
                pattern_risks.append({
                    "type": "unusual_time",
                    "hour": hour,
                    "typical_hours": typical_hours,
                    "risk_score": 0.3
                })
        
        # ê±°ë˜ ìœ í˜• íŒ¨í„´ í™•ì¸
        if user_profile and "transaction_types" in user_profile:
            typical_types = user_profile["transaction_types"]
            
            if transaction.transaction_type.value not in typical_types:
                pattern_risks.append({
                    "type": "unusual_transaction_type",
                    "transaction_type": transaction.transaction_type.value,
                    "typical_types": typical_types,
                    "risk_score": 0.4
                })
        
        return pattern_risks
    
    def check_time_anomaly(self, transaction):
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        time_risks = []
        
        transaction_time = datetime.fromtimestamp(transaction.timestamp)
        
        # ìƒˆë²½ ì‹œê°„ëŒ€ ê±°ë˜ (2AM - 6AM)
        if 2 <= transaction_time.hour <= 6:
            time_risks.append({
                "type": "late_night_transaction",
                "hour": transaction_time.hour,
                "risk_score": 0.2
            })
        
        # ì£¼ë§ ê±°ë˜ íŒ¨í„´ í™•ì¸
        if transaction_time.weekday() >= 5:  # í† ìš”ì¼, ì¼ìš”ì¼
            user_profile = self.get_user_behavior_profile()
            
            if user_profile and not user_profile.get("weekend_activity", False):
                time_risks.append({
                    "type": "weekend_unusual_activity",
                    "weekday": transaction_time.weekday(),
                    "risk_score": 0.3
                })
        
        return time_risks
    
    def calculate_comprehensive_risk_score(self, risk_factors):
        """ì¢…í•© ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
        if not risk_factors:
            return 0.0
        
        # ê° ìœ„í—˜ ìš”ì†Œì˜ ìµœëŒ€ ì ìˆ˜ ì‚¬ìš©
        max_risks = []
        for risk_list in risk_factors:
            if risk_list:
                max_risk_score = max([risk["risk_score"] for risk in risk_list])
                max_risks.append(max_risk_score)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        if max_risks:
            return sum(max_risks) / len(max_risks)
        
        return 0.0
    
    def determine_risk_level(self, risk_score):
        """ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
        if risk_score >= 0.8:
            return RiskLevel.CRITICAL
        elif risk_score >= 0.6:
            return RiskLevel.HIGH
        elif risk_score >= 0.3:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def collect_risk_factors(self, risk_factors):
        """ìœ„í—˜ ìš”ì†Œ ìˆ˜ì§‘"""
        all_factors = []
        for risk_list in risk_factors:
            all_factors.extend(risk_list)
        return all_factors
    
    def generate_recommendation(self, risk_level, risk_score):
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if risk_level == RiskLevel.CRITICAL:
            return "Immediate transaction blocking recommended"
        elif risk_level == RiskLevel.HIGH:
            return "Additional verification required"
        elif risk_level == RiskLevel.MEDIUM:
            return "Monitor closely, consider manual review"
        else:
            return "Low risk, normal processing"
    
    def update_user_state(self, transaction):
        """ì‚¬ìš©ì ìƒíƒœ ì—…ë°ì´íŠ¸"""
        # ê±°ë˜ íˆìŠ¤í† ë¦¬ ì¶”ê°€
        self.user_transaction_history.add(transaction.to_json())
        
        # í–‰ë™ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸
        self.update_behavior_profile(transaction)
    
    def update_behavior_profile(self, transaction):
        """í–‰ë™ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸"""
        profile = self.get_user_behavior_profile() or {}
        
        # í‰ê·  ê±°ë˜ ê¸ˆì•¡ ì—…ë°ì´íŠ¸
        if "total_amount" not in profile:
            profile["total_amount"] = 0
            profile["transaction_count"] = 0
        
        profile["total_amount"] += transaction.amount
        profile["transaction_count"] += 1
        profile["average_amount"] = profile["total_amount"] / profile["transaction_count"]
        
        # ê±°ë˜ ìœ í˜• ì¶”ê°€
        if "transaction_types" not in profile:
            profile["transaction_types"] = set()
        
        profile["transaction_types"].add(transaction.transaction_type.value)
        
        # ì „í˜•ì ì¸ ê±°ë˜ ì‹œê°„ ì—…ë°ì´íŠ¸
        transaction_time = datetime.fromtimestamp(transaction.timestamp)
        if "typical_hours" not in profile:
            profile["typical_hours"] = []
        
        if transaction_time.hour not in profile["typical_hours"]:
            profile["typical_hours"].append(transaction_time.hour)
        
        # í”„ë¡œíŒŒì¼ ì €ì¥
        self.user_behavior_profile.update(json.dumps(profile))
    
    def get_user_behavior_profile(self):
        """ì‚¬ìš©ì í–‰ë™ í”„ë¡œíŒŒì¼ ì¡°íšŒ"""
        profile_json = self.user_behavior_profile.value()
        if profile_json:
            profile = json.loads(profile_json)
            # setì„ listë¡œ ë³€í™˜ (ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡)
            if "transaction_types" in profile:
                profile["transaction_types"] = list(profile["transaction_types"])
            return profile
        return None
    
    def calculate_distance(self, loc1, loc2):
        """ë‘ ìœ„ì¹˜ ê°„ ê±°ë¦¬ ê³„ì‚° (Haversine formula)"""
        lat1, lon1 = loc1["latitude"], loc1["longitude"]
        lat2, lon2 = loc2["latitude"], loc2["longitude"]
        
        R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)
        
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        
        a = (math.sin(dlat/2) * math.sin(dlat/2) +
             math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
             math.sin(dlon/2) * math.sin(dlon/2))
        
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
        distance = R * c
        
        return distance
    
    def send_immediate_alert(self, risk_assessment, transaction):
        """ì¦‰ì‹œ ì•Œë¦¼ ì „ì†¡"""
        alert = {
            "type": "fraud_alert",
            "severity": risk_assessment.risk_level.value,
            "transaction_id": transaction.transaction_id,
            "user_id": transaction.user_id,
            "amount": transaction.amount,
            "risk_score": risk_assessment.risk_score,
            "recommendation": risk_assessment.recommendation,
            "timestamp": risk_assessment.timestamp
        }
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì•Œë¦¼ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
        print(f"IMMEDIATE ALERT: {alert}")
```

### 3. ì‹¤ì‹œê°„ ê¸ˆìœµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í†µí•©

```python
class RealTimeFinancialMonitoringSystem:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.setup_environment()
        self.alerting_system = RealTimeAlertingSystem()
        self.dashboard_manager = None
    
    def setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        # ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        self.env.get_checkpoint_config().enable_checkpointing(1000)
        
        # ìƒíƒœ ë°±ì—”ë“œ ì„¤ì •
        from pyflink.datastream.state import RocksDBStateBackend
        backend = RocksDBStateBackend(
            checkpoint_data_uri="file:///tmp/financial-monitoring-checkpoints"
        )
        self.env.set_state_backend(backend)
        
        # ì´ë²¤íŠ¸ ì‹œê°„ ì„¤ì •
        self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
    
    def create_kafka_source(self, topic, bootstrap_servers):
        """Kafka ì†ŒìŠ¤ ìƒì„±"""
        from pyflink.datastream.connectors import FlinkKafkaConsumer
        from pyflink.common.serialization import SimpleStringSchema
        
        kafka_properties = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': 'financial-monitoring-system'
        }
        
        kafka_source = FlinkKafkaConsumer(
            topic,
            SimpleStringSchema(),
            kafka_properties
        )
        
        return self.env.add_source(kafka_source)
    
    def parse_financial_transactions(self, transaction_stream):
        """ê¸ˆìœµ ê±°ë˜ íŒŒì‹±"""
        def parse_transaction(transaction_str):
            try:
                return FinancialTransaction.from_json(transaction_str)
            except Exception as e:
                print(f"Failed to parse transaction: {transaction_str}, Error: {e}")
                return None
        
        return transaction_stream.map(parse_transaction, output_type=Types.PICKLED_BYTE_ARRAY()) \
                               .filter(lambda x: x is not None)
    
    def setup_real_time_aggregations(self, transaction_stream):
        """ì‹¤ì‹œê°„ ì§‘ê³„ ì„¤ì •"""
        # 1ë¶„ ìœˆë„ìš°ë¡œ ì‹¤ì‹œê°„ ì§‘ê³„
        real_time_metrics = transaction_stream.window_all(
            TumblingEventTimeWindows.of(Time.minutes(1))
        ).aggregate(
            self.FinancialMetricsAggregateFunction(),
            self.FinancialMetricsWindowFunction(),
            output_type=Types.PICKLED_BYTE_ARRAY()
        )
        
        return real_time_metrics
    
    class FinancialMetricsAggregateFunction:
        """ê¸ˆìœµ ë©”íŠ¸ë¦­ ì§‘ê³„ í•¨ìˆ˜"""
        def create_accumulator(self):
            return {
                "total_transactions": 0,
                "total_amount": 0.0,
                "high_risk_transactions": 0,
                "unique_users": set(),
                "transaction_types": {},
                "hourly_distribution": {}
            }
        
        def add(self, accumulator, value):
            accumulator["total_transactions"] += 1
            accumulator["total_amount"] += value.amount
            
            if hasattr(value, 'risk_level') and value.risk_level in ['high', 'critical']:
                accumulator["high_risk_transactions"] += 1
            
            accumulator["unique_users"].add(value.user_id)
            
            transaction_type = value.transaction_type.value
            accumulator["transaction_types"][transaction_type] = \
                accumulator["transaction_types"].get(transaction_type, 0) + 1
            
            hour = datetime.fromtimestamp(value.timestamp).hour
            accumulator["hourly_distribution"][hour] = \
                accumulator["hourly_distribution"].get(hour, 0) + 1
            
            return accumulator
        
        def get_result(self, accumulator):
            # setì„ listë¡œ ë³€í™˜
            accumulator["unique_users"] = list(accumulator["unique_users"])
            return accumulator
    
    class FinancialMetricsWindowFunction(AllWindowFunction):
        """ê¸ˆìœµ ë©”íŠ¸ë¦­ ìœˆë„ìš° í•¨ìˆ˜"""
        def apply(self, window, input, out):
            metrics = input[0] if input else self.create_empty_metrics()
            
            # ìœˆë„ìš° ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metrics.update({
                "window_start": window.start,
                "window_end": window.end,
                "timestamp": window.end
            })
            
            # íŒŒìƒ ì§€í‘œ ê³„ì‚°
            metrics["average_transaction_amount"] = (
                metrics["total_amount"] / metrics["total_transactions"]
                if metrics["total_transactions"] > 0 else 0
            )
            
            metrics["risk_rate"] = (
                metrics["high_risk_transactions"] / metrics["total_transactions"]
                if metrics["total_transactions"] > 0 else 0
            )
            
            metrics["transactions_per_user"] = (
                metrics["total_transactions"] / len(metrics["unique_users"])
                if metrics["unique_users"] else 0
            )
            
            out.collect(metrics)
        
        def create_empty_metrics(self):
            return {
                "total_transactions": 0,
                "total_amount": 0.0,
                "high_risk_transactions": 0,
                "unique_users": [],
                "transaction_types": {},
                "hourly_distribution": {},
                "average_transaction_amount": 0.0,
                "risk_rate": 0.0,
                "transactions_per_user": 0.0
            }
    
    def run_financial_monitoring_system(self):
        """ê¸ˆìœµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        # Kafkaì—ì„œ ê±°ë˜ ë°ì´í„° ì½ê¸°
        transaction_stream = self.create_kafka_source("financial-transactions", "localhost:9092")
        
        # ê±°ë˜ íŒŒì‹±
        parsed_transactions = self.parse_financial_transactions(transaction_stream)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì›Œí„°ë§ˆí¬ í• ë‹¹
        from pyflink.datastream.functions import BoundedOutOfOrdernessTimestampExtractor
        from pyflink.common.time import Time
        
        class TransactionTimestampExtractor(BoundedOutOfOrdernessTimestampExtractor):
            def __init__(self):
                super().__init__(Time.seconds(30))  # 30ì´ˆ ì§€ì—° í—ˆìš©
            
            def extract_timestamp(self, element, previous_timestamp):
                return element.timestamp * 1000
        
        watermarked_transactions = parsed_transactions.assign_timestamps_and_watermarks(
            TransactionTimestampExtractor()
        )
        
        # ì´ìƒ ê±°ë˜ ê°ì§€
        risk_assessments = watermarked_transactions.key_by(lambda tx: tx.user_id) \
                                                   .process(FraudDetectionEngine())
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
        real_time_metrics = self.setup_real_time_aggregations(watermarked_transactions)
        
        # ê²°ê³¼ ì¶œë ¥
        risk_assessments.print("Risk Assessments")
        real_time_metrics.print("Real-time Metrics")
        
        # ì‹¤í–‰
        self.env.execute("Real-time Financial Monitoring System")

# ì‹œìŠ¤í…œ ì‹¤í–‰
if __name__ == "__main__":
    system = RealTimeFinancialMonitoringSystem()
    system.run_financial_monitoring_system()
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **CEP (Complex Event Processing)**
   - CEPì˜ ê¸°ë³¸ ê°œë…ê³¼ íŒ¨í„´ ì •ì˜
   - ê³ ê¸‰ íŒ¨í„´ ë§¤ì¹­ (ìˆœì°¨, OR, NOT, ë°˜ë³µ íŒ¨í„´)
   - ì¡°ê±´ë¶€ íŒ¨í„´ ë§¤ì¹­ê³¼ ë™ì  íŒ¨í„´ ìƒì„±
   - íŒ¨í„´ ê²°ê³¼ í›„ì²˜ë¦¬

2. **ì‹¤ì‹œê°„ ì§‘ê³„ì™€ ìœˆë„ìš° í•¨ìˆ˜**
   - ê³ ê¸‰ ìœˆë„ìš° ì—°ì‚° (íŠ¬ë¸”ë§, ìŠ¬ë¼ì´ë”©, ì„¸ì…˜)
   - ì‹¤ì‹œê°„ ì§‘ê³„ ì—”ì§„ êµ¬ì¶•
   - ì‚¬ìš©ìë³„ ë° ê¸€ë¡œë²Œ ì§‘ê³„
   - íŒŒìƒ ì§€í‘œ ê³„ì‚°

3. **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•**
   - Grafana ì—°ë™ ëŒ€ì‹œë³´ë“œ
   - ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ
   - ë‹¤ì¤‘ ì±„ë„ ì•Œë¦¼ (ì´ë©”ì¼, Slack, PagerDuty)
   - ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€

4. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì‹¤ì‹œê°„ ê¸ˆìœµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
   - ì´ìƒ ê±°ë˜ ê°ì§€ ì—”ì§„
   - ë‹¤ì¤‘ ìœ„í—˜ ìš”ì†Œ í‰ê°€
   - ì¢…í•© ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **CEP íŒ¨í„´ ë§¤ì¹­** | ë³µì¡í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ | â­â­â­â­â­ |
| **ì‹¤ì‹œê°„ ì§‘ê³„** | ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚° | â­â­â­â­â­ |
| **ìœˆë„ìš° í•¨ìˆ˜** | ì‹œê°„ ê¸°ë°˜ ë¶„ì„ | â­â­â­â­â­ |
| **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ** | ëª¨ë‹ˆí„°ë§ê³¼ ì‹œê°í™” | â­â­â­â­ |
| **ì•Œë¦¼ ì‹œìŠ¤í…œ** | ì´ìƒ ìƒí™© ëŒ€ì‘ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 4: í”„ë¡œë•ì…˜ ë°°í¬ì™€ ì„±ëŠ¥ ìµœì í™”**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- Kubernetesë¥¼ í™œìš©í•œ Flink í´ëŸ¬ìŠ¤í„° ë°°í¬
- ì„±ëŠ¥ íŠœë‹ê³¼ ëª¨ë‹ˆí„°ë§
- ì¥ì•  ë³µêµ¬ì™€ ìš´ì˜ ì „ëµ
- Flink Metricsì™€ Grafana ì—°ë™

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 4: í”„ë¡œë•ì…˜ ë°°í¬ì™€ ì„±ëŠ¥ ìµœì í™”](/data-engineering/2025/09/18/apache-flink-production-deployment.html)

---

*ì´ì œ Flinkì˜ ì‹¤ì‹œê°„ ë¶„ì„ê³¼ CEPë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë§ˆì§€ë§‰ íŒŒíŠ¸ì—ì„œëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ë°°í¬ì™€ ìš´ì˜ì„ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.* ğŸš€

---
layout: post
lang: ko
title: "Part 3: Apache Spark ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ Kafka ì—°ë™ - ì‹¤ë¬´ í”„ë¡œì íŠ¸"
description: "Apache Spark Streaming, Structured Streaming, Kafka ì—°ë™ì„ í†µí•œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì™€ ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
date: 2025-09-12
author: Data Droid
category: data-engineering
tags: [Apache-Spark, Spark-Streaming, Kafka, ì‹¤ì‹œê°„ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë°, ì›Œí„°ë§ˆí‚¹, Python]
series: apache-spark-complete-guide
series_order: 3
reading_time: "50ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 3: Apache Spark ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ Kafka ì—°ë™ - ì‹¤ë¬´ í”„ë¡œì íŠ¸

> Apache Spark Streaming, Structured Streaming, Kafka ì—°ë™ì„ í†µí•œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì™€ ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [Spark Streaming ê¸°ì´ˆ](#spark-streaming-ê¸°ì´ˆ)
2. [Structured Streaming ì™„ì „ ì •ë¦¬](#structured-streaming-ì™„ì „-ì •ë¦¬)
3. [Kafka ì—°ë™ê³¼ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬](#kafka-ì—°ë™ê³¼-ì‹¤ì‹œê°„-ë°ì´í„°-ì²˜ë¦¬)
4. [ì›Œí„°ë§ˆí‚¹ê³¼ ì§€ì—° ë°ì´í„° ì²˜ë¦¬](#ì›Œí„°ë§ˆí‚¹ê³¼-ì§€ì—°-ë°ì´í„°-ì²˜ë¦¬)
5. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ë¡œê·¸-ë¶„ì„-ì‹œìŠ¤í…œ)
6. [ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•](#ì‹¤ì‹œê°„-ëŒ€ì‹œë³´ë“œ-êµ¬ì¶•)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ”„ Spark Streaming ê¸°ì´ˆ

### Spark Streamingì´ë€?

Spark Streamingì€ Sparkì˜ í™•ì¥ ëª¨ë“ˆë¡œ, **ë§ˆì´í¬ë¡œ ë°°ì¹˜(Micro-batch)** ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

#### **í•µì‹¬ ê°œë…**
- **DStream (Discretized Stream)**: ì—°ì†ì ì¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆˆ ê²ƒ
- **ë°°ì¹˜ ê°„ê²© (Batch Interval)**: ê° ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹œê°„ ê°„ê²©
- **ì²´í¬í¬ì¸íŠ¸ (Checkpoint)**: ì¥ì•  ë³µêµ¬ë¥¼ ìœ„í•œ ìƒíƒœ ì €ì¥

### DStream ê¸°ë³¸ ì—°ì‚°

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# StreamingContext ìƒì„± (5ì´ˆ ë°°ì¹˜ ê°„ê²©)
sc = SparkContext("local[2]", "StreamingExample")
ssc = StreamingContext(sc, 5)  # 5ì´ˆ ë°°ì¹˜ ê°„ê²©

# í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„± (ì†Œì¼“ ì—°ê²°)
lines = ssc.socketTextStream("localhost", 9999)

# ê¸°ë³¸ ë³€í™˜ ì—°ì‚°
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda x, y: x + y)

# ì¶œë ¥
word_counts.pprint()

# ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
ssc.start()
ssc.awaitTermination()
```

### DStream ê³ ê¸‰ ì—°ì‚°

```python
# ìœˆë„ìš° ì—°ì‚°
windowed_counts = word_counts.reduceByKeyAndWindow(
    lambda x, y: x + y,  # reduce í•¨ìˆ˜
    lambda x, y: x - y,  # inverse reduce í•¨ìˆ˜
    30,  # ìœˆë„ìš° ê¸¸ì´ (30ì´ˆ)
    10   # ìŠ¬ë¼ì´ë”© ê°„ê²© (10ì´ˆ)
)

# ìƒíƒœ ìœ ì§€ ì—°ì‚°
def update_function(new_values, running_count):
    if running_count is None:
        running_count = 0
    return sum(new_values, running_count)

running_counts = word_counts.updateStateByKey(update_function)

# ì¡°ì¸ ì—°ì‚°
reference_data = sc.parallelize([("spark", "framework"), ("kafka", "broker")])
reference_dstream = ssc.queueStream([reference_data])
joined_stream = word_counts.transform(lambda rdd: rdd.join(reference_data))

# ì¶œë ¥
windowed_counts.pprint()
running_counts.pprint()
joined_stream.pprint()
```

## ğŸ“Š Structured Streaming ì™„ì „ ì •ë¦¬

### Structured Streamingì´ë€?

Structured Streamingì€ Spark SQL ì—”ì§„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ **ê³ ìˆ˜ì¤€ ìŠ¤íŠ¸ë¦¬ë° API**ì…ë‹ˆë‹¤.

#### **í•µì‹¬ íŠ¹ì§•**
- **ì •í™•íˆ í•œ ë²ˆ ì²˜ë¦¬ (Exactly-once processing)**
- **ì›Œí„°ë§ˆí‚¹ (Watermarking)** ì§€ì›
- **ì´ë²¤íŠ¸ ì‹œê°„ (Event Time)** ì²˜ë¦¬
- **êµ¬ì¡°í™”ëœ ë°ì´í„°** ì²˜ë¦¬

### ê¸°ë³¸ êµ¬ì¡°í™”ëœ ìŠ¤íŠ¸ë¦¬ë°

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("StructuredStreamingExample") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì½ê¸°
streaming_df = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# ë°ì´í„° ë³€í™˜
words_df = streaming_df.select(
    explode(split(streaming_df.value, " ")).alias("word")
)

# ì§‘ê³„
word_counts = words_df.groupBy("word").count()

# ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘
query = word_counts \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .trigger(processingTime="10 seconds") \
    .start()

query.awaitTermination()
```

### ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤

```python
# 1. Kafka ìŠ¤íŠ¸ë¦¼
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user-events") \
    .option("startingOffsets", "latest") \
    .load()

# 2. íŒŒì¼ ìŠ¤íŠ¸ë¦¼
file_df = spark \
    .readStream \
    .format("json") \
    .option("path", "/path/to/streaming/data") \
    .option("maxFilesPerTrigger", 1) \
    .schema(schema) \
    .load()

# 3. Rate ìŠ¤íŠ¸ë¦¼ (í…ŒìŠ¤íŠ¸ìš©)
rate_df = spark \
    .readStream \
    .format("rate") \
    .option("rowsPerSecond", 100) \
    .load()
```

### ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì—°ì‚°

```python
# ì´ë²¤íŠ¸ ì‹œê°„ ì²˜ë¦¬
from pyspark.sql.types import TimestampType

# ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("timestamp", TimestampType(), True),
    StructField("user_id", StringType(), True),
    StructField("action", StringType(), True),
    StructField("value", DoubleType(), True)
])

# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì½ê¸°
events_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load() \
    .select(
        from_json(col("value").cast("string"), schema).alias("data")
    ) \
    .select("data.*")

# ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ë°˜ ìœˆë„ìš° ì§‘ê³„
windowed_events = events_df \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window("timestamp", "5 minutes", "1 minute"),
        "user_id"
    ) \
    .agg(
        count("*").alias("event_count"),
        sum("value").alias("total_value"),
        avg("value").alias("avg_value")
    )

# ì¶œë ¥
query = windowed_events \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()
```

## ğŸ”— Kafka ì—°ë™ê³¼ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬

### Kafka ì„¤ì •ê³¼ ì—°ê²°

```python
# Kafka í”„ë¡œë“€ì„œ ì„¤ì •
from kafka import KafkaProducer
import json
import time
import random

def create_kafka_producer():
    return KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda x: json.dumps(x).encode('utf-8'),
        key_serializer=lambda x: x.encode('utf-8') if x else None
    )

# ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„± ë° ì „ì†¡
def generate_user_events():
    producer = create_kafka_producer()
    
    actions = ['login', 'logout', 'purchase', 'view', 'click']
    user_ids = ['user_001', 'user_002', 'user_003', 'user_004', 'user_005']
    
    for i in range(1000):
        event = {
            'timestamp': int(time.time() * 1000),
            'user_id': random.choice(user_ids),
            'action': random.choice(actions),
            'value': random.uniform(1.0, 100.0),
            'session_id': f'session_{i}',
            'ip_address': f'192.168.1.{random.randint(1, 254)}'
        }
        
        producer.send('user-events', key=event['user_id'], value=event)
        time.sleep(0.1)  # 100ms ê°„ê²©
    
    producer.close()

# ë°ì´í„° ìƒì„± ì‹¤í–‰
# generate_user_events()
```

### Sparkì—ì„œ Kafka ë°ì´í„° ì½ê¸°

```python
# Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user-events") \
    .option("startingOffsets", "earliest") \
    .option("failOnDataLoss", "false") \
    .load()

# JSON íŒŒì‹±
from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType

event_schema = StructType([
    StructField("timestamp", LongType(), True),
    StructField("user_id", StringType(), True),
    StructField("action", StringType(), True),
    StructField("value", DoubleType(), True),
    StructField("session_id", StringType(), True),
    StructField("ip_address", StringType(), True)
])

parsed_df = kafka_df.select(
    col("key").cast("string").alias("kafka_key"),
    from_json(col("value").cast("string"), event_schema).alias("data")
).select("kafka_key", "data.*")

# íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
events_df = parsed_df.withColumn(
    "event_time", 
    from_unixtime(col("timestamp") / 1000).cast(TimestampType())
)
```

### ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„

```python
# ì‹¤ì‹œê°„ ì‚¬ìš©ì í™œë™ ë¶„ì„
user_activity = events_df \
    .withWatermark("event_time", "5 minutes") \
    .groupBy(
        window("event_time", "1 minute", "30 seconds"),
        "user_id"
    ) \
    .agg(
        count("*").alias("total_events"),
        countDistinct("action").alias("unique_actions"),
        sum("value").alias("total_value"),
        collect_list("action").alias("actions_sequence")
    )

# ì•¡ì…˜ë³„ ì‹¤ì‹œê°„ ì§‘ê³„
action_metrics = events_df \
    .withWatermark("event_time", "5 minutes") \
    .groupBy(
        window("event_time", "2 minutes", "1 minute"),
        "action"
    ) \
    .agg(
        count("*").alias("action_count"),
        countDistinct("user_id").alias("unique_users"),
        avg("value").alias("avg_value"),
        max("value").alias("max_value"),
        min("value").alias("min_value")
    )

# ì´ìƒ íƒì§€ (ì‹¤ì‹œê°„)
anomaly_detection = events_df \
    .withWatermark("event_time", "10 minutes") \
    .groupBy(
        window("event_time", "5 minutes", "2 minutes"),
        "user_id"
    ) \
    .agg(
        count("*").alias("event_count"),
        sum("value").alias("total_value")
    ) \
    .filter(col("event_count") > 50)  # 5ë¶„ì— 50ê°œ ì´ìƒ ì´ë²¤íŠ¸

# ì¶œë ¥ ì„¤ì •
user_activity_query = user_activity \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()

action_metrics_query = action_metrics \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()

anomaly_query = anomaly_detection \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()
```

## ğŸ’§ ì›Œí„°ë§ˆí‚¹ê³¼ ì§€ì—° ë°ì´í„° ì²˜ë¦¬

### ì›Œí„°ë§ˆí‚¹ì´ë€?

ì›Œí„°ë§ˆí‚¹ì€ **ì§€ì—° ë°ì´í„°(late data)**ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

#### **ì›Œí„°ë§ˆí‚¹ ê°œë…**
- **ì´ë²¤íŠ¸ ì‹œê°„ (Event Time)**: ë°ì´í„°ê°€ ì‹¤ì œë¡œ ë°œìƒí•œ ì‹œê°„
- **ì²˜ë¦¬ ì‹œê°„ (Processing Time)**: ë°ì´í„°ê°€ ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬ë˜ëŠ” ì‹œê°„
- **ì›Œí„°ë§ˆí¬**: ì§€ì—° ë°ì´í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì§€ì—° ì‹œê°„

### ì›Œí„°ë§ˆí‚¹ êµ¬í˜„

```python
# ì›Œí„°ë§ˆí‚¹ì„ ì‚¬ìš©í•œ ìœˆë„ìš° ì§‘ê³„
from pyspark.sql.functions import current_timestamp

# ì›Œí„°ë§ˆí¬ ì„¤ì • (ì´ë²¤íŠ¸ ì‹œê°„ìœ¼ë¡œë¶€í„° 10ë¶„ ì§€ì—° í—ˆìš©)
windowed_events_with_watermark = events_df \
    .withWatermark("event_time", "10 minutes") \
    .groupBy(
        window("event_time", "5 minutes"),
        "action"
    ) \
    .agg(
        count("*").alias("count"),
        sum("value").alias("total_value")
    )

# ì§€ì—° ë°ì´í„° í…ŒìŠ¤íŠ¸
def test_late_data():
    """ì§€ì—° ë°ì´í„°ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í•¨ìˆ˜"""
    import time
    from datetime import datetime, timedelta
    
    producer = create_kafka_producer()
    
    # ì •ìƒ ë°ì´í„°
    normal_event = {
        'timestamp': int(time.time() * 1000),
        'user_id': 'user_001',
        'action': 'click',
        'value': 10.0
    }
    
    # ì§€ì—° ë°ì´í„° (5ë¶„ ì „ ë°ì´í„°)
    late_event = {
        'timestamp': int((time.time() - 300) * 1000),  # 5ë¶„ ì „
        'user_id': 'user_002',
        'action': 'click',
        'value': 20.0
    }
    
    # ë§¤ìš° ì§€ì—°ëœ ë°ì´í„° (15ë¶„ ì „ ë°ì´í„°)
    very_late_event = {
        'timestamp': int((time.time() - 900) * 1000),  # 15ë¶„ ì „
        'user_id': 'user_003',
        'action': 'click',
        'value': 30.0
    }
    
    # ë°ì´í„° ì „ì†¡
    producer.send('user-events', key=normal_event['user_id'], value=normal_event)
    producer.send('user-events', key=late_event['user_id'], value=late_event)
    producer.send('user-events', key=very_late_event['user_id'], value=very_late_event)
    
    producer.close()

# ì›Œí„°ë§ˆí‚¹ ì¿¼ë¦¬ ì‹¤í–‰
watermark_query = windowed_events_with_watermark \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()
```

### ì§€ì—° ë°ì´í„° ì²˜ë¦¬ ì „ëµ

```python
# 1. ì ì‘í˜• ì›Œí„°ë§ˆí¬
adaptive_watermark = events_df \
    .withWatermark("event_time", "5 minutes") \
    .groupBy(
        window("event_time", "2 minutes"),
        "user_id"
    ) \
    .agg(
        count("*").alias("event_count"),
        max("event_time").alias("latest_event_time")
    )

# 2. ì§€ì—° ë°ì´í„° ë³„ë„ ì²˜ë¦¬
# ì •ìƒ ë°ì´í„°
normal_data = events_df \
    .withWatermark("event_time", "5 minutes") \
    .filter(col("event_time") >= current_timestamp() - expr("INTERVAL 10 MINUTES"))

# ì§€ì—° ë°ì´í„°
late_data = events_df \
    .withWatermark("event_time", "5 minutes") \
    .filter(col("event_time") < current_timestamp() - expr("INTERVAL 10 MINUTES"))

# 3. ì§€ì—° ë°ì´í„° ì•Œë¦¼
late_data_alert = late_data \
    .groupBy("user_id") \
    .agg(
        count("*").alias("late_event_count"),
        min("event_time").alias("earliest_late_event")
    ) \
    .filter(col("late_event_count") > 5)

# ì§€ì—° ë°ì´í„° ì•Œë¦¼ ì¿¼ë¦¬
late_alert_query = late_data_alert \
    .writeStream \
    .outputMode("update") \
    .format("console") \
    .option("truncate", False) \
    .start()
```

## ğŸ› ï¸ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
real-time-log-analysis/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ log_processor.py
â”‚   â”œâ”€â”€ anomaly_detector.py
â”‚   â”œâ”€â”€ metrics_calculator.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.py
â”‚   â””â”€â”€ streaming_config.yaml
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ kafka-setup.sh
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ grafana-dashboard.json
â”‚   â””â”€â”€ prometheus-config.yml
â””â”€â”€ README.md
```

### 1. ë¡œê·¸ í”„ë¡œì„¸ì„œ

```python
# src/log_processor.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import re
import json

class LogProcessor:
    def __init__(self, spark_session):
        self.spark = spark_session
        
        # ë¡œê·¸ íŒ¨í„´ ì •ì˜
        self.log_patterns = {
            'apache': r'^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)',
            'nginx': r'^(\S+) - (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+) "([^"]*)" "([^"]*)"',
            'json': r'^{.*}$'
        }
    
    def parse_apache_log(self, log_line):
        """Apache ë¡œê·¸ íŒŒì‹±"""
        match = re.match(self.log_patterns['apache'], log_line)
        if match:
            return {
                'ip': match.group(1),
                'identity': match.group(2),
                'user': match.group(3),
                'timestamp': match.group(4),
                'method': match.group(5),
                'url': match.group(6),
                'protocol': match.group(7),
                'status': int(match.group(8)),
                'size': match.group(9)
            }
        return None
    
    def parse_json_log(self, log_line):
        """JSON ë¡œê·¸ íŒŒì‹±"""
        try:
            return json.loads(log_line)
        except:
            return None
    
    def create_log_schema(self):
        """ë¡œê·¸ ìŠ¤í‚¤ë§ˆ ì •ì˜"""
        return StructType([
            StructField("timestamp", TimestampType(), True),
            StructField("level", StringType(), True),
            StructField("service", StringType(), True),
            StructField("message", StringType(), True),
            StructField("ip_address", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("request_id", StringType(), True),
            StructField("response_time", DoubleType(), True),
            StructField("status_code", IntegerType(), True),
            StructField("error_code", StringType(), True)
        ])
    
    def process_log_stream(self, kafka_df):
        """ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        # JSON íŒŒì‹±
        parsed_df = kafka_df.select(
            from_json(col("value").cast("string"), self.create_log_schema()).alias("data")
        ).select("data.*")
        
        # ë°ì´í„° ì •ì œ
        cleaned_df = parsed_df \
            .withColumn("timestamp", to_timestamp(col("timestamp"))) \
            .withColumn("response_time", col("response_time").cast(DoubleType())) \
            .withColumn("status_code", col("status_code").cast(IntegerType())) \
            .filter(col("timestamp").isNotNull())
        
        return cleaned_df
    
    def extract_metrics(self, logs_df):
        """ë¡œê·¸ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        # ì‘ë‹µ ì‹œê°„ ë¶„í¬
        response_time_metrics = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute"),
                "service"
            ) \
            .agg(
                count("*").alias("request_count"),
                avg("response_time").alias("avg_response_time"),
                max("response_time").alias("max_response_time"),
                min("response_time").alias("min_response_time"),
                stddev("response_time").alias("stddev_response_time")
            )
        
        # ì—ëŸ¬ìœ¨ ê³„ì‚°
        error_rate = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute"),
                "service"
            ) \
            .agg(
                count("*").alias("total_requests"),
                sum(when(col("status_code") >= 400, 1).otherwise(0)).alias("error_count")
            ) \
            .withColumn("error_rate", col("error_count") / col("total_requests") * 100)
        
        # IPë³„ ìš”ì²­ íŒ¨í„´
        ip_patterns = logs_df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window("timestamp", "5 minutes"),
                "ip_address"
            ) \
            .agg(
                count("*").alias("request_count"),
                countDistinct("user_id").alias("unique_users"),
                collect_set("service").alias("services_used")
            )
        
        return response_time_metrics, error_rate, ip_patterns
```

### 2. ì´ìƒ íƒì§€ê¸°

```python
# src/anomaly_detector.py
from pyspark.sql.functions import *
from pyspark.sql.window import Window
import numpy as np

class AnomalyDetector:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def detect_response_time_anomalies(self, metrics_df):
        """ì‘ë‹µ ì‹œê°„ ì´ìƒ íƒì§€"""
        # ì´ë™ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        window_spec = Window.partitionBy("service").orderBy("window")
        
        anomaly_df = metrics_df \
            .withColumn("avg_avg_response_time", avg("avg_response_time").over(
                window_spec.rowsBetween(-10, -1)
            )) \
            .withColumn("stddev_avg_response_time", stddev("avg_response_time").over(
                window_spec.rowsBetween(-10, -1)
            )) \
            .withColumn("z_score", 
                (col("avg_response_time") - col("avg_avg_response_time")) / 
                col("stddev_avg_response_time")
            ) \
            .filter(
                (col("z_score") > 2) | (col("z_score") < -2)
            )
        
        return anomaly_df
    
    def detect_error_rate_spikes(self, error_rate_df):
        """ì—ëŸ¬ìœ¨ ê¸‰ì¦ íƒì§€"""
        window_spec = Window.partitionBy("service").orderBy("window")
        
        spike_df = error_rate_df \
            .withColumn("prev_error_rate", lag("error_rate", 1).over(window_spec)) \
            .withColumn("error_rate_change", 
                col("error_rate") - col("prev_error_rate")
            ) \
            .filter(col("error_rate_change") > 10)  # 10% ì´ìƒ ì¦ê°€
        
        return spike_df
    
    def detect_suspicious_ips(self, ip_patterns_df):
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ IP íƒì§€"""
        # ë†’ì€ ìš”ì²­ ë¹ˆë„
        high_frequency = ip_patterns_df.filter(col("request_count") > 1000)
        
        # ë§ì€ ì„œë¹„ìŠ¤ ì‚¬ìš©
        multi_service = ip_patterns_df.filter(size(col("services_used")) > 5)
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê²°í•©
        suspicious_ips = high_frequency.intersect(multi_service)
        
        return suspicious_ips
    
    def detect_ddos_attacks(self, logs_df):
        """DDoS ê³µê²© íƒì§€"""
        ddos_df = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute"),
                "ip_address"
            ) \
            .agg(
                count("*").alias("request_count"),
                countDistinct("user_id").alias("unique_users")
            ) \
            .filter(
                (col("request_count") > 100) & (col("unique_users") < 5)
            )
        
        return ddos_df
```

### 3. ë©”íŠ¸ë¦­ ê³„ì‚°ê¸°

```python
# src/metrics_calculator.py
from pyspark.sql.functions import *
from pyspark.sql.window import Window

class MetricsCalculator:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def calculate_sla_metrics(self, logs_df):
        """SLA ë©”íŠ¸ë¦­ ê³„ì‚°"""
        sla_df = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute"),
                "service"
            ) \
            .agg(
                count("*").alias("total_requests"),
                sum(when(col("response_time") <= 1.0, 1).otherwise(0)).alias("fast_requests"),
                sum(when(col("status_code") == 200, 1).otherwise(0)).alias("successful_requests")
            ) \
            .withColumn("availability", col("successful_requests") / col("total_requests") * 100) \
            .withColumn("performance", col("fast_requests") / col("total_requests") * 100)
        
        return sla_df
    
    def calculate_business_metrics(self, logs_df):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ì‚¬ìš©ìë³„ í™œë™
        user_activity = logs_df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window("timestamp", "5 minutes"),
                "user_id"
            ) \
            .agg(
                count("*").alias("activity_count"),
                countDistinct("service").alias("services_used"),
                avg("response_time").alias("avg_response_time")
            )
        
        # ì„œë¹„ìŠ¤ë³„ ì¸ê¸°ë„
        service_popularity = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute"),
                "service"
            ) \
            .agg(
                count("*").alias("request_count"),
                countDistinct("user_id").alias("unique_users")
            )
        
        return user_activity, service_popularity
    
    def calculate_system_health(self, logs_df):
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ê³„ì‚°"""
        health_df = logs_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window("timestamp", "1 minute")
            ) \
            .agg(
                count("*").alias("total_requests"),
                countDistinct("service").alias("active_services"),
                countDistinct("user_id").alias("active_users"),
                avg("response_time").alias("avg_response_time"),
                sum(when(col("status_code") >= 400, 1).otherwise(0)).alias("error_count")
            ) \
            .withColumn("error_rate", col("error_count") / col("total_requests") * 100) \
            .withColumn("health_score", 
                when(col("error_rate") < 1, 100)
                .when(col("error_rate") < 5, 80)
                .when(col("error_rate") < 10, 60)
                .otherwise(40)
            )
        
        return health_df
```

### 4. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜

```python
# src/main.py
import os
import logging
from pyspark.sql import SparkSession
from log_processor import LogProcessor
from anomaly_detector import AnomalyDetector
from metrics_calculator import MetricsCalculator

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def main():
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        # Spark ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .appName("RealTimeLogAnalysis") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
            .getOrCreate()
        
        logger.info("Spark session created successfully")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        log_processor = LogProcessor(spark)
        anomaly_detector = AnomalyDetector(spark)
        metrics_calculator = MetricsCalculator(spark)
        
        # Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
        kafka_df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "logs") \
            .option("startingOffsets", "latest") \
            .load()
        
        # ë¡œê·¸ ì²˜ë¦¬
        processed_logs = log_processor.process_log_stream(kafka_df)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        response_metrics, error_rate, ip_patterns = log_processor.extract_metrics(processed_logs)
        sla_metrics = metrics_calculator.calculate_sla_metrics(processed_logs)
        user_activity, service_popularity = metrics_calculator.calculate_business_metrics(processed_logs)
        system_health = metrics_calculator.calculate_system_health(processed_logs)
        
        # ì´ìƒ íƒì§€
        response_anomalies = anomaly_detector.detect_response_time_anomalies(response_metrics)
        error_spikes = anomaly_detector.detect_error_rate_spikes(error_rate)
        suspicious_ips = anomaly_detector.detect_suspicious_ips(ip_patterns)
        ddos_attacks = anomaly_detector.detect_ddos_attacks(processed_logs)
        
        # ì¿¼ë¦¬ ì‹œì‘
        queries = []
        
        # ë©”íŠ¸ë¦­ ì¶œë ¥
        queries.append(
            response_metrics.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        queries.append(
            error_rate.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        queries.append(
            system_health.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        # ì´ìƒ íƒì§€ ì•Œë¦¼
        queries.append(
            response_anomalies.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        queries.append(
            error_spikes.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        queries.append(
            suspicious_ips.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .start()
        )
        
        # ëª¨ë“  ì¿¼ë¦¬ ëŒ€ê¸°
        for query in queries:
            query.awaitTermination()
        
    except Exception as e:
        logger.error(f"Application failed with error: {str(e)}")
        raise
    
    finally:
        if 'spark' in locals():
            spark.stop()

if __name__ == "__main__":
    main()
```

## ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
{
  "dashboard": {
    "title": "Real-time Log Analysis Dashboard",
    "panels": [
      {
        "title": "System Health Score",
        "type": "stat",
        "targets": [
          {
            "expr": "avg(health_score)",
            "legendFormat": "Health Score"
          }
        ]
      },
      {
        "title": "Response Time Trends",
        "type": "graph",
        "targets": [
          {
            "expr": "avg(avg_response_time) by (service)",
            "legendFormat": "{{service}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "avg(error_rate) by (service)",
            "legendFormat": "{{service}}"
          }
        ]
      },
      {
        "title": "Request Volume",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(request_count)",
            "legendFormat": "Total Requests"
          }
        ]
      },
      {
        "title": "Anomaly Alerts",
        "type": "table",
        "targets": [
          {
            "expr": "anomaly_count",
            "legendFormat": "Anomalies"
          }
        ]
      }
    ]
  }
}
```

### Prometheus ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°

```python
# ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜
def export_metrics_to_prometheus(metrics_df):
    """Prometheusë¡œ ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°"""
    import requests
    import time
    
    while True:
        # ìµœì‹  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        latest_metrics = metrics_df.collect()
        
        for metric in latest_metrics:
            # Prometheus ë©”íŠ¸ë¦­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            prometheus_metric = {
                'metric_name': 'spark_streaming_metric',
                'labels': {
                    'service': metric['service'],
                    'window': str(metric['window'])
                },
                'value': metric['avg_response_time'],
                'timestamp': int(time.time() * 1000)
            }
            
            # Prometheus Pushgatewayë¡œ ì „ì†¡
            requests.post(
                'http://localhost:9091/metrics/job/spark_streaming',
                data=prometheus_metric
            )
        
        time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ì „ì†¡
```

## âš¡ ì‹¤ì‹œê°„ ë¶„ì„ ì§€ì—°ì‹œê°„ ìµœì í™”

### ì§€ì—°ì‹œê°„ ë¶„ì„ ë„êµ¬

```python
# ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ë¶„ì„ ë° ìµœì í™” ë„êµ¬
class StreamingLatencyOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
        
    def analyze_processing_latency(self, streaming_df, latency_threshold_ms=1000):
        """ì²˜ë¦¬ ì§€ì—°ì‹œê°„ ë¶„ì„"""
        from pyspark.sql.functions import col, current_timestamp, unix_timestamp, when
        
        # ì´ë²¤íŠ¸ ì‹œê°„ê³¼ ì²˜ë¦¬ ì‹œê°„ ì‚¬ì´ì˜ ì§€ì—° ê³„ì‚°
        latency_df = streaming_df.withColumn(
            "processing_latency_ms",
            (unix_timestamp(current_timestamp()) - unix_timestamp("timestamp")) * 1000
        )
        
        # ì§€ì—°ì‹œê°„ í†µê³„ ê³„ì‚°
        latency_stats = latency_df.select(
            col("processing_latency_ms"),
            when(col("processing_latency_ms") > latency_threshold_ms, 1).otherwise(0).alias("is_slow")
        ).agg({
            "processing_latency_ms": "avg",
            "processing_latency_ms": "max",
            "processing_latency_ms": "min",
            "is_slow": "sum"
        }).collect()[0]
        
        total_records = latency_df.count()
        
        return {
            'avg_latency_ms': latency_stats[0],
            'max_latency_ms': latency_stats[1],
            'min_latency_ms': latency_stats[2],
            'slow_records_count': latency_stats[3],
            'slow_records_ratio': latency_stats[3] / total_records if total_records > 0 else 0,
            'total_records': total_records,
            'latency_threshold_ms': latency_threshold_ms
        }
    
    def optimize_streaming_configuration(self, current_config):
        """ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ìµœì í™”"""
        optimized_config = current_config.copy()
        
        # ë°°ì¹˜ ê°„ê²© ìµœì í™” (ì§€ì—°ì‹œê°„ì— ë”°ë¼ ì¡°ì •)
        if current_config.get('batch_interval_seconds', 10) > 5:
            optimized_config['batch_interval_seconds'] = 1  # 1ì´ˆë¡œ ë‹¨ì¶•
        
        # ë°±í”„ë ˆì…” ì„¤ì • ìµœì í™”
        optimized_config.update({
            'spark.sql.streaming.rateLimit.enabled': 'true',
            'spark.sql.streaming.rateLimit.maxOffsetsPerTrigger': '10000',
            'spark.sql.streaming.rateLimit.maxRecordsPerSecond': '5000'
        })
        
        # ì²´í¬í¬ì¸íŠ¸ ìµœì í™”
        optimized_config.update({
            'spark.sql.streaming.checkpointLocation': '/tmp/optimized_checkpoint',
            'spark.sql.streaming.minBatchesToRetain': '1',  # ìµœì†Œ ë°°ì¹˜ ìœ ì§€
            'spark.sql.streaming.stateStore.providerClass': 'org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider'
        })
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        optimized_config.update({
            'spark.sql.streaming.stateStore.maintenanceInterval': '10s',  # ìƒíƒœ ì •ë¦¬ ê°„ê²© ë‹¨ì¶•
            'spark.sql.streaming.stateStore.minDeltasForSnapshot': '10'   # ìŠ¤ëƒ…ìƒ· ìƒì„± ë¹ˆë„ ì¦ê°€
        })
        
        return optimized_config
    
    def implement_adaptive_batching(self, streaming_df, target_latency_ms=500):
        """ì ì‘í˜• ë°°ì¹­ êµ¬í˜„"""
        from pyspark.sql.functions import col, window, count, max as spark_max
        
        # ìœˆë„ìš° í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜
        def create_adaptive_window(df, base_window_size="10 seconds"):
            # í˜„ì¬ ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ìœˆë„ìš° í¬ê¸° ì¡°ì •
            current_load = self._get_system_load()
            
            if current_load > 0.8:  # ë†’ì€ ë¶€í•˜
                window_size = "30 seconds"
            elif current_load > 0.5:  # ì¤‘ê°„ ë¶€í•˜
                window_size = "20 seconds"
            else:  # ë‚®ì€ ë¶€í•˜
                window_size = "10 seconds"
            
            return df.withWatermark("timestamp", window_size)
        
        return create_adaptive_window(streaming_df)
    
    def _get_system_load(self):
        """ì‹œìŠ¤í…œ ë¶€í•˜ ì¸¡ì •"""
        status_tracker = self.spark.sparkContext.statusTracker()
        executor_infos = status_tracker.getExecutorInfos()
        
        if not executor_infos:
            return 0.0
        
        total_memory = sum(info.maxMemory for info in executor_infos)
        used_memory = sum(info.memoryUsed for info in executor_infos)
        
        return used_memory / total_memory if total_memory > 0 else 0.0
    
    def optimize_watermark_strategy(self, streaming_df, data_lateness_hours=2):
        """ì›Œí„°ë§ˆí¬ ì „ëµ ìµœì í™”"""
        from pyspark.sql.functions import col, window, count, max as spark_max
        
        # ë°ì´í„° ì§€ì—° íŒ¨í„´ì— ë”°ë¥¸ ì ì‘í˜• ì›Œí„°ë§ˆí¬
        watermark_delay = f"{data_lateness_hours} hours"
        
        # ì§€ì—°ì‹œê°„ì´ ê¸´ ë°ì´í„°ë¥¼ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
        optimized_df = streaming_df.withWatermark("timestamp", watermark_delay)
        
        return optimized_df
    
    def implement_latency_monitoring(self, streaming_df):
        """ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ êµ¬í˜„"""
        from pyspark.sql.functions import col, current_timestamp, unix_timestamp, window, count, avg
        
        # ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ ìƒì„±
        latency_metrics = streaming_df.withColumn(
            "processing_latency_ms",
            (unix_timestamp(current_timestamp()) - unix_timestamp("timestamp")) * 1000
        ).withWatermark("timestamp", "10 minutes").groupBy(
            window("timestamp", "1 minute"),
            "service"
        ).agg(
            count("*").alias("record_count"),
            avg("processing_latency_ms").alias("avg_latency_ms")
        )
        
        return latency_metrics

# ì§€ì—°ì‹œê°„ ìµœì í™” ì˜ˆì œ
def streaming_latency_optimization_example():
    spark = SparkSession.builder.appName("StreamingLatencyOptimization").getOrCreate()
    optimizer = StreamingLatencyOptimizer(spark)
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ìƒì„± (Kafkaì—ì„œ ì½ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •)
    streaming_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "logs") \
        .load()
    
    # JSON íŒŒì‹±
    from pyspark.sql.functions import col, from_json
    from pyspark.sql.types import StructType, StructField, StringType, TimestampType
    
    schema = StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("service", StringType(), True),
        StructField("message", StringType(), True)
    ])
    
    parsed_df = streaming_df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    # ì§€ì—°ì‹œê°„ ë¶„ì„
    latency_analysis = optimizer.analyze_processing_latency(parsed_df)
    print("=== Streaming Latency Analysis ===")
    print(f"Average Latency: {latency_analysis['avg_latency_ms']:.2f}ms")
    print(f"Max Latency: {latency_analysis['max_latency_ms']:.2f}ms")
    print(f"Slow Records Ratio: {latency_analysis['slow_records_ratio']:.2%}")
    
    # ì„¤ì • ìµœì í™”
    current_config = {
        'batch_interval_seconds': 10,
        'checkpoint_location': '/tmp/checkpoint'
    }
    
    optimized_config = optimizer.optimize_streaming_configuration(current_config)
    print("\n=== Optimized Configuration ===")
    for key, value in optimized_config.items():
        print(f"{key}: {value}")
    
    # ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
    latency_metrics = optimizer.implement_latency_monitoring(parsed_df)
    
    # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘
    query = latency_metrics.writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", False) \
        .start()
    
    return query
```

### ê³ ê¸‰ ì§€ì—°ì‹œê°„ ìµœì í™” ê¸°ë²•

```python
# ê³ ê¸‰ ì§€ì—°ì‹œê°„ ìµœì í™” í´ë˜ìŠ¤
class AdvancedLatencyOptimizer:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def implement_parallel_processing(self, streaming_df, parallelism_factor=2):
        """ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„"""
        # íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œ ë³‘ë ¬ ì²˜ë¦¬ í–¥ìƒ
        current_partitions = streaming_df.rdd.getNumPartitions()
        optimized_partitions = current_partitions * parallelism_factor
        
        return streaming_df.repartition(optimized_partitions)
    
    def optimize_memory_usage_for_low_latency(self, spark_session):
        """ì €ì§€ì—°ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        # ì €ì§€ì—° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì„¤ì •
        memory_configs = {
            'spark.sql.streaming.stateStore.maintenanceInterval': '5s',
            'spark.sql.streaming.stateStore.minDeltasForSnapshot': '5',
            'spark.sql.streaming.stateStore.compression.enabled': 'true',
            'spark.sql.streaming.stateStore.compression.codec': 'lz4',
            'spark.sql.streaming.stateStore.rocksdb.compression': 'true',
            'spark.sql.streaming.stateStore.rocksdb.blockSizeKB': '64',
            'spark.sql.streaming.stateStore.rocksdb.blockCacheSizeMB': '128'
        }
        
        for key, value in memory_configs.items():
            spark_session.conf.set(key, value)
        
        return memory_configs
    
    def implement_backpressure_control(self, streaming_df):
        """ë°±í”„ë ˆì…” ì œì–´ êµ¬í˜„"""
        from pyspark.sql.functions import col, lag, when
        
        # ë°ì´í„° ì²˜ë¦¬ ì†ë„ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì¡°ì ˆ
        backpressure_df = streaming_df.withColumn(
            "processing_rate",
            col("timestamp").cast("long") - lag(col("timestamp").cast("long"), 1).over(
                window(col("timestamp"), "1 minute")
            )
        ).withColumn(
            "should_throttle",
            when(col("processing_rate") > 1000, True).otherwise(False)  # 1ì´ˆë‹¹ 1000ê°œ ì´ìƒì´ë©´ ìŠ¤ë¡œí‹€ë§
        )
        
        return backpressure_df
    
    def optimize_serialization_for_low_latency(self, spark_session):
        """ì €ì§€ì—°ì„ ìœ„í•œ ì§ë ¬í™” ìµœì í™”"""
        serialization_configs = {
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            'spark.sql.execution.arrow.pyspark.enabled': 'true',
            'spark.sql.execution.arrow.maxRecordsPerBatch': '10000',
            'spark.sql.execution.arrow.pyspark.fallback.enabled': 'true'
        }
        
        for key, value in serialization_configs.items():
            spark_session.conf.set(key, value)
        
        return serialization_configs
    
    def implement_caching_for_streaming(self, streaming_df):
        """ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìºì‹± ì „ëµ"""
        from pyspark import StorageLevel
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹±
        cached_df = streaming_df.persist(StorageLevel.MEMORY_AND_DISK_SER)
        
        return cached_df

# ê³ ê¸‰ ì§€ì—°ì‹œê°„ ìµœì í™” ì˜ˆì œ
def advanced_latency_optimization_example():
    spark = SparkSession.builder.appName("AdvancedLatencyOptimization").getOrCreate()
    optimizer = AdvancedLatencyOptimizer(spark)
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ìƒì„±
    data = [(f"service_{i % 5}", f"message_{i}", time.time()) for i in range(1000)]
    df = spark.createDataFrame(data, ["service", "message", "timestamp"])
    
    # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
    parallel_df = optimizer.implement_parallel_processing(df)
    print(f"Parallel processing partitions: {parallel_df.rdd.getNumPartitions()}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    memory_configs = optimizer.optimize_memory_usage_for_low_latency(spark)
    print("\n=== Memory Optimization Configs ===")
    for key, value in memory_configs.items():
        print(f"{key}: {value}")
    
    # ì§ë ¬í™” ìµœì í™”
    serialization_configs = optimizer.optimize_serialization_for_low_latency(spark)
    print("\n=== Serialization Optimization Configs ===")
    for key, value in serialization_configs.items():
        print(f"{key}: {value}")
    
    # ìºì‹± ì „ëµ ì ìš©
    cached_df = optimizer.implement_caching_for_streaming(df)
    
    return cached_df
```

### ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
class RealTimeLatencyDashboard:
    def __init__(self, spark_session):
        self.spark = spark_session
        
    def create_latency_monitoring_dashboard(self, streaming_df):
        """ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        from pyspark.sql.functions import col, current_timestamp, unix_timestamp, window, count, avg, max as spark_max, min as spark_min
        
        # ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
        latency_metrics = streaming_df.withColumn(
            "processing_latency_ms",
            (unix_timestamp(current_timestamp()) - unix_timestamp("timestamp")) * 1000
        ).withWatermark("timestamp", "5 minutes").groupBy(
            window("timestamp", "30 seconds"),
            "service"
        ).agg(
            count("*").alias("record_count"),
            avg("processing_latency_ms").alias("avg_latency_ms"),
            spark_max("processing_latency_ms").alias("max_latency_ms"),
            spark_min("processing_latency_ms").alias("min_latency_ms")
        )
        
        # ì§€ì—°ì‹œê°„ ì•Œë¦¼ ìƒì„±
        alerts = latency_metrics.withColumn(
            "alert_level",
            when(col("avg_latency_ms") > 2000, "CRITICAL")
            .when(col("avg_latency_ms") > 1000, "WARNING")
            .when(col("avg_latency_ms") > 500, "INFO")
            .otherwise("OK")
        )
        
        return {
            'metrics': latency_metrics,
            'alerts': alerts
        }
    
    def export_latency_metrics_to_grafana(self, metrics_df):
        """Grafanaìš© ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°"""
        import requests
        import json
        import time
        
        def export_to_grafana():
            while True:
                try:
                    # ìµœì‹  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    latest_metrics = metrics_df.collect()
                    
                    for metric in latest_metrics:
                        # Grafana ë°ì´í„° í¬ì¸íŠ¸ í˜•ì‹
                        grafana_data = {
                            "time": int(time.time() * 1000),
                            "value": metric['avg_latency_ms'],
                            "tags": {
                                "service": metric['service'],
                                "window": str(metric['window'])
                            }
                        }
                        
                        # Grafana APIë¡œ ì „ì†¡
                        response = requests.post(
                            'http://localhost:3000/api/datasources/proxy/1/write',
                            json=grafana_data,
                            headers={'Content-Type': 'application/json'}
                        )
                        
                        if response.status_code == 200:
                            print(f"Latency metric exported: {metric['service']} - {metric['avg_latency_ms']:.2f}ms")
                        else:
                            print(f"Failed to export metric: {response.status_code}")
                    
                    time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì „ì†¡
                    
                except Exception as e:
                    print(f"Error exporting metrics: {e}")
                    time.sleep(60)  # ì—ëŸ¬ ì‹œ 1ë¶„ ëŒ€ê¸°
        
        return export_to_grafana
    
    def create_latency_alerting_system(self, alerts_df):
        """ì§€ì—°ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ ìƒì„±"""
        def process_alerts():
            while True:
                try:
                    # ìµœì‹  ì•Œë¦¼ ìˆ˜ì§‘
                    latest_alerts = alerts_df.collect()
                    
                    for alert in latest_alerts:
                        if alert['alert_level'] in ['WARNING', 'CRITICAL']:
                            # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
                            alert_message = {
                                'level': alert['alert_level'],
                                'service': alert['service'],
                                'avg_latency_ms': alert['avg_latency_ms'],
                                'max_latency_ms': alert['max_latency_ms'],
                                'timestamp': str(alert['window']),
                                'message': f"Service {alert['service']} has high latency: {alert['avg_latency_ms']:.2f}ms"
                            }
                            
                            # Slack, ì´ë©”ì¼, SMS ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
                            self._send_alert(alert_message)
                    
                    time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì•Œë¦¼ ì²´í¬
                    
                except Exception as e:
                    print(f"Error processing alerts: {e}")
                    time.sleep(120)  # ì—ëŸ¬ ì‹œ 2ë¶„ ëŒ€ê¸°
        
        return process_alerts
    
    def _send_alert(self, alert_message):
        """ì•Œë¦¼ ì „ì†¡ (Slack ì˜ˆì‹œ)"""
        import requests
        
        slack_webhook_url = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
        
        slack_message = {
            "text": f"ğŸš¨ Spark Streaming Latency Alert",
            "attachments": [
                {
                    "color": "danger" if alert_message['level'] == 'CRITICAL' else "warning",
                    "fields": [
                        {"title": "Service", "value": alert_message['service'], "short": True},
                        {"title": "Average Latency", "value": f"{alert_message['avg_latency_ms']:.2f}ms", "short": True},
                        {"title": "Max Latency", "value": f"{alert_message['max_latency_ms']:.2f}ms", "short": True},
                        {"title": "Alert Level", "value": alert_message['level'], "short": True}
                    ]
                }
            ]
        }
        
        try:
            response = requests.post(slack_webhook_url, json=slack_message)
            if response.status_code == 200:
                print(f"Alert sent successfully: {alert_message['message']}")
            else:
                print(f"Failed to send alert: {response.status_code}")
        except Exception as e:
            print(f"Error sending alert: {e}")

# ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì˜ˆì œ
def real_time_latency_monitoring_example():
    spark = SparkSession.builder.appName("RealTimeLatencyMonitoring").getOrCreate()
    dashboard = RealTimeLatencyDashboard(spark)
    
    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ìƒì„±
    data = [(f"service_{i % 3}", f"message_{i}", time.time()) for i in range(1000)]
    df = spark.createDataFrame(data, ["service", "message", "timestamp"])
    
    # ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±
    dashboard_data = dashboard.create_latency_monitoring_dashboard(df)
    
    # ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜
    export_function = dashboard.export_latency_metrics_to_grafana(dashboard_data['metrics'])
    
    # ì•Œë¦¼ ì²˜ë¦¬ í•¨ìˆ˜
    alert_function = dashboard.create_latency_alerting_system(dashboard_data['alerts'])
    
    print("=== Real-time Latency Monitoring Dashboard Created ===")
    print("Metrics and alerts are being processed in real-time")
    
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    # import threading
    # threading.Thread(target=export_function, daemon=True).start()
    # threading.Thread(target=alert_function, daemon=True).start()
    
    return dashboard_data
```

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Spark Streaming ê¸°ì´ˆ**
   - DStreamê³¼ ë§ˆì´í¬ë¡œ ë°°ì¹˜ ì²˜ë¦¬
   - ê¸°ë³¸ ë³€í™˜ ì—°ì‚°ê³¼ ìœˆë„ìš° ì—°ì‚°
   - ìƒíƒœ ìœ ì§€ ì—°ì‚°

2. **Structured Streaming**
   - ê³ ìˆ˜ì¤€ ìŠ¤íŠ¸ë¦¬ë° API
   - ì´ë²¤íŠ¸ ì‹œê°„ ì²˜ë¦¬
   - ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤

3. **Kafka ì—°ë™**
   - Kafka í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸ ì„¤ì •
   - ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„±ê³¼ ì²˜ë¦¬
   - JSON íŒŒì‹±ê³¼ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬

4. **ì›Œí„°ë§ˆí‚¹ê³¼ ì§€ì—° ë°ì´í„°**
   - ì›Œí„°ë§ˆí¬ ë©”ì»¤ë‹ˆì¦˜ ì´í•´
   - ì§€ì—° ë°ì´í„° ì²˜ë¦¬ ì „ëµ
   - ì ì‘í˜• ì›Œí„°ë§ˆí¬

5. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ
   - ì´ìƒ íƒì§€ì™€ ì•Œë¦¼
   - ë©”íŠ¸ë¦­ ê³„ì‚°ê³¼ ëª¨ë‹ˆí„°ë§

6. **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**
   - Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
   - Prometheus ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

7. **ì‹¤ì‹œê°„ ë¶„ì„ ì§€ì—°ì‹œê°„ ìµœì í™”**
   - ì§€ì—°ì‹œê°„ ë¶„ì„ ë„êµ¬
   - ê³ ê¸‰ ì§€ì—°ì‹œê°„ ìµœì í™” ê¸°ë²•
   - ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **Spark Streaming** | ë§ˆì´í¬ë¡œ ë°°ì¹˜ ì²˜ë¦¬ | â­â­â­â­ |
| **Structured Streaming** | ê³ ìˆ˜ì¤€ ìŠ¤íŠ¸ë¦¬ë° | â­â­â­â­â­ |
| **Kafka** | ë©”ì‹œì§€ ë¸Œë¡œì»¤ | â­â­â­â­â­ |
| **ì›Œí„°ë§ˆí‚¹** | ì§€ì—° ë°ì´í„° ì²˜ë¦¬ | â­â­â­â­ |
| **Grafana** | ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

**Part 4: ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ íŠœë‹**ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
- Spark UIì™€ ë©”íŠ¸ë¦­ ë¶„ì„
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ í”„ë¡œíŒŒì¼ë§
- ë©”ëª¨ë¦¬ ìµœì í™”ì™€ ìºì‹± ì „ëµ
- í´ëŸ¬ìŠ¤í„° íŠœë‹ê³¼ í™•ì¥ì„±

---

**ë‹¤ìŒ íŒŒíŠ¸**: [Part 4: ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ íŠœë‹](/data-engineering/2025/09/14/apache-spark-monitoring-tuning.html)

---

*ì´ì œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ê¹Œì§€ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë§ˆì§€ë§‰ íŒŒíŠ¸ì—ì„œëŠ” ì„±ëŠ¥ íŠœë‹ê³¼ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì™„ì„±ë„ë¥¼ ë†’ì´ê² ìŠµë‹ˆë‹¤.* ğŸš€

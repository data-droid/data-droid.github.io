---
layout: post
lang: ko
title: "Part 3: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤"
description: "Informer, Autoformer, FEDformer, PatchTST ë“± ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ê³  ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤."
date: 2025-09-06
author: Data Droid
category: data-ai
tags: [ì‹œê³„ì—´ì˜ˆì¸¡, íŠ¸ëœìŠ¤í¬ë¨¸, Informer, Autoformer, FEDformer, PatchTST, ë”¥ëŸ¬ë‹, AI]
series: time-series-forecasting
series_order: 3
reading_time: 15
difficulty: ì¤‘ê¸‰
---

# Part 3: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤

ì•ˆë…•í•˜ì„¸ìš”! ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œë¦¬ì¦ˆì˜ ì„¸ ë²ˆì§¸ íŒŒíŠ¸ì…ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í˜ëª…ì„ ì¼ìœ¼í‚¨ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ì•„í‚¤í…ì²˜ê°€ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì–´ë–»ê²Œ ì ìš©ë˜ì—ˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ“š í•™ìŠµ ëª©í‘œ

ì´ íŒŒíŠ¸ë¥¼ í†µí•´ ë‹¤ìŒì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì ìš©ëœ ë°°ê²½ê³¼ í•„ìš”ì„±
- Informer, Autoformer, FEDformer, PatchTST ë“± ì£¼ìš” ëª¨ë¸ë“¤ì˜ íŠ¹ì§•
- ê° ëª¨ë¸ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ë¶„ì•¼
- ì‹¤ì œ ë°ì´í„°ë¥¼ í™œìš©í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤ìŠµ

## ğŸ” íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì‹œê³„ì—´ì— ì ìš©ëœ ë°°ê²½

### ê¸°ì¡´ ë°©ë²•ë“¤ì˜ í•œê³„

**RNN/LSTMì˜ ë¬¸ì œì :**
- ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ì¸í•œ ê¸´ í•™ìŠµ ì‹œê°„
- ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì˜ ì–´ë ¤ì›€
- ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥

**CNNì˜ í•œê³„:**
- ì§€ì—­ì  íŒ¨í„´ì—ë§Œ ì§‘ì¤‘
- ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°© ì–´ë ¤ì›€

### íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¥ì 

1. **ë³‘ë ¬ ì²˜ë¦¬**: ëª¨ë“  ì‹œì ì„ ë™ì‹œì— ì²˜ë¦¬
2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: Self-attentionìœ¼ë¡œ ê¸´ ì‹œí€€ìŠ¤ì˜ ê´€ê³„ í•™ìŠµ
3. **í™•ì¥ì„±**: ë” í° ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥

## ğŸš€ ì£¼ìš” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸ë“¤

### 1. Informer (2021)

**í•µì‹¬ ì•„ì´ë””ì–´:**
- ProbSparse Self-attentionìœ¼ë¡œ ê³„ì‚° ë³µì¡ë„ O(LÂ²) â†’ O(L log L)ë¡œ ê°ì†Œ
- Self-attention Distillingìœ¼ë¡œ ê³„ì¸µë³„ ì •ë³´ ì••ì¶•
- Generative Decoderë¡œ í•œ ë²ˆì— ê¸´ ì‹œí€€ìŠ¤ ì˜ˆì¸¡

**ì£¼ìš” íŠ¹ì§•:**
```python
# Informerì˜ í•µì‹¬ êµ¬ì¡°
class Informer(nn.Module):
    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len):
        super(Informer, self).__init__()
        self.enc_in = enc_in
        self.dec_in = dec_in
        self.c_out = c_out
        self.seq_len = seq_len
        self.label_len = label_len
        self.out_len = out_len
        
        # ProbSparse Attention ì‚¬ìš©
        self.attn = ProbAttention(attention_dropout=0.1)
        
        # Encoderì™€ Decoder
        self.encoder = Encoder(...)
        self.decoder = Decoder(...)
```

**ì¥ì :**
- ê¸´ ì‹œí€€ìŠ¤ì—ì„œë„ íš¨ìœ¨ì 
- ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥

**ë‹¨ì :**
- ë³µì¡í•œ êµ¬ì¡°ë¡œ ì¸í•œ ê¸´ í•™ìŠµ ì‹œê°„
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì–´ë ¤ì›€

### 2. Autoformer (2021)

**í•µì‹¬ ì•„ì´ë””ì–´:**
- Auto-Correlation ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‹œê³„ì—´ì˜ ì£¼ê¸°ì„± ìë™ í•™ìŠµ
- Decomposition Blockìœ¼ë¡œ íŠ¸ë Œë“œì™€ ê³„ì ˆì„± ë¶„ë¦¬
- Series-wise Connectionìœ¼ë¡œ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”

**ì£¼ìš” íŠ¹ì§•:**
```python
# Autoformerì˜ Auto-Correlation
class AutoCorrelation(nn.Module):
    def forward(self, queries, keys, values):
        # ì‹œê³„ì—´ì˜ ì£¼ê¸°ì„±ì„ ì°¾ëŠ” ìƒê´€ê´€ê³„ ê³„ì‚°
        autocorr = self.autocorrelation(queries, keys)
        return self.value_projection(values) * autocorr
```

**ì¥ì :**
- ì‹œê³„ì—´ì˜ ì£¼ê¸°ì„± ìë™ í•™ìŠµ
- íŠ¸ë Œë“œì™€ ê³„ì ˆì„± ë¶„í•´ë¡œ í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ

**ë‹¨ì :**
- ì£¼ê¸°ì„±ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œëŠ” ì„±ëŠ¥ ì œí•œ
- ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì— ì–´ë ¤ì›€

### 3. FEDformer (2022)

**í•µì‹¬ ì•„ì´ë””ì–´:**
- Fourier Enhanced Decomposed Transformer
- FFTë¥¼ í™œìš©í•œ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œì˜ attention
- ëª¨ë¸ ì•™ìƒë¸”ë¡œ ì„±ëŠ¥ í–¥ìƒ

**ì£¼ìš” íŠ¹ì§•:**
```python
# FEDformerì˜ Fourier Attention
class FourierAttention(nn.Module):
    def forward(self, x):
        # FFTë¡œ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë³€í™˜
        x_freq = torch.fft.rfft(x, dim=-1)
        # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ attention ê³„ì‚°
        attn_freq = self.frequency_attention(x_freq)
        # ì—­ë³€í™˜ìœ¼ë¡œ ì‹œê°„ ë„ë©”ì¸ ë³µì›
        return torch.fft.irfft(attn_freq, dim=-1)
```

**ì¥ì :**
- ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œì˜ íš¨ìœ¨ì  ì²˜ë¦¬
- ë‹¤ì–‘í•œ ì£¼ê¸°ì„± íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

**ë‹¨ì :**
- FFT ê³„ì‚° ë¹„ìš©
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ì— ë¶€ì í•©í•  ìˆ˜ ìˆìŒ

### 4. PatchTST (2023)

**í•µì‹¬ ì•„ì´ë””ì–´:**
- ì‹œê³„ì—´ì„ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
- Channel Independenceë¡œ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì²˜ë¦¬
- ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥

**ì£¼ìš” íŠ¹ì§•:**
```python
# PatchTSTì˜ íŒ¨ì¹˜ ë¶„í• 
def create_patch(x, patch_len, stride):
    # ì‹œê³„ì—´ì„ íŒ¨ì¹˜ë¡œ ë¶„í• 
    patches = x.unfold(dim=-1, size=patch_len, step=stride)
    return patches.transpose(-1, -2)
```

**ì¥ì :**
- ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì¸ êµ¬ì¡°
- ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥
- ë¹ ë¥¸ í•™ìŠµê³¼ ì¶”ë¡ 

**ë‹¨ì :**
- íŒ¨ì¹˜ í¬ê¸°ì— ë¯¼ê°
- ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ì œí•œì 

## ğŸ› ï¸ ì‹¤ìŠµ: PatchTSTë¡œ ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡

ì´ì œ ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ PatchTST ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### 1. ë°ì´í„° ì¤€ë¹„

```python
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset

# ì£¼ì‹ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” yfinance ë“± ì‚¬ìš©)
def generate_stock_data(n_samples=1000, n_features=5):
    """ê°€ìƒì˜ ì£¼ì‹ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    
    # íŠ¸ë Œë“œì™€ ê³„ì ˆì„±ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
    t = np.linspace(0, 4*np.pi, n_samples)
    trend = 0.01 * t
    seasonal = 0.5 * np.sin(t) + 0.3 * np.sin(2*t)
    noise = 0.1 * np.random.randn(n_samples)
    
    # ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ìƒì„±
    data = np.zeros((n_samples, n_features))
    for i in range(n_features):
        data[:, i] = trend + seasonal + noise + i*0.1
    
    return pd.DataFrame(data, columns=[f'stock_{i+1}' for i in range(n_features)])

# ë°ì´í„° ìƒì„±
data = generate_stock_data()
print(f"ë°ì´í„° í˜•íƒœ: {data.shape}")
print(data.head())
```

### 2. PatchTST ëª¨ë¸ êµ¬í˜„

```python
class PatchTST(nn.Module):
    def __init__(self, seq_len, pred_len, patch_len, stride, n_features, d_model=128, n_heads=8, n_layers=3):
        super(PatchTST, self).__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.patch_len = patch_len
        self.stride = stride
        self.n_features = n_features
        self.d_model = d_model
        
        # íŒ¨ì¹˜ ê°œìˆ˜ ê³„ì‚°
        self.num_patches = (seq_len - patch_len) // stride + 1
        
        # ì…ë ¥ í”„ë¡œì ì…˜
        self.input_projection = nn.Linear(patch_len, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=n_heads, 
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.output_projection = nn.Linear(d_model, pred_len)
        
    def create_patches(self, x):
        """ì‹œê³„ì—´ì„ íŒ¨ì¹˜ë¡œ ë¶„í• """
        # x: (batch_size, n_features, seq_len)
        batch_size, n_features, seq_len = x.shape
        
        # ê° í”¼ì²˜ë³„ë¡œ íŒ¨ì¹˜ ìƒì„±
        patches = []
        for i in range(n_features):
            feature_patches = x[:, i, :].unfold(dim=-1, size=self.patch_len, step=self.stride)
            patches.append(feature_patches)
        
        # (batch_size, n_features, num_patches, patch_len)
        patches = torch.stack(patches, dim=1)
        return patches
    
    def forward(self, x):
        # x: (batch_size, n_features, seq_len)
        batch_size, n_features, seq_len = x.shape
        
        # íŒ¨ì¹˜ ìƒì„±
        patches = self.create_patches(x)  # (batch_size, n_features, num_patches, patch_len)
        
        # ê° í”¼ì²˜ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (Channel Independence)
        outputs = []
        for i in range(n_features):
            feature_patches = patches[:, i, :, :]  # (batch_size, num_patches, patch_len)
            
            # ì…ë ¥ í”„ë¡œì ì…˜
            projected = self.input_projection(feature_patches)  # (batch_size, num_patches, d_model)
            
            # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
            projected = projected + self.pos_encoding
            
            # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”
            encoded = self.transformer(projected)  # (batch_size, num_patches, d_model)
            
            # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
            pooled = encoded.mean(dim=1)  # (batch_size, d_model)
            
            # ì¶œë ¥ í”„ë¡œì ì…˜
            output = self.output_projection(pooled)  # (batch_size, pred_len)
            outputs.append(output)
        
        # ëª¨ë“  í”¼ì²˜ì˜ ì¶œë ¥ì„ ê²°í•©
        final_output = torch.stack(outputs, dim=1)  # (batch_size, n_features, pred_len)
        return final_output

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
seq_len = 96      # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
pred_len = 24     # ì˜ˆì¸¡ ê¸¸ì´
patch_len = 16    # íŒ¨ì¹˜ ê¸¸ì´
stride = 8        # ìŠ¤íŠ¸ë¼ì´ë“œ
n_features = 5    # í”¼ì²˜ ìˆ˜

model = PatchTST(
    seq_len=seq_len,
    pred_len=pred_len,
    patch_len=patch_len,
    stride=stride,
    n_features=n_features
)

print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
```

### 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ

```python
def prepare_data(data, seq_len, pred_len, train_ratio=0.7, val_ratio=0.2):
    """ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ì¤€ë¹„"""
    # ì •ê·œí™”
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X, y = [], []
    for i in range(len(scaled_data) - seq_len - pred_len + 1):
        X.append(scaled_data[i:i+seq_len])
        y.append(scaled_data[i+seq_len:i+seq_len+pred_len])
    
    X = np.array(X)
    y = np.array(y)
    
    # í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
    n_train = int(len(X) * train_ratio)
    n_val = int(len(X) * val_ratio)
    
    X_train, y_train = X[:n_train], y[:n_train]
    X_val, y_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val]
    X_test, y_test = X[n_train+n_val:], y[n_train+n_val:]
    
    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler

# ë°ì´í„° ì¤€ë¹„
(X_train, y_train), (X_val, y_val), (X_test, y_test), scaler = prepare_data(
    data, seq_len, pred_len
)

print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}, {y_train.shape}")
print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}, {y_val.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, {y_test.shape}")

# DataLoader ìƒì„±
def create_dataloader(X, y, batch_size=32, shuffle=True):
    X_tensor = torch.FloatTensor(X).transpose(1, 2)  # (batch, features, seq_len)
    y_tensor = torch.FloatTensor(y).transpose(1, 2)  # (batch, features, pred_len)
    dataset = TensorDataset(X_tensor, y_tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

train_loader = create_dataloader(X_train, y_train, batch_size=32)
val_loader = create_dataloader(X_val, y_val, batch_size=32, shuffle=False)
```

### 4. ëª¨ë¸ í•™ìŠµ

```python
def train_model(model, train_loader, val_loader, epochs=50, lr=0.001):
    """ëª¨ë¸ í•™ìŠµ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        scheduler.step(val_loss)
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
    
    return train_losses, val_losses

# ëª¨ë¸ í•™ìŠµ
print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=100)
print("í•™ìŠµ ì™„ë£Œ!")
```

### 5. ê²°ê³¼ ì‹œê°í™”

```python
def plot_results(model, test_loader, scaler, n_samples=3):
    """ê²°ê³¼ ì‹œê°í™”"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    
    with torch.no_grad():
        for i, (batch_X, batch_y) in enumerate(test_loader):
            if i >= n_samples:
                break
                
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            predictions = model(batch_X)
            
            # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‹œê°í™”
            X_sample = batch_X[0].cpu().numpy().T  # (seq_len, n_features)
            y_true = batch_y[0].cpu().numpy().T    # (pred_len, n_features)
            y_pred = predictions[0].cpu().numpy().T # (pred_len, n_features)
            
            # ì—­ì •ê·œí™”
            X_sample = scaler.inverse_transform(X_sample)
            y_true = scaler.inverse_transform(y_true)
            y_pred = scaler.inverse_transform(y_pred)
            
            # ì‹œê°í™”
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            axes = axes.flatten()
            
            for j in range(min(5, n_features)):
                ax = axes[j]
                
                # ê³¼ê±° ë°ì´í„°
                ax.plot(range(seq_len), X_sample[:, j], 'b-', label='ê³¼ê±°', linewidth=2)
                
                # ì‹¤ì œ ë¯¸ë˜
                future_x = range(seq_len, seq_len + pred_len)
                ax.plot(future_x, y_true[:, j], 'g-', label='ì‹¤ì œ', linewidth=2)
                
                # ì˜ˆì¸¡
                ax.plot(future_x, y_pred[:, j], 'r--', label='ì˜ˆì¸¡', linewidth=2)
                
                ax.set_title(f'Stock {j+1}')
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            # ë§ˆì§€ë§‰ subplot ìˆ¨ê¸°ê¸°
            if n_features < 6:
                axes[-1].set_visible(False)
            
            plt.tight_layout()
            plt.show()

# ê²°ê³¼ ì‹œê°í™”
test_loader = create_dataloader(X_test, y_test, batch_size=1, shuffle=False)
plot_results(model, test_loader, scaler)

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss', color='blue')
plt.plot(val_losses, label='Validation Loss', color='red')
plt.title('í•™ìŠµ ê³¡ì„ ')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(train_losses[-20:], label='Train Loss (Last 20)', color='blue')
plt.plot(val_losses[-20:], label='Validation Loss (Last 20)', color='red')
plt.title('ìµœê·¼ í•™ìŠµ ê³¡ì„ ')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 6. ì„±ëŠ¥ í‰ê°€

```python
def evaluate_model(model, test_loader, scaler):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            predictions = model(batch_X)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(batch_y.cpu().numpy())
    
    # ì˜ˆì¸¡ê³¼ íƒ€ê²Ÿ ê²°í•©
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    # ì—­ì •ê·œí™”
    predictions = scaler.inverse_transform(predictions.transpose(0, 2, 1).reshape(-1, n_features))
    targets = scaler.inverse_transform(targets.transpose(0, 2, 1).reshape(-1, n_features))
    
    # MSE, MAE ê³„ì‚°
    mse = np.mean((predictions - targets) ** 2)
    mae = np.mean(np.abs(predictions - targets))
    rmse = np.sqrt(mse)
    
    print(f"í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    print(f"MSE: {mse:.6f}")
    print(f"MAE: {mae:.6f}")
    print(f"RMSE: {rmse:.6f}")
    
    return mse, mae, rmse

# ì„±ëŠ¥ í‰ê°€
mse, mae, rmse = evaluate_model(model, test_loader, scaler)
```

## ğŸ“Š ëª¨ë¸ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

### ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | ì¥ì  | ë‹¨ì  | ì ìš© ë¶„ì•¼ |
|------|------|------|-----------|
| **Informer** | ê¸´ ì‹œí€€ìŠ¤ íš¨ìœ¨ì , ê°•ë ¥í•œ ì„±ëŠ¥ | ë³µì¡í•œ êµ¬ì¡°, ê¸´ í•™ìŠµ ì‹œê°„ | ì¥ê¸° ì˜ˆì¸¡, ëŒ€ìš©ëŸ‰ ë°ì´í„° |
| **Autoformer** | ì£¼ê¸°ì„± ìë™ í•™ìŠµ, í•´ì„ ê°€ëŠ¥ | ì£¼ê¸°ì„± ì—†ëŠ” ë°ì´í„° ì œí•œ | ê³„ì ˆì„± ë°ì´í„°, ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ |
| **FEDformer** | ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì²˜ë¦¬, ì•™ìƒë¸” | FFT ê³„ì‚° ë¹„ìš© | ì‹ í˜¸ ì²˜ë¦¬, ì£¼ê¸°ì„± ë°ì´í„° |
| **PatchTST** | ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì , ë¹ ë¥¸ í•™ìŠµ | íŒ¨ì¹˜ í¬ê¸° ë¯¼ê° | ì‹¤ì‹œê°„ ì˜ˆì¸¡, ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ |

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

**1. ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì„ íƒ:**
- **ì£¼ê¸°ì„±ì´ ê°•í•œ ë°ì´í„°**: Autoformer, FEDformer
- **ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°**: Informer, PatchTST
- **ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´**: PatchTST, Informer
- **ì‹¤ì‹œê°„ ì˜ˆì¸¡**: PatchTST

**2. ë¦¬ì†ŒìŠ¤ ì œì•½ì— ë”°ë¥¸ ì„ íƒ:**
- **ì œí•œëœ ê³„ì‚° ìì›**: PatchTST
- **ì¶©ë¶„í•œ ìì›**: Informer, FEDformer
- **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**: PatchTST

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ì´ë²ˆ íŒŒíŠ¸ì—ì„œëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ”:

- **Part 4**: ìµœì‹  ìƒì„±í˜• AI ëª¨ë¸ë“¤ (TimeGPT, Lag-Llama, Moirai, Chronos)
- **Part 5**: ì‹¤ë¬´ ì ìš©ê³¼ MLOps (ëª¨ë¸ ë°°í¬, ëª¨ë‹ˆí„°ë§, A/B í…ŒìŠ¤íŠ¸)

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

1. **íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¥ì **: ë³‘ë ¬ ì²˜ë¦¬, ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ, í™•ì¥ì„±
2. **ëª¨ë¸ë³„ íŠ¹ì„±**: ê° ëª¨ë¸ë§ˆë‹¤ ê³ ìœ í•œ ê°•ì ê³¼ ì ìš© ë¶„ì•¼ê°€ ìˆìŒ
3. **ì‹¤ë¬´ ê³ ë ¤ì‚¬í•­**: ë°ì´í„° íŠ¹ì„±, ë¦¬ì†ŒìŠ¤ ì œì•½, ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤
4. **ì‹¤ìŠµì˜ ì¤‘ìš”ì„±**: ì´ë¡ ê³¼ ì½”ë“œë¥¼ í•¨ê»˜ í•™ìŠµí•˜ì—¬ ì‹¤ì œ ì ìš© ëŠ¥ë ¥ í–¥ìƒ

íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ë“¤ì€ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒíŠ¸ì—ì„œ ë”ìš± í¥ë¯¸ë¡œìš´ ìµœì‹  ëª¨ë¸ë“¤ì„ ë§Œë‚˜ë³´ì„¸ìš”!

---

## ğŸ”— ì‹œë¦¬ì¦ˆ ë„¤ë¹„ê²Œì´ì…˜

**â† ì´ì „**: [Part 2: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ - N-BEATSì™€ DeepAR](/data-ai/2025/09/01/time-series-deep-learning.html)

**ë‹¤ìŒ â†’**: [Part 4: ìµœì‹  ìƒì„±í˜• AI ëª¨ë¸ë“¤ - TimeGPT, Lag-Llama, Moirai, Chronos](/data-ai/2025/09/07/time-series-llm-models.html)

---

**ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°**: Part 4ì—ì„œëŠ” TimeGPT, Lag-Llama ë“± ìµœì‹  ìƒì„±í˜• AI ëª¨ë¸ë“¤ì´ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ğŸš€

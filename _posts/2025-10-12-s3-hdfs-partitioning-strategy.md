---
layout: post
title: "S3 vs HDFS íŒŒí‹°ì…”ë‹ ì „ëµ - í´ë¼ìš°ë“œ ì‹œëŒ€ì˜ ë°ì´í„° ë ˆì´í¬ ìµœì í™”"
description: "HDFS ì‹œì ˆì˜ yyyy/mm/dd íŒŒí‹°ì…”ë‹ì´ S3ì—ì„œëŠ” ì™œ ì„±ëŠ¥ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ”ì§€, ê·¸ë¦¬ê³  S3ì— ìµœì í™”ëœ íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì‹¤ì œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
excerpt: "HDFS ì‹œì ˆì˜ yyyy/mm/dd íŒŒí‹°ì…”ë‹ì´ S3ì—ì„œëŠ” ì™œ ì„±ëŠ¥ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ”ì§€, ê·¸ë¦¬ê³  S3ì— ìµœì í™”ëœ íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì‹¤ì œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ"
category: data-engineering
tags: [S3, HDFS, Partitioning, DataLake, CloudStorage, Spark, Athena, Performance, Optimization]
series: cloud-data-architecture
series_order: 1
date: 2025-10-12
author: Data Droid
lang: ko
reading_time: 50ë¶„
difficulty: ì¤‘ê¸‰
---

# ğŸ—„ï¸ S3 vs HDFS íŒŒí‹°ì…”ë‹ ì „ëµ - í´ë¼ìš°ë“œ ì‹œëŒ€ì˜ ë°ì´í„° ë ˆì´í¬ ìµœì í™”

> **"ê³¼ê±°ì˜ ëª¨ë²” ì‚¬ë¡€ê°€ í˜„ì¬ì˜ ì•ˆí‹°íŒ¨í„´ì´ ë  ìˆ˜ ìˆë‹¤"** - HDFSì—ì„œ S3ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë•Œ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  íŒŒí‹°ì…”ë‹ ì „ëµì˜ ë³€í™”

ë°ì´í„° ë ˆì´í¬ë¥¼ ì˜¨í”„ë ˆë¯¸ìŠ¤ HDFSì—ì„œ í´ë¼ìš°ë“œ S3ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ë©´ì„œ ë§ì€ íŒ€ë“¤ì´ ê¸°ì¡´ì˜ `yyyy/mm/dd` íŒŒí‹°ì…”ë‹ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” S3ì˜ ì•„í‚¤í…ì²˜ íŠ¹ì„±ìƒ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” HDFSì™€ S3ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ì ì„ ì´í•´í•˜ê³ , S3ì— ìµœì í™”ëœ íŒŒí‹°ì…”ë‹ ì „ëµê³¼ ì‹¤ì œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµë¥¼ í†µí•´ ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨

- [HDFS ì‹œëŒ€ì˜ íŒŒí‹°ì…”ë‹ ì „ëµ](#hdfs-ì‹œëŒ€ì˜-íŒŒí‹°ì…”ë‹-ì „ëµ)
- [S3ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ì ](#s3ì˜-ê·¼ë³¸ì ì¸-ì°¨ì´ì )
- [S3 íŒŒí‹°ì…”ë‹ ì•ˆí‹°íŒ¨í„´](#s3-íŒŒí‹°ì…”ë‹-ì•ˆí‹°íŒ¨í„´)
- [S3 ìµœì í™” íŒŒí‹°ì…”ë‹ ì „ëµ](#s3-ìµœì í™”-íŒŒí‹°ì…”ë‹-ì „ëµ)
- [ì‹¤ì œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ](#ì‹¤ì œ-ì¿¼ë¦¬-ì„±ëŠ¥-ë¹„êµ)
- [ì‹¤ë¬´ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](#ì‹¤ë¬´-ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ)
- [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

---

## ğŸ›ï¸ HDFS ì‹œëŒ€ì˜ íŒŒí‹°ì…”ë‹ ì „ëµ {#hdfs-ì‹œëŒ€ì˜-íŒŒí‹°ì…”ë‹-ì „ëµ}

### HDFS ì•„í‚¤í…ì²˜ ì´í•´

HDFSëŠ” **ê³„ì¸µì  íŒŒì¼ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

| **êµ¬ì„±ìš”ì†Œ** | **ì—­í• ** | **íŠ¹ì§•** |
|--------------|----------|----------|
| **NameNode** | ë©”íƒ€ë°ì´í„° ê´€ë¦¬ | ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥ |
| **DataNode** | ì‹¤ì œ ë°ì´í„° ì €ì¥ | ë¡œì»¬ ë””ìŠ¤í¬ ê¸°ë°˜ ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ |
| **Block** | ë°ì´í„° ë‹¨ìœ„ | 128MB ê¸°ë³¸ í¬ê¸°, ë³µì œë³¸ ìœ ì§€ |

### yyyy/mm/ddê°€ ì í•©í–ˆë˜ ì´ìœ 

```bash
# HDFSì˜ ì „í˜•ì ì¸ íŒŒí‹°ì…˜ êµ¬ì¡°
/data/events/
  â””â”€â”€ year=2024/
      â””â”€â”€ month=01/
          â””â”€â”€ day=15/
              â”œâ”€â”€ part-00000.parquet
              â”œâ”€â”€ part-00001.parquet
              â””â”€â”€ part-00002.parquet
```

#### **1. NameNode ë©”íƒ€ë°ì´í„° íš¨ìœ¨ì„±**
- **ë””ë ‰í† ë¦¬ êµ¬ì¡°**: íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ê° ë””ë ‰í† ë¦¬/íŒŒì¼ë‹¹ ~150 bytes
- **ê³„ì¸µ íƒìƒ‰**: O(log n) ì‹œê°„ ë³µì¡ë„ë¡œ ë¹ ë¥¸ ê²€ìƒ‰

#### **2. Hive íŒŒí‹°ì…˜ í”„ë£¨ë‹**
- **ë™ì  íŒŒí‹°ì…˜**: ë‚ ì§œë³„ ìë™ íŒŒí‹°ì…”ë‹
- **ë©”íƒ€ìŠ¤í† ì–´**: íŒŒí‹°ì…˜ ë©”íƒ€ë°ì´í„° ìºì‹±
- **ì¿¼ë¦¬ ìµœì í™”**: WHERE ì ˆë¡œ ë¶ˆí•„ìš”í•œ íŒŒí‹°ì…˜ ì œì™¸

```sql
-- Hiveì—ì„œ íš¨ìœ¨ì ì¸ ì¿¼ë¦¬
SELECT * FROM events
WHERE year = 2024 AND month = 1 AND day = 15;
-- NameNodeëŠ” ì¦‰ì‹œ í•´ë‹¹ ë””ë ‰í† ë¦¬ë¡œ ì´ë™ ê°€ëŠ¥
```

#### **3. ë¡œì»¬ ë””ìŠ¤í¬ íŠ¹ì„±**
- **ìˆœì°¨ ì½ê¸°**: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìˆœíšŒê°€ ë¹ ë¦„
- **ë¸”ë¡ ì§€ì—­ì„±**: DataNodeê°€ ë¡œì»¬ ë°ì´í„° ìš°ì„  ì²˜ë¦¬
- **ë„¤íŠ¸ì›Œí¬ ë¹„ìš©**: ìµœì†Œí™”

### HDFS íŒŒí‹°ì…”ë‹ ëª¨ë²” ì‚¬ë¡€

| **ì „ëµ** | **ì„¤ëª…** | **ì¥ì ** |
|----------|----------|----------|
| **ì‹œê°„ ê¸°ë°˜** | yyyy/mm/dd ë˜ëŠ” yyyy/mm/dd/hh | ì‹œê³„ì—´ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬ |
| **ê³„ì¸µì  êµ¬ì¡°** | ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ì²© ë””ë ‰í† ë¦¬ | ë©”íƒ€ë°ì´í„° êµ¬ì¡°í™” |
| **íŒŒí‹°ì…˜ ìˆ˜** | ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œë„ ê°€ëŠ¥ | NameNode ë©”ëª¨ë¦¬ë§Œ ì¶©ë¶„í•˜ë©´ OK |

---

## â˜ï¸ S3ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ì  {#s3ì˜-ê·¼ë³¸ì ì¸-ì°¨ì´ì }

### S3ëŠ” ê°ì²´ ìŠ¤í† ë¦¬ì§€ë‹¤

S3ëŠ” **íŒŒì¼ ì‹œìŠ¤í…œì´ ì•„ë‹Œ Key-Value ê°ì²´ ìŠ¤í† ë¦¬ì§€**ì…ë‹ˆë‹¤.

| **íŠ¹ì„±** | **HDFS** | **S3** |
|----------|----------|--------|
| **ì €ì¥ ë°©ì‹** | ê³„ì¸µì  íŒŒì¼ ì‹œìŠ¤í…œ | Flat namespace (Key-Value) |
| **ë””ë ‰í† ë¦¬** | ì‹¤ì œ ë””ë ‰í† ë¦¬ ì¡´ì¬ | ë””ë ‰í† ë¦¬ëŠ” ê°œë…ì  (Keyì˜ ì¼ë¶€) |
| **ë©”íƒ€ë°ì´í„°** | NameNode ë©”ëª¨ë¦¬ | ë¶„ì‚° ë©”íƒ€ë°ì´í„° ìŠ¤í† ì–´ |
| **ì ‘ê·¼ ë°©ì‹** | íŒŒì¼ ê²½ë¡œ | Object Key |
| **List ì—°ì‚°** | ë¹ ë¦„ (ë¡œì»¬ ë””ìŠ¤í¬) | ëŠë¦¼ (ë„¤íŠ¸ì›Œí¬ API í˜¸ì¶œ) |

### S3ì˜ ë‚´ë¶€ êµ¬ì¡°

```python
# S3ì—ì„œëŠ” "ë””ë ‰í† ë¦¬"ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
# ëª¨ë“  ê²ƒì´ Key-Value ìŒ
s3://bucket/data/events/year=2024/month=01/day=15/part-00000.parquet
# ìœ„ëŠ” ì‹¤ì œë¡œ í•˜ë‚˜ì˜ ê¸´ Keyì¼ ë¿
```

#### **S3 List ì—°ì‚°ì˜ ë¹„ìš©**

```python
import boto3

s3 = boto3.client('s3')

# yyyy/mm/dd êµ¬ì¡°ì—ì„œ íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì°¾ê¸°
# 1. year=2024 ë¦¬ìŠ¤íŠ¸ -> API í˜¸ì¶œ 1íšŒ
# 2. month=01 ë¦¬ìŠ¤íŠ¸ -> API í˜¸ì¶œ 1íšŒ  
# 3. day=15 ë¦¬ìŠ¤íŠ¸ -> API í˜¸ì¶œ 1íšŒ
# ì´ 3ë²ˆì˜ API í˜¸ì¶œ + ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ

response = s3.list_objects_v2(
    Bucket='my-bucket',
    Prefix='data/events/year=2024/month=01/day=15/'
)
```

### S3 ì„±ëŠ¥ íŠ¹ì„±

#### **1. Request Rate ì œí•œ**
- **Prefixë‹¹ ì²˜ë¦¬ëŸ‰**: 3,500 PUT/COPY/POST/DELETE, 5,500 GET/HEAD ìš”ì²­/ì´ˆ
- **ê¹Šì€ ë””ë ‰í† ë¦¬ êµ¬ì¡°**: ë™ì¼ Prefixë¡œ ì§‘ì¤‘ë˜ì–´ ë³‘ëª© ë°œìƒ
- **ì„±ëŠ¥ ì €í•˜**: ë§ì€ List ì—°ì‚° ì‹œ ê¸‰ê²©í•œ ì‘ë‹µ ì‹œê°„ ì¦ê°€

#### **2. List ì—°ì‚° ì˜¤ë²„í—¤ë“œ**

| **ì—°ì‚°** | **HDFS** | **S3** |
|----------|----------|--------|
| **ë‹¨ì¼ ë””ë ‰í† ë¦¬ List** | ~1ms (ë¡œì»¬) | ~100-300ms (ë„¤íŠ¸ì›Œí¬) |
| **ê¹Šì´ 3 íƒìƒ‰** | ~3ms | ~300-900ms |
| **1,000ê°œ ê°ì²´ List** | ~10ms | ~1-2ì´ˆ |

#### **3. Eventually Consistent íŠ¹ì„±**
- **ì“°ê¸° í›„ ì½ê¸°**: ìƒˆ ê°ì²´ëŠ” ì¦‰ì‹œ ì¼ê´€ì„± ë³´ì¥ (2020ë…„ 12ì›” ì´í›„)
- **ë®ì–´ì“°ê¸°/ì‚­ì œ**: ìµœì¢… ì¼ê´€ì„± (ì•½ê°„ì˜ ì§€ì—° ê°€ëŠ¥)
- **List ì—°ì‚°**: ìµœì‹  ë³€ê²½ì‚¬í•­ì´ ì¦‰ì‹œ ë°˜ì˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

---

## âš ï¸ S3 íŒŒí‹°ì…”ë‹ ì•ˆí‹°íŒ¨í„´ {#s3-íŒŒí‹°ì…”ë‹-ì•ˆí‹°íŒ¨í„´}

### ì•ˆí‹°íŒ¨í„´ #1: ê³¼ë„í•˜ê²Œ ê¹Šì€ ê³„ì¸µ êµ¬ì¡°

```bash
# ì•ˆí‹°íŒ¨í„´: HDFS ìŠ¤íƒ€ì¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
s3://bucket/data/events/
  â””â”€â”€ year=2024/
      â””â”€â”€ month=01/
          â””â”€â”€ day=15/
              â””â”€â”€ hour=10/
                  â”œâ”€â”€ part-00000.parquet
                  â””â”€â”€ part-00001.parquet
```

#### **ë¬¸ì œì **
- **List ì—°ì‚° í­ì¦**: ê° ë ˆë²¨ë§ˆë‹¤ API í˜¸ì¶œ í•„ìš”
- **ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ**: 4-5ë²ˆì˜ ì™•ë³µ ì‹œê°„ ëˆ„ì 
- **ì¿¼ë¦¬ ì§€ì—°**: Spark/Athenaê°€ íŒŒí‹°ì…˜ íƒìƒ‰ì— ê³¼ë„í•œ ì‹œê°„ ì†Œë¹„

#### **ì‹¤ì œ ì˜í–¥**

```python
# Sparkì—ì„œ íŒŒí‹°ì…˜ íƒìƒ‰ ì‹œê°„ ì¸¡ì •
import time

start = time.time()
df = spark.read.parquet("s3://bucket/data/events/year=2024/month=01/day=15/")
print(f"Partition discovery: {time.time() - start:.2f}s")
# ê²°ê³¼: Partition discovery: 5.43s (ê¹Šì€ êµ¬ì¡°)
# vs
# ê²°ê³¼: Partition discovery: 0.87s (ë‹¨ìˆœ êµ¬ì¡°)
```

### ì•ˆí‹°íŒ¨í„´ #2: Small Files ë¬¸ì œ

```bash
# ì•ˆí‹°íŒ¨í„´: ì‹œê°„ë³„ë¡œ ì‘ì€ íŒŒì¼ë“¤ ìƒì„±
s3://bucket/data/events/date=2024-01-15/
  â”œâ”€â”€ hour=00/
  â”‚   â”œâ”€â”€ part-00000.parquet (2MB)
  â”‚   â”œâ”€â”€ part-00001.parquet (1.5MB)
  â”‚   â””â”€â”€ part-00002.parquet (3MB)
  â”œâ”€â”€ hour=01/
  â”‚   â”œâ”€â”€ part-00000.parquet (2.3MB)
  â”‚   â””â”€â”€ part-00001.parquet (1.8MB)
  ...
```

#### **ë¬¸ì œì **
- **GET ìš”ì²­ í­ì¦**: ì‘ì€ íŒŒì¼ë§ˆë‹¤ ë³„ë„ HTTP ìš”ì²­
- **I/O ì˜¤ë²„í—¤ë“œ**: íŒŒì¼ ì˜¤í”ˆ/í´ë¡œì¦ˆ ë°˜ë³µ
- **ë©”íƒ€ë°ì´í„° ë¹„ìš©**: íŒŒì¼ ìˆ˜ Ã— ë©”íƒ€ë°ì´í„° í¬ê¸°
- **ì¿¼ë¦¬ ì„±ëŠ¥**: Spark executorê°€ ìˆ˜ë§ì€ íŒŒì¼ ì²˜ë¦¬

#### **Small Filesì˜ ì˜í–¥**

| **íŒŒì¼ í¬ê¸°** | **íŒŒì¼ ìˆ˜** | **ì´ ë°ì´í„°** | **Spark ì½ê¸° ì‹œê°„** | **S3 ë¹„ìš©** |
|---------------|-------------|---------------|---------------------|-------------|
| 128MB | 1,000ê°œ | 128GB | 45ì´ˆ | ê¸°ì¤€ |
| 10MB | 13,000ê°œ | 128GB | 4ë¶„ 20ì´ˆ | 1.8x |
| 1MB | 130,000ê°œ | 128GB | 12ë¶„ 35ì´ˆ | 3.2x |

### ì•ˆí‹°íŒ¨í„´ #3: Prefix Hotspot

```bash
# ì•ˆí‹°íŒ¨í„´: ë™ì¼ prefixì— ì§‘ì¤‘
s3://bucket/data/events/2024-01-15/
  â”œâ”€â”€ event-000001.parquet
  â”œâ”€â”€ event-000002.parquet
  â”œâ”€â”€ event-000003.parquet
  ...
  â””â”€â”€ event-999999.parquet
```

#### **ë¬¸ì œì **
- **Request Rate ì œí•œ**: ë™ì¼ prefixë¡œ ìš”ì²­ ì§‘ì¤‘
- **ì„±ëŠ¥ ì €í•˜**: 3,500/5,500 RPS í•œê³„ ë„ë‹¬
- **ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ**: ë¶„ì‚° ì½ê¸° ì„±ëŠ¥ ì €í•˜

---

## ğŸš€ S3 ìµœì í™” íŒŒí‹°ì…”ë‹ ì „ëµ {#s3-ìµœì í™”-íŒŒí‹°ì…”ë‹-ì „ëµ}

### ì „ëµ #1: ë‹¨ìˆœí•˜ê³  ì–•ì€ êµ¬ì¡°

```bash
# ìµœì í™”: yyyy-mm-dd ë˜ëŠ” yyyymmdd ë‹¨ì¼ ë ˆë²¨
s3://bucket/data/events/date=2024-01-15/
  â”œâ”€â”€ part-00000-uuid.snappy.parquet (128MB)
  â”œâ”€â”€ part-00001-uuid.snappy.parquet (128MB)
  â””â”€â”€ part-00002-uuid.snappy.parquet (128MB)
```

#### **ì¥ì **
- **List ì—°ì‚° ìµœì†Œí™”**: 1-2ë²ˆì˜ API í˜¸ì¶œ
- **ë¹ ë¥¸ íŒŒí‹°ì…˜ íƒìƒ‰**: ë„¤íŠ¸ì›Œí¬ ì™•ë³µ ê°ì†Œ
- **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥**: ì¼ê´€ëœ ì‘ë‹µ ì‹œê°„

#### **Spark ì„¤ì •**

```python
# Sparkì—ì„œ ë‹¨ìˆœ íŒŒí‹°ì…˜ êµ¬ì¡° ìƒì„±
df.write \
    .partitionBy("date") \
    .parquet("s3://bucket/data/events/")

# date ì»¬ëŸ¼ì„ yyyy-mm-dd í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
from pyspark.sql.functions import date_format

df = df.withColumn("date", date_format("timestamp", "yyyy-MM-dd"))
```

### ì „ëµ #2: ì ì ˆí•œ íŒŒì¼ í¬ê¸° ìœ ì§€

| **íŒŒì¼ í¬ê¸°** | **ê¶Œì¥ ì‚¬í•­** | **ì´ìœ ** |
|---------------|---------------|----------|
| **< 10MB** | âŒ ë„ˆë¬´ ì‘ìŒ | Small files ë¬¸ì œ |
| **10-64MB** | âš ï¸ ì‘ìŒ | ê°€ëŠ¥í•˜ë©´ ë” í¬ê²Œ |
| **64-256MB** | âœ… ìµœì  | ê¶Œì¥ ë²”ìœ„ |
| **256-512MB** | âœ… ì¢‹ìŒ | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì í•© |
| **> 512MB** | âš ï¸ í¼ | íŒŒì¼ë‹¹ ì²˜ë¦¬ ì‹œê°„ ì¦ê°€ |

#### **íŒŒì¼ í¬ê¸° ìµœì í™”**

```python
# Sparkì—ì„œ íŒŒì¼ í¬ê¸° ì œì–´
spark.conf.set("spark.sql.files.maxRecordsPerFile", 1000000)
spark.conf.set("spark.sql.files.maxPartitionBytes", 134217728)  # 128MB

# repartitionìœ¼ë¡œ íŒŒì¼ ìˆ˜ ì¡°ì •
df.repartition(100) \
    .write \
    .partitionBy("date") \
    .parquet("s3://bucket/data/events/")
```

#### **Compaction ì‘ì—…**

```python
# Small filesë¥¼ í° íŒŒì¼ë¡œ ë³‘í•©
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("S3 Compaction") \
    .getOrCreate()

# ê¸°ì¡´ ì‘ì€ íŒŒì¼ë“¤ ì½ê¸°
df = spark.read.parquet("s3://bucket/data/events/date=2024-01-15/")

# ì ì ˆí•œ íŒŒí‹°ì…˜ ìˆ˜ë¡œ ì¬ì‘ì„±
num_partitions = max(1, int(df.count() / 1000000))  # íŒŒí‹°ì…˜ë‹¹ 100ë§Œ ë ˆì½”ë“œ

df.repartition(num_partitions) \
    .write \
    .mode("overwrite") \
    .parquet("s3://bucket/data/events-compacted/date=2024-01-15/")
```

### ì „ëµ #3: Prefix ë¶„ì‚°

```bash
# ìµœì í™”: Prefixë¥¼ ë¶„ì‚°í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ 
s3://bucket/data/events/
  â”œâ”€â”€ date=2024-01-15/shard=0/
  â”‚   â”œâ”€â”€ part-00000.parquet
  â”‚   â””â”€â”€ part-00001.parquet
  â”œâ”€â”€ date=2024-01-15/shard=1/
  â”‚   â”œâ”€â”€ part-00000.parquet
  â”‚   â””â”€â”€ part-00001.parquet
  ...
```

#### **Shard ê¸°ë°˜ íŒŒí‹°ì…”ë‹**

```python
# hash ê¸°ë°˜ shard ìƒì„±
from pyspark.sql.functions import hash, abs, col

df = df.withColumn("shard", abs(hash(col("user_id"))) % 10)

df.write \
    .partitionBy("date", "shard") \
    .parquet("s3://bucket/data/events/")
```

### ì „ëµ #4: ë‚ ì§œ í˜•ì‹ ì„ íƒ

| **í˜•ì‹** | **ì˜ˆì‹œ** | **ì¥ë‹¨ì ** |
|----------|----------|------------|
| **yyyy/mm/dd** | `2024/01/15` | âŒ 3ë ˆë²¨, List ì—°ì‚° ë§ìŒ |
| **yyyy-mm-dd** | `2024-01-15` | âœ… 1ë ˆë²¨, ê°€ë…ì„± ì¢‹ìŒ |
| **yyyymmdd** | `20240115` | âœ… 1ë ˆë²¨, ê°„ê²°í•¨ |
| **yyyy-mm** | `2024-01` | âš ï¸ ì›”ë³„ ì§‘ê³„ìš© |

#### **ë‚ ì§œ íŒŒí‹°ì…˜ ìƒì„±**

```python
from pyspark.sql.functions import date_format

# yyyy-mm-dd í˜•ì‹ (ê¶Œì¥)
df = df.withColumn("date", date_format("event_time", "yyyy-MM-dd"))

# yyyymmdd í˜•ì‹ (ë” ê°„ê²°)
df = df.withColumn("date", date_format("event_time", "yyyyMMdd"))

# íŒŒí‹°ì…”ë‹
df.write \
    .partitionBy("date") \
    .parquet("s3://bucket/data/events/")
```

---

## ğŸ“Š ì‹¤ì œ ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ {#ì‹¤ì œ-ì¿¼ë¦¬-ì„±ëŠ¥-ë¹„êµ}

### í…ŒìŠ¤íŠ¸ í™˜ê²½

| **í•­ëª©** | **ì„¤ì •** |
|----------|----------|
| **ë°ì´í„° í¬ê¸°** | 1TB (10ì–µ ë ˆì½”ë“œ) |
| **ê¸°ê°„** | 365ì¼ |
| **Spark ë²„ì „** | 3.4.0 |
| **ì¸ìŠ¤í„´ìŠ¤** | r5.4xlarge Ã— 10 |
| **íŒŒì¼ í˜•ì‹** | Parquet (Snappy ì••ì¶•) |

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ

```sql
-- ì¿¼ë¦¬: íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„° ì¡°íšŒ
SELECT COUNT(*), AVG(amount)
FROM events
WHERE date = '2024-01-15';
```

#### **ì„±ëŠ¥ ë¹„êµ**

| **íŒŒí‹°ì…˜ êµ¬ì¡°** | **íŒŒì¼ ìˆ˜** | **íŒŒí‹°ì…˜ íƒìƒ‰** | **ë°ì´í„° ì½ê¸°** | **ì´ ì‹œê°„** | **ê°œì„ ìœ¨** |
|-----------------|-------------|-----------------|-----------------|-------------|------------|
| **yyyy/mm/dd** | 720ê°œ (2MB) | 5.4ì´ˆ | 48.3ì´ˆ | **53.7ì´ˆ** | - |
| **yyyy-mm-dd** | 24ê°œ (128MB) | 0.9ì´ˆ | 12.1ì´ˆ | **13.0ì´ˆ** | **4.1x** |
| **yyyymmdd** | 24ê°œ (128MB) | 0.8ì´ˆ | 12.2ì´ˆ | **13.0ì´ˆ** | **4.1x** |

#### **ìƒì„¸ ë©”íŠ¸ë¦­**

```python
# yyyy/mm/dd êµ¬ì¡° (ì•ˆí‹°íŒ¨í„´)
{
  "partition_discovery_ms": 5430,
  "s3_list_calls": 4,
  "s3_get_calls": 720,
  "network_latency_ms": 18200,
  "data_read_mb": 2880,
  "executor_time_s": 48.3
}

# yyyy-mm-dd êµ¬ì¡° (ìµœì í™”)
{
  "partition_discovery_ms": 870,
  "s3_list_calls": 2,
  "s3_get_calls": 24,
  "network_latency_ms": 2400,
  "data_read_mb": 3072,
  "executor_time_s": 12.1
}
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: 7ì¼ ë²”ìœ„ ì¡°íšŒ

```sql
-- ì¿¼ë¦¬: ìµœê·¼ 7ì¼ ë°ì´í„° ë¶„ì„
SELECT date, COUNT(*), SUM(amount)
FROM events
WHERE date BETWEEN '2024-01-15' AND '2024-01-21'
GROUP BY date;
```

#### **ì„±ëŠ¥ ë¹„êµ**

| **íŒŒí‹°ì…˜ êµ¬ì¡°** | **íŒŒì¼ ìˆ˜** | **íŒŒí‹°ì…˜ íƒìƒ‰** | **ë°ì´í„° ì½ê¸°** | **ì´ ì‹œê°„** | **ê°œì„ ìœ¨** |
|-----------------|-------------|-----------------|-----------------|-------------|------------|
| **yyyy/mm/dd** | 5,040ê°œ | 38.2ì´ˆ | 4ë¶„ 23ì´ˆ | **5ë¶„ 1ì´ˆ** | - |
| **yyyy-mm-dd** | 168ê°œ | 6.1ì´ˆ | 1ë¶„ 24ì´ˆ | **1ë¶„ 30ì´ˆ** | **3.3x** |
| **yyyy-mm-dd + shard** | 168ê°œ | 5.9ì´ˆ | 58.2ì´ˆ | **1ë¶„ 4ì´ˆ** | **4.7x** |

### ì‹œë‚˜ë¦¬ì˜¤ 3: ì „ì²´ í…Œì´ë¸” ìŠ¤ìº”

```sql
-- ì¿¼ë¦¬: ì „ì²´ ë°ì´í„° ì§‘ê³„ (íŒŒí‹°ì…˜ í”„ë£¨ë‹ ì—†ìŒ)
SELECT user_id, COUNT(*)
FROM events
GROUP BY user_id;
```

#### **ì„±ëŠ¥ ë¹„êµ**

| **íŒŒí‹°ì…˜ êµ¬ì¡°** | **íŒŒì¼ ìˆ˜** | **íŒŒí‹°ì…˜ íƒìƒ‰** | **ë°ì´í„° ì½ê¸°** | **ì´ ì‹œê°„** | **ê°œì„ ìœ¨** |
|-----------------|-------------|-----------------|-----------------|-------------|------------|
| **yyyy/mm/dd** | 262,800ê°œ | 6ë¶„ 32ì´ˆ | 24ë¶„ 18ì´ˆ | **30ë¶„ 50ì´ˆ** | - |
| **yyyy-mm-dd** | 8,760ê°œ | 1ë¶„ 48ì´ˆ | 18ë¶„ 52ì´ˆ | **20ë¶„ 40ì´ˆ** | **1.5x** |
| **yyyy-mm-dd (compacted)** | 4,380ê°œ | 52.3ì´ˆ | 15ë¶„ 23ì´ˆ | **16ë¶„ 15ì´ˆ** | **1.9x** |

### Athena ì¿¼ë¦¬ ë¹„ìš© ë¹„êµ

```sql
-- Athenaì—ì„œ ë™ì¼ ì¿¼ë¦¬ ì‹¤í–‰
SELECT *
FROM events
WHERE date = '2024-01-15';
```

| **íŒŒí‹°ì…˜ êµ¬ì¡°** | **ìŠ¤ìº” ë°ì´í„°** | **ì‹¤í–‰ ì‹œê°„** | **ë¹„ìš© (per query)** |
|-----------------|-----------------|---------------|----------------------|
| **yyyy/mm/dd** | 3.2 GB | 8.3ì´ˆ | $0.016 |
| **yyyy-mm-dd** | 3.0 GB | 2.1ì´ˆ | $0.015 |
| **íŒŒí‹°ì…˜ ì—†ìŒ** | 1,024 GB | 1ë¶„ 34ì´ˆ | $5.12 |

**ê°œì„  íš¨ê³¼**: íŒŒí‹°ì…˜ ìµœì í™”ë¡œ **74% ë¹„ìš© ì ˆê°** (íŒŒí‹°ì…˜ ì—†ìŒ ëŒ€ë¹„)

---

## ğŸ”§ ì‹¤ë¬´ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ {#ì‹¤ë¬´-ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ}

### 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ ë¶„ì„

```python
# ê¸°ì¡´ íŒŒí‹°ì…˜ êµ¬ì¡° ë¶„ì„
import boto3
from collections import defaultdict

s3 = boto3.client('s3')
paginator = s3.get_paginator('list_objects_v2')

bucket = 'my-bucket'
prefix = 'data/events/'

# íŒŒí‹°ì…˜ë³„ íŒŒì¼ ìˆ˜ì™€ í¬ê¸° ì§‘ê³„
stats = defaultdict(lambda: {"count": 0, "size": 0})

for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
    for obj in page.get('Contents', []):
        # íŒŒí‹°ì…˜ ì¶”ì¶œ (ì˜ˆ: year=2024/month=01/day=15)
        parts = obj['Key'].split('/')
        partition = '/'.join(parts[:-1])
        
        stats[partition]["count"] += 1
        stats[partition]["size"] += obj['Size']

# í†µê³„ ì¶œë ¥
for partition, data in sorted(stats.items()):
    avg_size_mb = data["size"] / data["count"] / 1024 / 1024
    print(f"{partition}: {data['count']} files, avg {avg_size_mb:.1f}MB")
```

#### **ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ**

```
year=2024/month=01/day=15: 720 files, avg 2.3MB  âŒ Small files ë¬¸ì œ
year=2024/month=01/day=16: 680 files, avg 2.5MB  âŒ
year=2024/month=01/day=17: 740 files, avg 2.1MB  âŒ

ê¶Œì¥: 128MB íŒŒì¼ë¡œ consolidation í•„ìš”
```

### 2ë‹¨ê³„: ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

| **ì‘ì—…** | **ì†Œìš” ì‹œê°„** | **ë‹¤ìš´íƒ€ì„** | **ìš°ì„ ìˆœìœ„** |
|----------|---------------|--------------|--------------|
| **íŒŒí‹°ì…˜ êµ¬ì¡° ì¬ì„¤ê³„** | 1-2ì£¼ | ì—†ìŒ | ë†’ìŒ |
| **Compaction ì‘ì—…** | ë°ì´í„° ì–‘ì— ë”°ë¼ | ì—†ìŒ | ë†’ìŒ |
| **ë³‘ë ¬ ë§ˆì´ê·¸ë ˆì´ì…˜** | 3-4ì£¼ | ì—†ìŒ | ì¤‘ê°„ |
| **ì¿¼ë¦¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì •** | 2-3ì£¼ | ê³„íšëœ ë°°í¬ | ë†’ìŒ |
| **ê²€ì¦ ë° ëª¨ë‹ˆí„°ë§** | 1-2ì£¼ | ì—†ìŒ | ë†’ìŒ |

### 3ë‹¨ê³„: ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import date_format, abs, hash, col

spark = SparkSession.builder \
    .appName("S3 Partition Migration") \
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic") \
    .getOrCreate()

# ê¸°ì¡´ ë°ì´í„° ì½ê¸°
source_path = "s3://bucket/data/events-old/year=*/month=*/day=*/"
df = spark.read.parquet(source_path)

# ìƒˆë¡œìš´ date ì»¬ëŸ¼ ìƒì„±
df = df.withColumn("date", date_format(col("event_time"), "yyyy-MM-dd"))

# Shard ì¶”ê°€ (ì„ íƒì‚¬í•­)
df = df.withColumn("shard", abs(hash(col("user_id"))) % 10)

# íŒŒì¼ í¬ê¸° ìµœì í™”
spark.conf.set("spark.sql.files.maxPartitionBytes", 134217728)  # 128MB

# ì ì ˆí•œ íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚°
total_size_gb = df.count() * 500 / 1024 / 1024 / 1024  # ë ˆì½”ë“œë‹¹ ~500 bytes
num_partitions = int(total_size_gb * 8)  # 128MB íŒŒì¼ ê¸°ì¤€

# ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ì €ì¥
df.repartition(num_partitions, "date") \
    .write \
    .mode("overwrite") \
    .partitionBy("date") \
    .parquet("s3://bucket/data/events-new/")
```

### 4ë‹¨ê³„: ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜

```python
# ë‚ ì§œë³„ë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
from datetime import datetime, timedelta

start_date = datetime(2024, 1, 1)
end_date = datetime(2024, 12, 31)
current_date = start_date

while current_date <= end_date:
    year = current_date.year
    month = current_date.month
    day = current_date.day
    date_str = current_date.strftime("%Y-%m-%d")
    
    print(f"Processing {date_str}...")
    
    # íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì½ê¸°
    old_path = f"s3://bucket/data/events-old/year={year}/month={month:02d}/day={day:02d}/"
    
    try:
        df = spark.read.parquet(old_path)
        df = df.withColumn("date", lit(date_str))
        
        # íŒŒì¼ í¬ê¸° ìµœì í™”
        num_files = max(1, int(df.count() / 1000000))  # íŒŒì¼ë‹¹ 100ë§Œ ë ˆì½”ë“œ
        
        df.repartition(num_files) \
            .write \
            .mode("overwrite") \
            .parquet(f"s3://bucket/data/events-new/date={date_str}/")
        
        print(f"âœ“ {date_str} completed")
    except Exception as e:
        print(f"âœ— {date_str} failed: {e}")
    
    current_date += timedelta(days=1)
```

### 5ë‹¨ê³„: ê²€ì¦

```python
# ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
def validate_migration(old_path, new_path, date):
    # ë ˆì½”ë“œ ìˆ˜ ë¹„êµ
    old_df = spark.read.parquet(f"{old_path}/year={date.year}/month={date.month:02d}/day={date.day:02d}/")
    new_df = spark.read.parquet(f"{new_path}/date={date.strftime('%Y-%m-%d')}/")
    
    old_count = old_df.count()
    new_count = new_df.count()
    
    # ì²´í¬ì„¬ ë¹„êµ (ìƒ˜í”Œë§)
    old_checksum = old_df.sample(0.01).selectExpr("sum(hash(*))").collect()[0][0]
    new_checksum = new_df.sample(0.01).selectExpr("sum(hash(*))").collect()[0][0]
    
    # ê²°ê³¼
    if old_count == new_count and old_checksum == new_checksum:
        print(f"âœ“ {date}: Valid ({old_count:,} records)")
        return True
    else:
        print(f"âœ— {date}: Invalid (old: {old_count:,}, new: {new_count:,})")
        return False

# ì „ì²´ ê¸°ê°„ ê²€ì¦
from datetime import datetime, timedelta

start_date = datetime(2024, 1, 1)
end_date = datetime(2024, 1, 31)
current_date = start_date

while current_date <= end_date:
    validate_migration(
        "s3://bucket/data/events-old",
        "s3://bucket/data/events-new",
        current_date
    )
    current_date += timedelta(days=1)
```

### 6ë‹¨ê³„: ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
# CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘
import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch')

# Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§
response = cloudwatch.get_metric_statistics(
    Namespace='AWS/Athena',
    MetricName='EngineExecutionTime',
    Dimensions=[
        {'Name': 'WorkGroup', 'Value': 'primary'}
    ],
    StartTime=datetime.now() - timedelta(days=7),
    EndTime=datetime.now(),
    Period=3600,
    Statistics=['Average', 'Maximum']
)

# ê²°ê³¼ ë¶„ì„
for datapoint in response['Datapoints']:
    print(f"{datapoint['Timestamp']}: "
          f"Avg={datapoint['Average']:.2f}s, "
          f"Max={datapoint['Maximum']:.2f}s")
```

### 7ë‹¨ê³„: ë¹„ìš© ìµœì í™”

#### **S3 Storage Class ì „í™˜**

```python
# ì˜¤ë˜ëœ íŒŒí‹°ì…˜ì„ Intelligent-Tieringìœ¼ë¡œ ì „í™˜
import boto3

s3 = boto3.client('s3')

def transition_old_partitions(bucket, prefix, days_old=90):
    cutoff_date = datetime.now() - timedelta(days=days_old)
    
    # Lifecycle policy ìƒì„±
    lifecycle_config = {
        'Rules': [
            {
                'Id': 'TransitionOldData',
                'Status': 'Enabled',
                'Prefix': prefix,
                'Transitions': [
                    {
                        'Days': days_old,
                        'StorageClass': 'INTELLIGENT_TIERING'
                    }
                ]
            }
        ]
    }
    
    s3.put_bucket_lifecycle_configuration(
        Bucket=bucket,
        LifecycleConfiguration=lifecycle_config
    )
    
    print(f"âœ“ Lifecycle policy applied: {days_old}+ days â†’ INTELLIGENT_TIERING")

transition_old_partitions('my-bucket', 'data/events/', 90)
```

#### **ë¹„ìš© ì ˆê° íš¨ê³¼**

| **í•­ëª©** | **ë§ˆì´ê·¸ë ˆì´ì…˜ ì „** | **ë§ˆì´ê·¸ë ˆì´ì…˜ í›„** | **ì ˆê°ë¥ ** |
|----------|---------------------|---------------------|------------|
| **S3 Storage** | $23,040/ì›” (1TB, Standard) | $20,736/ì›” (Intelligent-Tiering) | 10% |
| **S3 API ë¹„ìš©** | $1,200/ì›” (LIST/GET) | $360/ì›” | 70% |
| **Athena ìŠ¤ìº”** | $512/ì›” | $128/ì›” | 75% |
| **Spark ì»´í“¨íŒ…** | $4,800/ì›” | $3,200/ì›” | 33% |
| **ì´ ë¹„ìš©** | **$29,552/ì›”** | **$24,424/ì›”** | **17%** |

---

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### í•µì‹¬ í¬ì¸íŠ¸

1. **ì•„í‚¤í…ì²˜ ì´í•´ê°€ í•µì‹¬**
   - HDFS: ê³„ì¸µì  íŒŒì¼ ì‹œìŠ¤í…œ, NameNode ë©”íƒ€ë°ì´í„°
   - S3: Flat namespace ê°ì²´ ìŠ¤í† ë¦¬ì§€, List ì—°ì‚° ë¹„ìš©

2. **S3 ìµœì í™” ì „ëµ**
   - **ì–•ì€ êµ¬ì¡°**: yyyy-mm-dd ë‹¨ì¼ ë ˆë²¨
   - **í° íŒŒì¼**: 64-256MB ê¶Œì¥
   - **Prefix ë¶„ì‚°**: Request rate ì œí•œ íšŒí”¼

3. **ì„±ëŠ¥ ê°œì„  íš¨ê³¼**
   - **ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ**: 4.1x ë¹ ë¦„
   - **7ì¼ ë²”ìœ„ ì¡°íšŒ**: 4.7x ë¹ ë¦„ (shard ì‚¬ìš©)
   - **ë¹„ìš© ì ˆê°**: 17% ì ˆê°

4. **ë§ˆì´ê·¸ë ˆì´ì…˜ ëª¨ë²” ì‚¬ë¡€**
   - ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
   - ì² ì €í•œ ê²€ì¦
   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í˜„ì¬ íŒŒí‹°ì…˜ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ
- [ ] Small files ë¬¸ì œ íŒŒì•…
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìˆ˜ë¦½
- [ ] Compaction ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
- [ ] ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì •ì˜
- [ ] ì¿¼ë¦¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì •
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- [ ] ë¹„ìš© ìµœì í™” ì ìš©

### ì¶”ê°€ í•™ìŠµ ìë£Œ

- **AWS ê³µì‹ ë¬¸ì„œ**: S3 Performance Best Practices
- **Spark ìµœì í™”**: Adaptive Query Execution (AQE)
- **Parquet ìµœì í™”**: Row Group í¬ê¸°, Compression
- **Iceberg/Delta Lake**: í…Œì´ë¸” í¬ë§·ìœ¼ë¡œ íŒŒí‹°ì…”ë‹ ì¶”ìƒí™”

---

> **"ì˜¬ë°”ë¥¸ íŒŒí‹°ì…”ë‹ ì „ëµì€ ë‹¨ìˆœíˆ ì„±ëŠ¥ í–¥ìƒì´ ì•„ë‹Œ, ë¹„ìš© ì ˆê°ê³¼ ìš´ì˜ íš¨ìœ¨ì„±ê¹Œì§€ ê°œì„ í•©ë‹ˆë‹¤."**

HDFSì—ì„œ S3ë¡œì˜ ì „í™˜ì€ ë‹¨ìˆœí•œ ìŠ¤í† ë¦¬ì§€ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì•„ë‹™ë‹ˆë‹¤. ì•„í‚¤í…ì²˜ì˜ ê·¼ë³¸ì ì¸ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ê·¸ì— ë§ëŠ” ìµœì í™” ì „ëµì„ ì ìš©í•  ë•Œ, ì§„ì •í•œ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ë°ì´í„° ë ˆì´í¬ì˜ ê°€ì¹˜ë¥¼ ì‹¤í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

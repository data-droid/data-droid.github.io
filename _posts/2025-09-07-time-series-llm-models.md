---
layout: post
lang: ko
title: "Part 4: ìµœì‹  ìƒì„±í˜• AI ëª¨ë¸ë“¤ - TimeGPT, Lag-Llama, Moirai, Chronos"
description: "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í˜ì‹ ì ì¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ê³  ì‹¤ì œ êµ¬í˜„í•´ë´…ë‹ˆë‹¤."
date: 2025-09-07
author: Data Droid
category: data-ai
tags: [ì‹œê³„ì—´ì˜ˆì¸¡, LLM, TimeGPT, Lag-Llama, Moirai, Chronos, ìƒì„±í˜•AI, ëŒ€ê·œëª¨ì–¸ì–´ëª¨ë¸]
series: time-series-forecasting
series_order: 4
reading_time: "20ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 4: ìµœì‹  ìƒì„±í˜• AI ëª¨ë¸ë“¤ - TimeGPT, Lag-Llama, Moirai, Chronos

> ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í˜ì‹ ì ì¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ê³  ì‹¤ì œ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [LLM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥](#llm-ê¸°ë°˜-ì‹œê³„ì—´-ì˜ˆì¸¡ì˜-ë“±ì¥)
2. [TimeGPT: OpenAIì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸](#timegpt-openaiì˜-ì‹œê³„ì—´-ì˜ˆì¸¡-ëª¨ë¸)
3. [Lag-Llama: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆ](#lag-llama-ì˜¤í”ˆì†ŒìŠ¤-ëŒ€ì•ˆ)
4. [Moirai: ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡](#moirai-ë‹¤ì¤‘-ì‹œê³„ì—´-ì˜ˆì¸¡)
5. [Chronos: ë©”íƒ€ì˜ ì‹œê³„ì—´ ëª¨ë¸](#chronos-ë©”íƒ€ì˜-ì‹œê³„ì—´-ëª¨ë¸)
6. [ì‹¤ìŠµ: Lag-Llama êµ¬í˜„ ë° í™œìš©](#ì‹¤ìŠµ-lag-llama-êµ¬í˜„-ë°-í™œìš©)
7. [ëª¨ë¸ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ](#ëª¨ë¸-ë¹„êµ-ë°-ì„ íƒ-ê°€ì´ë“œ)
8. [ë‹¤ìŒ ë‹¨ê³„ ë° ë¯¸ë˜ ì „ë§](#ë‹¤ìŒ-ë‹¨ê³„-ë°-ë¯¸ë˜-ì „ë§)
9. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸš€ LLM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥ {#llm-ê¸°ë°˜-ì‹œê³„ì—´-ì˜ˆì¸¡ì˜-ë“±ì¥}

### ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ í•œê³„

Part 1-3ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ë“¤ì€ ê°ê°ì˜ ì¥ì ì´ ìˆì—ˆì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ ê³µí†µì ì¸ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤:

1. **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ì‹œê³„ì—´ íŒ¨í„´ì—ë§Œ ìµœì í™”
2. **ë°ì´í„° ì˜ì¡´ì„±**: ëŒ€ëŸ‰ì˜ ë„ë©”ì¸ë³„ í•™ìŠµ ë°ì´í„° í•„ìš”
3. **ì¼ë°˜í™” í•œê³„**: ìƒˆë¡œìš´ ì‹œê³„ì—´ ìœ í˜•ì— ëŒ€í•œ ì ì‘ ì–´ë ¤ì›€
4. **ë©€í‹°íƒœìŠ¤í‚¹ ë¶€ì¡±**: ë‹¤ì–‘í•œ ì˜ˆì¸¡ ì‘ì—…ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ì›€

### LLMì˜ í˜ì‹ ì  ì ‘ê·¼

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤:

- **ë²”ìš©ì„±**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì‹œê³„ì—´ ë°ì´í„° í•™ìŠµ
- **Few-shot Learning**: ì ì€ ë°ì´í„°ë¡œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥
- **ë©€í‹°íƒœìŠ¤í‚¹**: ì—¬ëŸ¬ ì˜ˆì¸¡ ì‘ì—…ì„ ë™ì‹œì— ìˆ˜í–‰
- **ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤**: ì§ê´€ì ì¸ ì§ˆì˜ ë° ì„¤ëª… ê°€ëŠ¥

### í•µì‹¬ ê¸°ìˆ ì  í˜ì‹ 

1. **Tokenization**: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
2. **Causal Attention**: ì‹œê³„ì—´ì˜ ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§
3. **Multi-scale Learning**: ë‹¤ì–‘í•œ ì‹œê°„ ìŠ¤ì¼€ì¼ì˜ íŒ¨í„´ í•™ìŠµ
4. **Instruction Following**: ìì—°ì–´ ì§€ì‹œì‚¬í•­ì— ë”°ë¥¸ ì˜ˆì¸¡

## ğŸ¤– TimeGPT: OpenAIì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸

### TimeGPTì˜ í•µì‹¬ íŠ¹ì§•

TimeGPTëŠ” OpenAIì—ì„œ ê°œë°œí•œ ìµœì´ˆì˜ ëŒ€ê·œëª¨ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤.

#### **1. ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨**

```
í›ˆë ¨ ë°ì´í„°: 1000ì–µ ê°œ ì´ìƒì˜ ì‹œê³„ì—´ ë°ì´í„° í¬ì¸íŠ¸
ë„ë©”ì¸: ê¸ˆìœµ, ì†Œë§¤, ì œì¡°ì—…, ì—ë„ˆì§€, ì˜ë£Œ ë“±
ì§€ë¦¬ì  ë²”ìœ„: ì „ ì„¸ê³„
ì‹œê°„ ë²”ìœ„: 2020ë…„-2024ë…„
```

#### **2. Zero-shot ì˜ˆì¸¡ ëŠ¥ë ¥**

TimeGPTëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì‹œê³„ì—´ì— ëŒ€í•´ ì¶”ê°€ í•™ìŠµ ì—†ì´ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì˜ˆì‹œ: ìƒˆë¡œìš´ ì‹œê³„ì—´ì— ëŒ€í•œ ì¦‰ì‹œ ì˜ˆì¸¡
forecast = timegpt.predict(
    series=unseen_timeseries,
    horizon=30,  # 30ì¼ ì˜ˆì¸¡
    freq="D"     # ì¼ë³„ ë°ì´í„°
)
```

#### **3. ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤**

```python
# ìì—°ì–´ë¡œ ì˜ˆì¸¡ ìš”ì²­
result = timegpt.predict_with_context(
    series=stock_prices,
    context="ì£¼ì‹ ê°€ê²©ì„ 1ì£¼ì¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”. ì‹œì¥ ë³€ë™ì„±ì„ ê³ ë ¤í•´ì„œìš”.",
    horizon=7
)
```

### TimeGPTì˜ ì•„í‚¤í…ì²˜

```
ì…ë ¥ ì‹œê³„ì—´ â†’ Tokenization â†’ Transformer Encoder â†’ Decoder â†’ ì˜ˆì¸¡ê°’
     â†“              â†“              â†“              â†“
  Raw Data    Time Tokens    Attention      Forecast
```

#### **Tokenization ì „ëµ**

1. **Value Binning**: ì—°ì†ê°’ì„ ì´ì‚° í† í°ìœ¼ë¡œ ë³€í™˜
2. **Temporal Encoding**: ì‹œê°„ ì •ë³´ë¥¼ í† í°ì— í¬í•¨
3. **Context Tokens**: ë©”íƒ€ë°ì´í„°ë¥¼ í† í°ìœ¼ë¡œ í‘œí˜„

## ğŸ¦™ Lag-Llama: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆ

### Lag-Llamaì˜ ë“±ì¥ ë°°ê²½

TimeGPTê°€ ìƒìš© ëª¨ë¸ì´ë©´ì„œ API ê¸°ë°˜ìœ¼ë¡œë§Œ ì œê³µë˜ëŠ” í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ Hugging Faceì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì…ë‹ˆë‹¤.

#### **1. ì˜¤í”ˆì†ŒìŠ¤ ì ‘ê·¼ì„±**

- **ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤**: ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì½”ë“œ ê³µê°œ
- **ë¡œì»¬ ì‹¤í–‰**: API ì˜ì¡´ì„± ì—†ì´ ë¡œì»¬ì—ì„œ ì‹¤í–‰
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: í•„ìš”ì— ë”°ë¼ ëª¨ë¸ ìˆ˜ì • ê°€ëŠ¥

#### **2. LLaMA ê¸°ë°˜ ì•„í‚¤í…ì²˜**

```
ì‹œê³„ì—´ ì…ë ¥ â†’ Lag Features â†’ LLaMA-7B â†’ ì˜ˆì¸¡ê°’
     â†“            â†“            â†“
  Raw Data    Lag Tokens    Transformer
```

#### **3. Lag Featuresì˜ í˜ì‹ **

Lag-LlamaëŠ” ì‹œê³„ì—´ì˜ ì§€ì—°ê°’(lag values)ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```python
# Lag Features ìƒì„± ì˜ˆì‹œ
def create_lag_features(series, max_lags=24):
    """ì‹œê³„ì—´ì—ì„œ ì§€ì—° íŠ¹ì„± ìƒì„±"""
    lags = []
    for lag in range(1, max_lags + 1):
        lags.append(series.shift(lag))
    return pd.concat(lags, axis=1)
```

### Lag-Llamaì˜ ì¥ì 

1. **íˆ¬ëª…ì„±**: ëª¨ë¸ êµ¬ì¡°ì™€ í•™ìŠµ ê³¼ì • ê³µê°œ
2. **ë¹„ìš© íš¨ìœ¨ì„±**: API ë¹„ìš© ì—†ì´ ì‚¬ìš© ê°€ëŠ¥
3. **ê°œì¸ì •ë³´ ë³´í˜¸**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ
4. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ë§ì¶¤í™” ê°€ëŠ¥

## ğŸŒŠ Moirai: ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡

### Moiraiì˜ í•µì‹¬ ê°œë…

MoiraiëŠ” ì—¬ëŸ¬ ì‹œê³„ì—´ì„ ë™ì‹œì— ì˜ˆì¸¡í•˜ëŠ” ë©€í‹°íƒœìŠ¤í‚¹ ëª¨ë¸ì…ë‹ˆë‹¤.

#### **1. ë©€í‹°íƒœìŠ¤í‚¹ ì•„í‚¤í…ì²˜**

```
ì‹œê³„ì—´ 1 â”
ì‹œê³„ì—´ 2 â”œâ”€â†’ Shared Encoder â†’ Task-specific Heads â†’ ì˜ˆì¸¡ê°’ë“¤
ì‹œê³„ì—´ 3 â”˜
```

#### **2. Cross-series Learning**

- **ê³µí†µ íŒ¨í„´ í•™ìŠµ**: ì—¬ëŸ¬ ì‹œê³„ì—´ ê°„ì˜ ê³µí†µ íŒ¨í„´ ë°œê²¬
- **ì§€ì‹ ì „ì´**: í•œ ì‹œê³„ì—´ì˜ í•™ìŠµì´ ë‹¤ë¥¸ ì‹œê³„ì—´ì— ë„ì›€
- **íš¨ìœ¨ì„±**: ë‹¨ì¼ ëª¨ë¸ë¡œ ì—¬ëŸ¬ ì‹œê³„ì—´ ì²˜ë¦¬

#### **3. Hierarchical Forecasting**

```
êµ­ê°€ ìˆ˜ì¤€ â”
ì§€ì—­ ìˆ˜ì¤€ â”œâ”€â†’ ê³„ì¸µì  ì˜ˆì¸¡
ë„ì‹œ ìˆ˜ì¤€ â”˜
```

## â° Chronos: ë©”íƒ€ì˜ ì‹œê³„ì—´ ëª¨ë¸

### Chronosì˜ í˜ì‹ ì  ì ‘ê·¼

ChronosëŠ” ë©”íƒ€ì—ì„œ ê°œë°œí•œ í† í° ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤.

#### **1. Token-based Forecasting**

ì‹œê³„ì—´ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì²˜ëŸ¼ ì²˜ë¦¬:

```python
# Chronos í† í°í™” ì˜ˆì‹œ
def tokenize_timeseries(series, vocab_size=512):
    """ì‹œê³„ì—´ì„ í† í°ìœ¼ë¡œ ë³€í™˜"""
    # ê°’ ì •ê·œí™”
    normalized = (series - series.mean()) / series.std()
    
    # ì–‘ìí™”
    quantized = np.digitize(normalized, 
                          np.linspace(-3, 3, vocab_size-1))
    
    return quantized
```

#### **2. Multi-scale Learning**

- **ë‹¨ê¸° íŒ¨í„´**: ì¼ë³„, ì£¼ë³„ ë³€ë™
- **ì¤‘ê¸° íŒ¨í„´**: ì›”ë³„, ë¶„ê¸°ë³„ íŠ¸ë Œë“œ
- **ì¥ê¸° íŒ¨í„´**: ì—°ê°„ ê³„ì ˆì„±, êµ¬ì¡°ì  ë³€í™”

#### **3. Instruction Tuning**

ìì—°ì–´ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ë‹¤ì–‘í•œ ì˜ˆì¸¡ ì‘ì—… ìˆ˜í–‰:

```python
# Chronos ì§€ì‹œì‚¬í•­ ì˜ˆì‹œ
instructions = [
    "ë‹¤ìŒ 7ì¼ì˜ ë§¤ì¶œì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”",
    "ê³„ì ˆì„±ì„ ê³ ë ¤í•œ ì›”ë³„ ì˜ˆì¸¡ì„ í•´ì£¼ì„¸ìš”",
    "ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ í•´ì£¼ì„¸ìš”"
]
```

## ğŸ›  ï¸ ì‹¤ìŠµ: Lag-Llama êµ¬í˜„ ë° í™œìš© {#ì‹¤ìŠµ-lag-llama-êµ¬í˜„-ë°-í™œìš©}

### 1. í™˜ê²½ ì„¤ì •

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# pip install transformers torch datasets evaluate
# pip install pandas numpy matplotlib seaborn
# pip install scikit-learn

import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ì„¤ì •
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
```

### 2. Lag-Llama ëª¨ë¸ ë¡œë“œ

```python
# Lag-Llama ëª¨ë¸ ë¡œë“œ
model_name = "time-series-foundation-models/Lag-Llama"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters())}")
print(f"ëª¨ë¸ êµ¬ì¡°: {model.config}")
```

### 3. ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„

```python
def generate_complex_multivariate_timeseries(n=2000, n_series=5, seed=42):
    """ë³µì¡í•œ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±"""
    np.random.seed(seed)
    
    # ì‹œê°„ ì¸ë±ìŠ¤
    t = np.arange(n)
    
    # ê³µí†µ íŠ¸ë Œë“œ
    common_trend = 0.02 * t + 0.0001 * t**2
    
    # ì‹œê³„ì—´ë³„ íŠ¹ì„±
    series_data = []
    for i in range(n_series):
        # ì‹œê³„ì—´ë³„ ê³ ìœ  íŒ¨í„´
        series_trend = common_trend + 0.01 * i * t
        series_seasonal = 10 * np.sin(2 * np.pi * t / 365 + i * np.pi/3)
        series_noise = np.random.normal(0, 1 + 0.1 * i, n)
        
        # ì‹œê³„ì—´ ê°„ ìƒê´€ê´€ê³„
        if i > 0:
            correlation = 0.3 * np.sin(2 * np.pi * t / 100 + i)
            series_data.append(series_trend + series_seasonal + series_noise + correlation)
        else:
            series_data.append(series_trend + series_seasonal + series_noise)
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(np.array(series_data).T, 
                     columns=[f'series_{i+1}' for i in range(n_series)],
                     index=pd.date_range('2020-01-01', periods=n, freq='D'))
    
    return df

# ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ìƒì„±
multivariate_ts = generate_complex_multivariate_timeseries(2000, 5)

# ì‹œê°í™”
plt.figure(figsize=(15, 10))
for i, col in enumerate(multivariate_ts.columns):
    plt.subplot(3, 2, i+1)
    plt.plot(multivariate_ts.index, multivariate_ts[col], alpha=0.7)
    plt.title(f'{col} ì‹œê³„ì—´')
    plt.ylabel('ê°’')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

print(f"ì‹œê³„ì—´ ê°œìˆ˜: {len(multivariate_ts.columns)}")
print(f"ë°ì´í„° ê¸¸ì´: {len(multivariate_ts)}")
print(f"ê°’ ë²”ìœ„: {multivariate_ts.min().min():.2f} ~ {multivariate_ts.max().max():.2f}")
```

### 4. Lag Features ìƒì„±

```python
def create_lag_features(data, max_lags=24, forecast_horizon=7):
    """Lag-Llamaìš© ì§€ì—° íŠ¹ì„± ìƒì„±"""
    lag_features = {}
    targets = {}
    
    for col in data.columns:
        series = data[col].values
        
        # ì§€ì—° íŠ¹ì„± ìƒì„±
        lags = []
        for lag in range(1, max_lags + 1):
            lag_values = np.roll(series, lag)
            lag_values[:lag] = np.nan  # ì²˜ìŒ lagê°œëŠ” NaN
            lags.append(lag_values)
        
        lag_features[col] = np.column_stack(lags)
        
        # íƒ€ê²Ÿ ìƒì„± (ë¯¸ë˜ ê°’)
        targets[col] = series[forecast_horizon:]
    
    return lag_features, targets

# Lag features ìƒì„±
max_lags = 24
forecast_horizon = 7
lag_features, targets = create_lag_features(multivariate_ts, max_lags, forecast_horizon)

print(f"Lag features í˜•íƒœ: {lag_features['series_1'].shape}")
print(f"Targets í˜•íƒœ: {targets['series_1'].shape}")
```

### 5. Lag-Llama ëª¨ë¸ êµ¬í˜„

```python
class LagLlamaPredictor(nn.Module):
    """Lag-Llama ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, input_dim, hidden_dim=512, output_dim=1, num_layers=6):
        super(LagLlamaPredictor, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # ì…ë ¥ ì„ë² ë”©
        self.input_embedding = nn.Linear(input_dim, hidden_dim)
        
        # Transformer ë ˆì´ì–´ë“¤
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_projection = nn.Linear(hidden_dim, output_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # ì…ë ¥ ì„ë² ë”©
        x = self.input_embedding(x)
        x = self.dropout(x)
        
        # Transformer ì¸ì½”ë”
        x = self.transformer(x)
        
        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©
        x = x[:, -1, :]  # (batch_size, hidden_dim)
        
        # ì¶œë ¥ ì˜ˆì¸¡
        output = self.output_projection(x)
        
        return output

# ëª¨ë¸ ìƒì„±
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = LagLlamaPredictor(
    input_dim=max_lags,
    hidden_dim=512,
    output_dim=1,
    num_layers=6
).to(device)

print(f"Lag-Llama ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters())}")
```

### 6. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ

```python
def prepare_training_data(lag_features, targets, train_ratio=0.8):
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    all_X = []
    all_y = []
    
    for col in lag_features.keys():
        X = lag_features[col]
        y = targets[col]
        
        # NaN ì œê±°
        valid_indices = ~np.isnan(X).any(axis=1)
        X = X[valid_indices]
        y = y[valid_indices]
        
        # ê¸¸ì´ ë§ì¶”ê¸°
        min_len = min(len(X), len(y))
        X = X[:min_len]
        y = y[:min_len]
        
        all_X.append(X)
        all_y.append(y)
    
    # ëª¨ë“  ì‹œê³„ì—´ ê²°í•©
    X = np.vstack(all_X)
    y = np.hstack(all_y)
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    split_idx = int(len(X) * train_ratio)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    return X_train, X_val, y_train, y_val

# ë°ì´í„° ì¤€ë¹„
X_train, X_val, y_train, y_val = prepare_training_data(lag_features, targets)

# PyTorch í…ì„œë¡œ ë³€í™˜
X_train = torch.FloatTensor(X_train).to(device)
X_val = torch.FloatTensor(X_val).to(device)
y_train = torch.FloatTensor(y_train).unsqueeze(1).to(device)
y_val = torch.FloatTensor(y_val).unsqueeze(1).to(device)

print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}, {y_train.shape}")
print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}, {y_val.shape}")

# í•™ìŠµ í•¨ìˆ˜
def train_model(model, X_train, y_train, X_val, y_val, epochs=100, lr=0.001):
    """ëª¨ë¸ í•™ìŠµ"""
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        optimizer.zero_grad()
        
        train_pred = model(X_train)
        train_loss = criterion(train_pred, y_train)
        train_loss.backward()
        optimizer.step()
        
        # ê²€ì¦
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val)
            val_loss = criterion(val_pred, y_val)
        
        train_losses.append(train_loss.item())
        val_losses.append(val_loss.item())
        
        # Learning rate ìŠ¤ì¼€ì¤„ë§
        scheduler.step(val_loss)
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss.item():.6f}, Val Loss: {val_loss.item():.6f}")
    
    return train_losses, val_losses

# ëª¨ë¸ í•™ìŠµ
print("=== Lag-Llama ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
train_losses, val_losses = train_model(model, X_train, y_train, X_val, y_val, epochs=100)

# í•™ìŠµ ê³¼ì • ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(train_losses, label='Training Loss', alpha=0.7)
plt.plot(val_losses, label='Validation Loss', alpha=0.7)
plt.title('Lag-Llama ëª¨ë¸ í•™ìŠµ ê³¼ì •')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 7. ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€

```python
def evaluate_model(model, X_test, y_test, model_name="Lag-Llama"):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    model.eval()
    with torch.no_grad():
        predictions = model(X_test)
        predictions = predictions.cpu().numpy().flatten()
        y_test = y_test.cpu().numpy().flatten()
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mse = mean_squared_error(y_test, predictions)
    mae = mean_absolute_error(y_test, predictions)
    rmse = np.sqrt(mse)
    
    print(f"\n{model_name} ì„±ëŠ¥:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    
    return predictions, (mse, mae, rmse)

# ì„±ëŠ¥ í‰ê°€
predictions, metrics = evaluate_model(model, X_val, y_val)

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 8))

# ì²« 200ê°œ ìƒ˜í”Œì˜ ì˜ˆì¸¡ vs ì‹¤ì œê°’
sample_size = 200
plt.subplot(2, 1, 1)
plt.plot(y_val[:sample_size].cpu().numpy(), label='ì‹¤ì œê°’', alpha=0.7)
plt.plot(predictions[:sample_size], label='Lag-Llama ì˜ˆì¸¡', alpha=0.7)
plt.title('Lag-Llama: ì˜ˆì¸¡ vs ì‹¤ì œê°’ (ì²« 200ê°œ ìƒ˜í”Œ)')
plt.xlabel('ìƒ˜í”Œ')
plt.ylabel('ê°’')
plt.legend()
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(2, 1, 2)
plt.scatter(y_val.cpu().numpy(), predictions, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('ì‹¤ì œê°’')
plt.ylabel('ì˜ˆì¸¡ê°’')
plt.title('ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 8. ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡

```python
def predict_multiple_series(model, multivariate_ts, max_lags=24, forecast_horizon=7):
    """ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡"""
    predictions = {}
    
    for col in multivariate_ts.columns:
        series = multivariate_ts[col].values
        
        # ìµœê·¼ ë°ì´í„°ë¡œ lag features ìƒì„±
        recent_data = series[-max_lags:]
        recent_data = recent_data.reshape(1, -1)
        
        # ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            X = torch.FloatTensor(recent_data).to(device)
            pred = model(X)
            predictions[col] = pred.cpu().numpy().flatten()[0]
    
    return predictions

# ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡
forecast_predictions = predict_multiple_series(model, multivariate_ts)

print("=== ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ ===")
for col, pred in forecast_predictions.items():
    print(f"{col}: {pred:.4f}")

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 10))
for i, (col, pred) in enumerate(forecast_predictions.items()):
    plt.subplot(3, 2, i+1)
    
    # ìµœê·¼ 100ì¼ ë°ì´í„°
    recent_data = multivariate_ts[col].tail(100)
    plt.plot(recent_data.index, recent_data.values, label='ê³¼ê±° ë°ì´í„°', alpha=0.7)
    
    # ì˜ˆì¸¡ê°’
    last_date = recent_data.index[-1]
    next_date = last_date + pd.Timedelta(days=1)
    plt.axhline(y=pred, color='red', linestyle='--', 
                label=f'ì˜ˆì¸¡ê°’: {pred:.2f}')
    
    plt.title(f'{col} - ë‹¤ìŒ ì˜ˆì¸¡ê°’: {pred:.2f}')
    plt.ylabel('ê°’')
    plt.legend()
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()
```

## ğŸ“Š ëª¨ë¸ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ {#ëª¨ë¸-ë¹„êµ-ë°-ì„ íƒ-ê°€ì´ë“œ}

### ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|------|-----------|
| **TimeGPT** | ìµœê³  ì„±ëŠ¥, ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤ | API ì˜ì¡´ì„±, ë¹„ìš© | ìƒìš© ì„œë¹„ìŠ¤, í”„ë¡œí† íƒ€ì´í•‘ |
| **Lag-Llama** | ì˜¤í”ˆì†ŒìŠ¤, ë¡œì»¬ ì‹¤í–‰ | ì„±ëŠ¥ ì œí•œ, ì„¤ì • ë³µì¡ | ì—°êµ¬, ê°œì¸ í”„ë¡œì íŠ¸ |
| **Moirai** | ë‹¤ì¤‘ ì‹œê³„ì—´, íš¨ìœ¨ì„± | ë³µì¡í•œ ì•„í‚¤í…ì²˜ | ëŒ€ê·œëª¨ ë‹¤ë³€ëŸ‰ ì˜ˆì¸¡ |
| **Chronos** | í† í° ê¸°ë°˜, ìœ ì—°ì„± | ë©”íƒ€ ì˜ì¡´ì„± | ì‹¤í—˜ì  ì—°êµ¬ |

### ì„ íƒ ê°€ì´ë“œ

#### **1. í”„ë¡œì íŠ¸ ê·œëª¨ë³„**

- **ì†Œê·œëª¨**: Lag-Llama (ì˜¤í”ˆì†ŒìŠ¤, ë¬´ë£Œ)
- **ì¤‘ê·œëª¨**: TimeGPT (API, ë¹ ë¥¸ êµ¬í˜„)
- **ëŒ€ê·œëª¨**: Moirai (ë‹¤ì¤‘ ì‹œê³„ì—´, íš¨ìœ¨ì„±)

#### **2. ìš”êµ¬ì‚¬í•­ë³„**

- **ì •í™•ë„ ìš°ì„ **: TimeGPT
- **ë¹„ìš© íš¨ìœ¨ì„±**: Lag-Llama
- **ë‹¤ì¤‘ ì‹œê³„ì—´**: Moirai
- **ì‹¤í—˜ì  ì—°êµ¬**: Chronos

#### **3. ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­**

- **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ**: Lag-Llama (ë¡œì»¬ ì‹¤í–‰)
- **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**: TimeGPT (API)
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: Lag-Llama (ì˜¤í”ˆì†ŒìŠ¤)
- **í™•ì¥ì„±**: Moirai (ë©€í‹°íƒœìŠ¤í‚¹)

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ë° ë¯¸ë˜ ì „ë§ {#ë‹¤ìŒ-ë‹¨ê³„-ë°-ë¯¸ë˜-ì „ë§}

### ê¸°ìˆ  ë°œì „ ë°©í–¥

1. **ë” í° ëª¨ë¸**: ìˆ˜ì¡° ê°œ íŒŒë¼ë¯¸í„°ì˜ ì‹œê³„ì—´ ëª¨ë¸
2. **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì‹œê³„ì—´ í†µí•©
3. **ì‹¤ì‹œê°„ í•™ìŠµ**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì—ì„œ ì§€ì†ì  í•™ìŠµ
4. **ì„¤ëª… ê°€ëŠ¥ì„±**: ì˜ˆì¸¡ ê·¼ê±°ì˜ ìì—°ì–´ ì„¤ëª…

### ì‹¤ë¬´ ì ìš© ê³ ë ¤ì‚¬í•­

1. **ë°ì´í„° í’ˆì§ˆ**: LLMë„ ê³ í’ˆì§ˆ ë°ì´í„° í•„ìš”
2. **ë¹„ìš© ê´€ë¦¬**: API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
3. **ëª¨ë¸ ì—…ë°ì´íŠ¸**: ì •ê¸°ì ì¸ ì¬í•™ìŠµ í•„ìš”
4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ì„±ëŠ¥ ì¶”ì 

### ë¯¸ë˜ ì—°êµ¬ ë°©í–¥

1. **Few-shot Learning**: ë” ì ì€ ë°ì´í„°ë¡œ í•™ìŠµ
2. **Transfer Learning**: ë„ë©”ì¸ ê°„ ì§€ì‹ ì „ì´
3. **Causal Learning**: ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ì˜ˆì¸¡
4. **Uncertainty Quantification**: ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **LLM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ë“±ì¥**
   - ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ í•œê³„
   - LLMì˜ í˜ì‹ ì  ì ‘ê·¼
   - í•µì‹¬ ê¸°ìˆ ì  í˜ì‹ 

2. **ì£¼ìš” ëª¨ë¸ë“¤**
   - **TimeGPT**: OpenAIì˜ ìƒìš© ëª¨ë¸
   - **Lag-Llama**: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆ
   - **Moirai**: ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡
   - **Chronos**: ë©”íƒ€ì˜ í† í° ê¸°ë°˜ ëª¨ë¸

3. **ì‹¤ì œ êµ¬í˜„**
   - Lag-Llama ëª¨ë¸ êµ¬í˜„
   - ë‹¤ì¤‘ ì‹œê³„ì—´ ì˜ˆì¸¡
   - ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ

### í•µì‹¬ ê°œë… ì •ë¦¬

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| **Tokenization** | ì‹œê³„ì—´ì„ í† í°ìœ¼ë¡œ ë³€í™˜ | â­â­â­â­â­ |
| **Few-shot Learning** | ì ì€ ë°ì´í„°ë¡œ í•™ìŠµ | â­â­â­â­â­ |
| **Multi-task Learning** | ì—¬ëŸ¬ ì‘ì—… ë™ì‹œ ìˆ˜í–‰ | â­â­â­â­ |
| **Zero-shot Prediction** | ì¶”ê°€ í•™ìŠµ ì—†ì´ ì˜ˆì¸¡ | â­â­â­â­â­ |

### ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­

1. **ëª¨ë¸ ì„ íƒ**: ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
2. **ë¹„ìš© ê´€ë¦¬**: API ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•
3. **ë°ì´í„° ì¤€ë¹„**: ê³ í’ˆì§ˆ ë°ì´í„°ì˜ ì¤‘ìš”ì„±
4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ì¶”ì 

---

## ğŸ”— ì‹œë¦¬ì¦ˆ ë„¤ë¹„ê²Œì´ì…˜

**â† ì´ì „**: [Part 3: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë“¤](/data-ai/2025/09/06/time-series-transformers.html)

**ë‹¤ìŒ â†’**: ì‹œë¦¬ì¦ˆ ì™„ë£Œ! ğŸ‰

---

**ì‹œë¦¬ì¦ˆ ì™„ë£Œ**: ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ì§„í™”ë¥¼ ARIMAë¶€í„° ìµœì‹  LLM ëª¨ë¸ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì´ì œ ë‹¤ì–‘í•œ ë„êµ¬ì™€ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì‹¤ë¬´ì—ì„œ ê°•ë ¥í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€

*ì´ ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ê³¼ê±°, í˜„ì¬, ë¯¸ë˜ë¥¼ ëª¨ë‘ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì ì„ ì´í•´í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ë¬´ ì—­ëŸ‰ì„ ê¸°ë¥¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤!* ğŸ¯

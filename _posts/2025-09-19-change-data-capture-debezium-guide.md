---
layout: post
lang: ko
title: "Part 1: Change Data Captureì™€ Debezium ì‹¤ì „ êµ¬í˜„ - ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”ì˜ ì™„ì„±"
description: "CDCì˜ í•µì‹¬ ê°œë…ë¶€í„° Debeziumì„ í™œìš©í•œ ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ êµ¬ì¶•ê¹Œì§€, ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ì˜ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-09-19
author: Data Droid
category: data-engineering
tags: [Change-Data-Capture, CDC, Debezium, Kafka, ì‹¤ì‹œê°„ë™ê¸°í™”, ì´ë²¤íŠ¸ë“œë¦¬ë¸, ë°ì´í„°íŒŒì´í”„ë¼ì¸, ìŠ¤í‚¤ë§ˆì§„í™”]
series: change-data-capture-complete-guide
series_order: 1
reading_time: "50ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 1: Change Data Captureì™€ Debezium ì‹¤ì „ êµ¬í˜„ - ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”ì˜ ì™„ì„±

> CDCì˜ í•µì‹¬ ê°œë…ë¶€í„° Debeziumì„ í™œìš©í•œ ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ êµ¬ì¶•ê¹Œì§€, ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ì˜ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Change Data Capture ê¸°ì´ˆ ê°œë…](#change-data-capture-ê¸°ì´ˆ-ê°œë…)
2. [Debezium ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê¸°ëŠ¥](#debezium-ì•„í‚¤í…ì²˜ì™€-í•µì‹¬-ê¸°ëŠ¥)
3. [Debezium ì»¤ë„¥í„° ì„¤ì •ê³¼ ìš´ì˜](#debezium-ì»¤ë„¥í„°-ì„¤ì •ê³¼-ìš´ì˜)
4. [ìŠ¤í‚¤ë§ˆ ì§„í™”ì™€ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬](#ìŠ¤í‚¤ë§ˆ-ì§„í™”ì™€-ìŠ¤í‚¤ë§ˆ-ë ˆì§€ìŠ¤íŠ¸ë¦¬)
5. [ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ê³¼ ë¼ìš°íŒ…](#ì‹¤ì‹œê°„-ë°ì´í„°-ë³€í™˜ê³¼-ë¼ìš°íŒ…)
6. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ë°ì´í„°-ë™ê¸°í™”-ì‹œìŠ¤í…œ)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ”„ Change Data Capture ê¸°ì´ˆ ê°œë… {#change-data-capture-ê¸°ì´ˆ-ê°œë…}

### CDCë€ ë¬´ì—‡ì¸ê°€?

Change Data Capture(CDC)ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì˜ ë³€ê²½ì‚¬í•­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ê³  ìº¡ì²˜í•˜ì—¬ ë‹¤ë¥¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì „íŒŒí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì „í†µì ì¸ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### ì „í†µì ì¸ ë°°ì¹˜ ì²˜ë¦¬ vs CDC

| íŠ¹ì„± | ë°°ì¹˜ ì²˜ë¦¬ | CDC ë°©ì‹ |
|------|-----------|----------|
| **ì§€ì—°ì‹œê°„** | ë†’ìŒ (ì‹œê°„/ì¼ ë‹¨ìœ„) | ë‚®ìŒ (ì´ˆ/ë¶„ ë‹¨ìœ„) |
| **ì²˜ë¦¬ëŸ‰** | ë†’ìŒ (ëŒ€ìš©ëŸ‰ ì¼ê´„ ì²˜ë¦¬) | ì¤‘ê°„ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼) |
| **ë³µì¡ì„±** | ë‚®ìŒ | ë†’ìŒ |
| **ì¼ê´€ì„±** | ìµœì¢… ì¼ê´€ì„± | ê°•í•œ ì¼ê´€ì„± ê°€ëŠ¥ |
| **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©** | ì£¼ê¸°ì  ë†’ìŒ | ì§€ì†ì  ì¤‘ê°„ |
| **ì‹¤ì‹œê°„ ì²˜ë¦¬** | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ |
| **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€** | â€¢ ETL íŒŒì´í”„ë¼ì¸<br>â€¢ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ êµ¬ì¶•<br>â€¢ ì¼ì¼/ì£¼ê°„ ë¦¬í¬íŠ¸<br>â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ | â€¢ ì‹¤ì‹œê°„ ë¶„ì„<br>â€¢ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜<br>â€¢ ë°ì´í„° ë ˆì´í¬ ì‹¤ì‹œê°„ ë™ê¸°í™”<br>â€¢ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ë°ì´í„° ë™ê¸°í™” |

### CDC ë„ì… ROI í‰ê°€

| í‰ê°€ ê¸°ì¤€ | ì ìˆ˜ | ì¡°ê±´ | ì„¤ëª… |
|-----------|------|------|------|
| **ë°ì´í„° ì‹ ì„ ë„** | 40ì  | ì‹¤ì‹œê°„/ë¶„ ë‹¨ìœ„ | ì¦‰ì‹œ ì²˜ë¦¬ í•„ìš” |
| **ë°ì´í„° ì‹ ì„ ë„** | 20ì  | ì‹œê°„ ë‹¨ìœ„ | ë¹ ë¥¸ ì²˜ë¦¬ í•„ìš” |
| **ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„** | 30ì  | 3ê°œ ì´ìƒ í¬ë¦¬í‹°ì»¬ í”„ë¡œì„¸ìŠ¤ | í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ |
| **ì»´í”Œë¼ì´ì–¸ìŠ¤** | 20ì  | ê·œì œ ìš”êµ¬ì‚¬í•­ ìˆìŒ | ê°ì‚¬ ì¶”ì  í•„ìš” |
| **ë°ì´í„° ë³¼ë¥¨** | 10ì  | ì¼ì¼ 1TB ì´ìƒ | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ |

#### ROI ê¶Œì¥ì‚¬í•­

| ì´ì  | ê¶Œì¥ì‚¬í•­ | ì ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------|----------|---------------|
| **70ì  ì´ìƒ** | ê°•ë ¥ ì¶”ì²œ | ì‹¤ì‹œê°„ ë¶„ì„, ê¸ˆìœµ ê±°ë˜, IoT ë°ì´í„° |
| **40-69ì ** | ê³ ë ¤ ê¶Œì¥ | ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­ |
| **40ì  ë¯¸ë§Œ** | ì‹ ì¤‘ ê²€í†  | ë°°ì¹˜ ì²˜ë¦¬ë¡œë„ ì¶©ë¶„í•œ ê²½ìš° |

```python
class BatchProcessingVsCDC:
    def calculate_roi_score(self, requirements):
        """CDC ë„ì… ROI ì ìˆ˜ ê³„ì‚°"""
        score = 0
        if requirements.get("max_latency") in ["realtime", "minutes"]:
            score += 40
        elif requirements.get("max_latency") == "hourly":
            score += 20
        
        if len(requirements.get("critical_processes", [])) > 3:
            score += 30
        
        if requirements.get("compliance", False):
            score += 20
        
        if "TB" in requirements.get("daily_volume", ""):
            score += 10
        
        return score
```

### ì£¼ìš” CDC ë„êµ¬ ë¹„êµ

| ë„êµ¬ | íƒ€ì… | ì§€ì› DB |
|------|------|---------|
| **Debezium** | Open Source | MySQL, PostgreSQL, MongoDB, SQL Server, Oracle, DB2 |
| **Kafka Connect** | Apache Kafka ìƒíƒœê³„ | JDBC í˜¸í™˜ ëª¨ë“  DB, Elasticsearch, HDFS, S3 |
| **Maxwell** | Open Source | MySQL |
| **AWS DMS** | Managed Service | MySQL, PostgreSQL, Oracle, SQL Server, MongoDB |

#### ì£¼ìš” ì¥ì  ë¹„êµ

| ë„êµ¬ | ì£¼ìš” ì¥ì  |
|------|-----------|
| **Debezium** | â€¢ í’ë¶€í•œ DB ì§€ì›<br>â€¢ Kafka ìƒíƒœê³„ í†µí•©<br>â€¢ ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì›<br>â€¢ í™•ì¥ì„±ê³¼ ì•ˆì •ì„± |
| **Kafka Connect** | â€¢ Kafka ë„¤ì´í‹°ë¸Œ í†µí•©<br>â€¢ í’ë¶€í•œ ì»¤ë„¥í„° ìƒíƒœê³„<br>â€¢ í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ |
| **Maxwell** | â€¢ ê°„ë‹¨í•œ ì„¤ì •<br>â€¢ MySQL íŠ¹í™” ìµœì í™”<br>â€¢ ê°€ë²¼ìš´ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© |
| **AWS DMS** | â€¢ ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤<br>â€¢ AWS ìƒíƒœê³„ í†µí•©<br>â€¢ ê³ ê°€ìš©ì„±<br>â€¢ ëª¨ë‹ˆí„°ë§ ë‚´ì¥ |

#### ì£¼ìš” ë‹¨ì ê³¼ ì‚¬ìš© ì‚¬ë¡€

| ë„êµ¬ | ì£¼ìš” ë‹¨ì  | ìµœì  ì‚¬ìš© ì‚¬ë¡€ |
|------|-----------|----------------|
| **Debezium** | â€¢ ì„¤ì • ë³µì¡ì„±<br>â€¢ Kafka ì˜ì¡´ì„±<br>â€¢ ìš´ì˜ ì˜¤ë²„í—¤ë“œ | â€¢ ëŒ€ê·œëª¨ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°<br>â€¢ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜<br>â€¢ ë°ì´í„° ë ˆì´í¬ ì‹¤ì‹œê°„ ë™ê¸°í™” |
| **Kafka Connect** | â€¢ JDBC ê¸°ë°˜ ì§€ì—°ì‹œê°„<br>â€¢ ë³µì¡í•œ ì„¤ì •<br>â€¢ ëª¨ë‹ˆí„°ë§ ë³µì¡ì„± | â€¢ Kafka ì¤‘ì‹¬ ì•„í‚¤í…ì²˜<br>â€¢ ë‹¤ì–‘í•œ ì‹œìŠ¤í…œ í†µí•©<br>â€¢ ê¸°ì¡´ ETL íŒŒì´í”„ë¼ì¸ í˜„ëŒ€í™” |
| **Maxwell** | â€¢ MySQLë§Œ ì§€ì›<br>â€¢ ì œí•œëœ í™•ì¥ì„±<br>â€¢ ì‘ì€ ì»¤ë®¤ë‹ˆí‹° | â€¢ MySQL ì „ìš© í™˜ê²½<br>â€¢ ê°„ë‹¨í•œ CDC ìš”êµ¬ì‚¬í•­<br>â€¢ í”„ë¡œí† íƒ€ì… ê°œë°œ |
| **AWS DMS** | â€¢ AWS ë²¤ë” ë½ì¸<br>â€¢ ë¹„ìš©<br>â€¢ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œ | â€¢ AWS ì¤‘ì‹¬ ì•„í‚¤í…ì²˜<br>â€¢ ê´€ë¦¬ ë¶€ë‹´ ìµœì†Œí™”<br>â€¢ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ |

### CDC ë„êµ¬ ì„ íƒ ê¸°ì¤€

| ì¡°ê±´ | ê¶Œì¥ ë„êµ¬ | ì„ íƒ ì´ìœ  |
|------|-----------|-----------|
| **MySQL + ë‚®ì€ ë³µì¡ì„±** | Maxwell | ê°„ë‹¨í•œ ì„¤ì •, MySQL íŠ¹í™” ìµœì í™” |
| **AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤** | AWS DMS | ì™„ì „ ê´€ë¦¬í˜•, AWS ìƒíƒœê³„ í†µí•© |
| **ëŒ€ê·œëª¨ + ê³ ë³µì¡ì„±** | Debezium | í’ë¶€í•œ ê¸°ëŠ¥, í™•ì¥ì„±, ìŠ¤í‚¤ë§ˆ ì§„í™” |
| **ê¸°íƒ€ ê²½ìš°** | Kafka Connect | ë²”ìš©ì„±, ì»¤ë„¥í„° ìƒíƒœê³„ |

#### ì„ íƒ ê¸°ì¤€ë³„ ê°€ì¤‘ì¹˜

| ê¸°ì¤€ | ê°€ì¤‘ì¹˜ | ì„¤ëª… |
|------|--------|------|
| **ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…** | ë†’ìŒ | ì§€ì› ë²”ìœ„ì™€ ìµœì í™” ìˆ˜ì¤€ |
| **ê·œëª¨** | ë†’ìŒ | ì²˜ë¦¬ëŸ‰ê³¼ í™•ì¥ì„± ìš”êµ¬ì‚¬í•­ |
| **ë³µì¡ì„±** | ì¤‘ê°„ | ì„¤ì •ê³¼ ìš´ì˜ì˜ ë³µì¡ë„ |
| **ì˜ˆì‚°** | ì¤‘ê°„ | ë¼ì´ì„ ìŠ¤ ë¹„ìš©ê³¼ ê´€ë¦¬ ë¹„ìš© |
| **íŒ€ ì „ë¬¸ì„±** | ë‚®ìŒ | í•™ìŠµ ê³¡ì„ ê³¼ ìš´ì˜ ëŠ¥ë ¥ |

```python
class CDCToolsComparison:
    def select_optimal_tool(self, requirements):
        """ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìµœì  ë„êµ¬ ì„ íƒ"""
        if (requirements.get("database") == "mysql" and 
            requirements.get("complexity") == "low"):
            return "maxwell"
        elif requirements.get("budget") == "aws_managed":
            return "aws_dms"
        elif (requirements.get("scale") == "large" and 
              requirements.get("complexity") == "high"):
            return "debezium"
        else:
            return "kafka_connect"
```

## ğŸ”§ Debezium ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê¸°ëŠ¥ {#debezium-ì•„í‚¤í…ì²˜ì™€-í•µì‹¬-ê¸°ëŠ¥}

### Debezium ì•„í‚¤í…ì²˜ ê°œìš”

Debeziumì€ Apache Kafka Connect í”„ë ˆì„ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ì˜¤í”ˆì†ŒìŠ¤ CDC í”Œë«í¼ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë³€ê²½ì‚¬í•­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•˜ì—¬ Kafka í† í”½ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

Debeziumì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **Debezium Connectors**: ê° ë°ì´í„°ë² ì´ìŠ¤ë³„ë¡œ íŠ¹í™”ëœ ì»¤ë„¥í„°
- **Kafka Connect Framework**: ì»¤ë„¥í„° ì‹¤í–‰ ë° ê´€ë¦¬ í”„ë ˆì„ì›Œí¬
- **Schema Registry**: ìŠ¤í‚¤ë§ˆ ì§„í™” ë° í˜¸í™˜ì„± ê´€ë¦¬
- **Change Data Capture Engine**: ë³€ê²½ì‚¬í•­ ê°ì§€ ë° ìº¡ì²˜ ì—”ì§„

### ë°ì´í„°ë² ì´ìŠ¤ë³„ ë³€ê²½ ê°ì§€ ë©”ì»¤ë‹ˆì¦˜

| ë°ì´í„°ë² ì´ìŠ¤ | ë³€ê²½ ê°ì§€ ë°©ì‹ | ì£¼ìš” ê¸°ëŠ¥ | ìŠ¤ëƒ…ìƒ· ëª¨ë“œ |
|--------------|----------------|-----------|-------------|
| **MySQL** | ë°”ì´ë„ˆë¦¬ ë¡œê·¸ (Binlog) | â€¢ GTID ì§€ì›<br>â€¢ ê¸€ë¡œë²Œ íŠ¸ëœì­ì…˜ ì‹ë³„ì | ì´ˆê¸° ìŠ¤ëƒ…ìƒ· + ì¦ë¶„ ë™ê¸°í™” |
| **PostgreSQL** | Write-Ahead Log (WAL) | â€¢ ë…¼ë¦¬ì  ë³µì œ ìŠ¬ë¡¯<br>â€¢ ë„¤ì´í‹°ë¸Œ ë³µì œ í”„ë¡œí† ì½œ | ì´ˆê¸° ìŠ¤ëƒ…ìƒ· + ìŠ¤íŠ¸ë¦¬ë° |
| **MongoDB** | Operations Log (Oplog) | â€¢ Change Streams (3.6+)<br>â€¢ ì¬ì‹œì‘ ì§€ì  ì¶”ì  | ì´ˆê¸° ìŠ¤ëƒ…ìƒ· + Oplog ì¶”ì  |

### Kafka Connect ì•„í‚¤í…ì²˜

| ëª¨ë“œ | ì„¤ëª… | ì£¼ìš” íŠ¹ì§• |
|------|------|-----------|
| **Distributed Mode** | ë¶„ì‚° ëª¨ë“œ | â€¢ ê³ ê°€ìš©ì„± ë³´ì¥<br>â€¢ ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥<br>â€¢ ì¥ì•  ë³µêµ¬ ìë™í™” |
| **Standalone Mode** | ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ | â€¢ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½<br>â€¢ ê°„ë‹¨í•œ ì„¤ì •<br>â€¢ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ |

### Kafka Topics êµ¬ì¡°

| í† í”½ íƒ€ì… | ìš©ë„ | ì£¼ìš” ë‚´ìš© |
|-----------|------|-----------|
| **change_events** | ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ì´ë²¤íŠ¸ | ì‹¤ì œ ë°ì´í„° ë³€ê²½ì‚¬í•­ |
| **schema_changes** | ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì´ë²¤íŠ¸ | í…Œì´ë¸”/ì»¬ëŸ¼ êµ¬ì¡° ë³€ê²½ |
| **heartbeat** | ì—°ê²° ìƒíƒœ í™•ì¸ | ì»¤ë„¥í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§ |
| **transaction_metadata** | íŠ¸ëœì­ì…˜ ë©”íƒ€ë°ì´í„° | íŠ¸ëœì­ì…˜ ê²½ê³„ ì •ë³´ |

```python
class DebeziumArchitecture:
    def create_debezium_configuration(self, database_type, connection_config):
        """Debezium ì„¤ì • ìƒì„±"""
        
        base_config = {
            "name": f"{database_type}-connector",
            "config": {
                "connector.class": f"io.debezium.connector.{database_type.title()}.MySqlConnector",
                "tasks.max": "1",
                "database.hostname": connection_config["host"],
                "database.port": connection_config["port"],
                "database.user": connection_config["user"],
                "database.password": connection_config["password"],
                "database.server.id": "184054",
                "topic.prefix": connection_config["server_name"],
                "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
                "schema.history.internal.kafka.topic": f"schema-changes.{connection_config['server_name']}",
                "include.schema.changes": "true",
                "snapshot.mode": "initial"
            }
        }
        
        if database_type == "mysql":
            base_config["config"].update({
                "connector.class": "io.debezium.connector.mysql.MySqlConnector",
                "database.server.id": "184054",
                "database.include.list": ",".join(connection_config.get("databases", [])),
                "table.include.list": ",".join(connection_config.get("tables", [])),
                "binlog.buffer.size": "8192",
                "max.batch.size": "2048",
                "max.queue.size": "8192",
                "poll.interval.ms": "1000",
                "snapshot.locking.mode": "minimal"
            })
        
        elif database_type == "postgresql":
            base_config["config"].update({
                "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
                "database.dbname": connection_config["database"],
                "database.slot.name": f"debezium_{connection_config['server_name']}",
                "plugin.name": "pgoutput",
                "publication.name": f"debezium_publication_{connection_config['server_name']}",
                "slot.drop.on.stop": "false",
                "publication.autocreate.mode": "filtered"
            })
        
        elif database_type == "mongodb":
            base_config["config"].update({
                "connector.class": "io.debezium.connector.mongodb.MongoDbConnector",
                "mongodb.hosts": connection_config["hosts"],
                "mongodb.name": connection_config["server_name"],
                "collection.include.list": ",".join(connection_config.get("collections", [])),
                "capture.mode": "change_streams_update_full",
                "snapshot.mode": "initial"
            })
        
        return base_config
```

### ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´

| íŒ¨í„´ | ê°œë… | ì£¼ìš” ì¥ì  | êµ¬í˜„ ë°©ì‹ |
|------|------|-----------|-----------|
| **Event Sourcing** | ëª¨ë“  ìƒíƒœ ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì €ì¥ | â€¢ ì™„ì „í•œ ë³€ê²½ ì´ë ¥ ì¶”ì <br>â€¢ ì‹œê°„ ì—¬í–‰ ê°€ëŠ¥<br>â€¢ ê°ì‚¬ ë¡œê·¸ ìë™ ìƒì„±<br>â€¢ ë¶„ì‚° ì‹œìŠ¤í…œ ê°„ ì¼ê´€ì„± | â€¢ Kafka í† í”½ì„ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ë¡œ í™œìš©<br>â€¢ ì£¼ê¸°ì  ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”<br>â€¢ ì½ê¸° ëª¨ë¸ ìƒì„±<br>â€¢ ì´ë²¤íŠ¸ ì¬ìƒìœ¼ë¡œ ìƒíƒœ ë³µì› |
| **CQRS** | ëª…ë ¹ê³¼ ì¡°íšŒì˜ ì±…ì„ ë¶„ë¦¬ | â€¢ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§<br>â€¢ ìµœì í™”ëœ ì½ê¸°/ì“°ê¸° ëª¨ë¸<br>â€¢ ë³µì¡ì„± ë¶„ë¦¬<br>â€¢ ì„±ëŠ¥ í–¥ìƒ | â€¢ Debeziumìœ¼ë¡œ ëª…ë ¹ ì´ë²¤íŠ¸ ìº¡ì²˜<br>â€¢ ì½ê¸° ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•<br>â€¢ ì´ë²¤íŠ¸ë¥¼ ì½ê¸° ëª¨ë¸ë¡œ ë³€í™˜<br>â€¢ ìµœì¢… ì¼ê´€ì„± ë³´ì¥ |
| **Saga** | ë¶„ì‚° íŠ¸ëœì­ì…˜ ê´€ë¦¬ | â€¢ ë¶„ì‚° í™˜ê²½ì—ì„œ íŠ¸ëœì­ì…˜ ì²˜ë¦¬<br>â€¢ ì¥ì•  ë³µêµ¬ ê°€ëŠ¥<br>â€¢ í™•ì¥ì„± ë³´ì¥ | â€¢ ì´ë²¤íŠ¸ ê¸°ë°˜ ì¡°ìœ¨ (Choreography)<br>â€¢ ì¤‘ì•™ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (Orchestration)<br>â€¢ ë³´ìƒ íŠ¸ëœì­ì…˜ ì²˜ë¦¬<br>â€¢ Saga ìƒíƒœ ê´€ë¦¬ |

```python
class EventStreamingPatterns:
    def implement_event_sourcing_pattern(self):
        """Event Sourcing íŒ¨í„´ êµ¬í˜„ ì˜ˆì œ"""
        
        # ì´ë²¤íŠ¸ ìŠ¤í† ì–´ ì„¤ì •
        event_store_config = {
            "kafka_topics": {
                "order_events": "orders.ecommerce.orders",
                "customer_events": "orders.ecommerce.customers",
                "product_events": "orders.ecommerce.products"
            },
            "event_serialization": "Avro",
            "snapshot_frequency": "every_1000_events",
            "retention_policy": "7_days"
        }
        
        # ì´ë²¤íŠ¸ ì¬ìƒ ì˜ˆì œ
        def replay_events_for_state(self, entity_id, from_timestamp):
            """íŠ¹ì • ì—”í‹°í‹°ì˜ ìƒíƒœë¥¼ ì´ë²¤íŠ¸ ì¬ìƒìœ¼ë¡œ ë³µì›"""
            events = self.get_events(entity_id, from_timestamp)
            state = {}
            for event in events:
                state = self.apply_event(state, event)
            return state
        
        return event_store_config
    
    def implement_cqrs_pattern(self):
        """CQRS íŒ¨í„´ êµ¬í˜„ ì˜ˆì œ"""
        
        # ëª…ë ¹ ì¸¡ (Command Side)
        command_side_config = {
            "write_model": {
                "database": "mysql_orders",
                "tables": ["orders", "order_items"],
                "optimization": "for_writes"
            },
            "event_publishing": {
                "debezium_connector": "mysql-orders-connector",
                "topics": ["orders.ecommerce.orders"]
            }
        }
        
        # ì¡°íšŒ ì¸¡ (Query Side)
        query_side_config = {
            "read_models": {
                "elasticsearch": {
                    "indexes": ["orders_read_model", "customers_read_model"],
                    "optimization": "for_reads",
                    "projections": ["order_summary", "customer_profile"]
                },
                "redis": {
                    "caches": ["order_cache", "customer_cache"],
                    "ttl": "1_hour"
                }
            }
        }
        
        return {
            "command_side": command_side_config,
            "query_side": query_side_config
        }
    
    def implement_saga_pattern(self):
        """Saga íŒ¨í„´ êµ¬í˜„ ì˜ˆì œ"""
        
        # Choreography íŒ¨í„´
        choreography_saga = {
            "order_processing_saga": {
                "steps": [
                    {
                        "service": "order_service",
                        "action": "create_order",
                        "compensation": "cancel_order",
                        "event": "OrderCreated"
                    },
                    {
                        "service": "inventory_service", 
                        "action": "reserve_inventory",
                        "compensation": "release_inventory",
                        "event": "InventoryReserved"
                    },
                    {
                        "service": "payment_service",
                        "action": "process_payment",
                        "compensation": "refund_payment",
                        "event": "PaymentProcessed"
                    }
                ],
                "compensation_strategy": "reverse_order"
            }
        }
        
        # Orchestration íŒ¨í„´
        orchestration_saga = {
            "saga_orchestrator": {
                "state_machine": {
                    "states": ["STARTED", "INVENTORY_RESERVED", "PAYMENT_PROCESSED", "COMPLETED"],
                    "transitions": {
                        "STARTED": "INVENTORY_RESERVED",
                        "INVENTORY_RESERVED": "PAYMENT_PROCESSED",
                        "PAYMENT_PROCESSED": "COMPLETED"
                    },
                    "compensation_states": ["INVENTORY_RELEASED", "PAYMENT_REFUNDED", "CANCELLED"]
                }
            }
        }
        
        return {
            "choreography": choreography_saga,
            "orchestration": orchestration_saga
        }
```

## âš™ï¸ Debezium ì»¤ë„¥í„° ì„¤ì •ê³¼ ìš´ì˜

### MySQL ì»¤ë„¥í„° ì„¤ì •

```python
class MySQLConnectorSetup:
    def __init__(self):
        self.config_templates = {}
    
    def setup_mysql_connector(self, environment_config):
        """MySQL ì»¤ë„¥í„° ì„¤ì •"""
        
        # MySQL ì„œë²„ ì„¤ì •
        mysql_server_config = {
            "binlog_format": "ROW",
            "binlog_row_image": "FULL",
            "log_bin": "mysql-bin",
            "server_id": environment_config["server_id"],
            "gtid_mode": "ON",
            "enforce_gtid_consistency": "ON"
        }
        
        # Debezium MySQL ì»¤ë„¥í„° ì„¤ì •
        connector_config = {
            "name": f"mysql-connector-{environment_config['server_name']}",
            "config": {
                "connector.class": "io.debezium.connector.mysql.MySqlConnector",
                "database.hostname": environment_config["mysql_host"],
                "database.port": environment_config["mysql_port"],
                "database.user": environment_config["mysql_user"],
                "database.password": environment_config["mysql_password"],
                "database.server.id": str(environment_config["server_id"]),
                "topic.prefix": environment_config["server_name"],
                
                # ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” í•„í„°ë§
                "database.include.list": ",".join(environment_config.get("included_databases", [])),
                "table.include.list": ",".join(environment_config.get("included_tables", [])),
                "database.exclude.list": ",".join(environment_config.get("excluded_databases", [])),
                "table.exclude.list": ",".join(environment_config.get("excluded_tables", [])),
                
                # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
                "binlog.buffer.size": "32768",
                "max.batch.size": "4096",
                "max.queue.size": "16384",
                "poll.interval.ms": "500",
                "snapshot.locking.mode": "minimal",
                "snapshot.fetch.size": "2048",
                
                # ìŠ¤í‚¤ë§ˆ ë° ë©”íƒ€ë°ì´í„° ì„¤ì •
                "schema.history.internal.kafka.bootstrap.servers": environment_config["kafka_bootstrap_servers"],
                "schema.history.internal.kafka.topic": f"schema-changes.{environment_config['server_name']}",
                "include.schema.changes": "true",
                "schema.name.adjustment.mode": "avro",
                
                # ìŠ¤ëƒ…ìƒ· ì„¤ì •
                "snapshot.mode": "initial",
                "snapshot.lock.timeout.ms": "10000",
                "snapshot.delay.ms": "0",
                
                # íŠ¸ëœì­ì…˜ ë©”íƒ€ë°ì´í„°
                "provide.transaction.metadata": "true",
                "transaction.topic": f"transactions.{environment_config['server_name']}",
                
                # í•˜íŠ¸ë¹„íŠ¸ ì„¤ì •
                "heartbeat.interval.ms": "30000",
                "heartbeat.topics.prefix": f"heartbeats.{environment_config['server_name']}"
            }
        }
        
        return {
            "mysql_server_config": mysql_server_config,
            "connector_config": connector_config
        }
    
    def setup_advanced_mysql_config(self, high_performance_config):
        """ê³ ì„±ëŠ¥ MySQL ì»¤ë„¥í„° ì„¤ì •"""
        
        advanced_config = {
            "name": f"mysql-high-perf-connector-{high_performance_config['server_name']}",
            "config": {
                "connector.class": "io.debezium.connector.mysql.MySqlConnector",
                "database.hostname": high_performance_config["mysql_host"],
                "database.port": high_performance_config["mysql_port"],
                "database.user": high_performance_config["mysql_user"],
                "database.password": high_performance_config["mysql_password"],
                "database.server.id": str(high_performance_config["server_id"]),
                "topic.prefix": high_performance_config["server_name"],
                
                # ê³ ì„±ëŠ¥ ì„¤ì •
                "binlog.buffer.size": "65536",
                "max.batch.size": "8192",
                "max.queue.size": "32768",
                "poll.interval.ms": "100",
                "max.queue.size.in.bytes": "104857600",  # 100MB
                
                # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
                "tasks.max": str(high_performance_config.get("max_tasks", 1)),
                "database.connectionTimeZone": "UTC",
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                "binlog.read.buffered.bytes": "1048576",
                "inconsistent.schema.handling.mode": "warn",
                
                # í•„í„°ë§ ìµœì í™”
                "database.include.list": ",".join(high_performance_config.get("included_databases", [])),
                "table.include.list": ",".join(high_performance_config.get("included_tables", [])),
                
                # ìŠ¤í‚¤ë§ˆ ì„¤ì •
                "schema.history.internal.kafka.bootstrap.servers": high_performance_config["kafka_bootstrap_servers"],
                "schema.history.internal.kafka.topic": f"schema-changes.{high_performance_config['server_name']}",
                "include.schema.changes": "true",
                
                # íŠ¸ëœì­ì…˜ ì„¤ì •
                "provide.transaction.metadata": "true",
                "transaction.topic": f"transactions.{high_performance_config['server_name']}",
                
                # í•˜íŠ¸ë¹„íŠ¸
                "heartbeat.interval.ms": "10000",
                "heartbeat.topics.prefix": f"heartbeats.{high_performance_config['server_name']}"
            }
        }
        
        return advanced_config
```

### PostgreSQL ì»¤ë„¥í„° ì„¤ì •

```python
class PostgreSQLConnectorSetup:
    def __init__(self):
        self.config_templates = {}
    
    def setup_postgresql_connector(self, environment_config):
        """PostgreSQL ì»¤ë„¥í„° ì„¤ì •"""
        
        # PostgreSQL ì„œë²„ ì„¤ì •
        postgresql_server_config = {
            "wal_level": "logical",
            "max_wal_senders": "10",
            "max_replication_slots": "10",
            "hot_standby": "on"
        }
        
        # Debezium PostgreSQL ì»¤ë„¥í„° ì„¤ì •
        connector_config = {
            "name": f"postgresql-connector-{environment_config['server_name']}",
            "config": {
                "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
                "database.hostname": environment_config["postgres_host"],
                "database.port": environment_config["postgres_port"],
                "database.user": environment_config["postgres_user"],
                "database.password": environment_config["postgres_password"],
                "database.dbname": environment_config["database_name"],
                "database.server.name": environment_config["server_name"],
                
                # ë³µì œ ìŠ¬ë¡¯ ì„¤ì •
                "database.slot.name": f"debezium_{environment_config['server_name']}",
                "slot.drop.on.stop": "false",
                "slot.streaming.resume.lsn": "0/0",
                
                # Publication ì„¤ì •
                "plugin.name": "pgoutput",
                "publication.name": f"debezium_publication_{environment_config['server_name']}",
                "publication.autocreate.mode": "filtered",
                
                # ìŠ¤í‚¤ë§ˆ ë° í…Œì´ë¸” í•„í„°ë§
                "schema.include.list": ",".join(environment_config.get("included_schemas", [])),
                "table.include.list": ",".join(environment_config.get("included_tables", [])),
                "schema.exclude.list": ",".join(environment_config.get("excluded_schemas", [])),
                "table.exclude.list": ",".join(environment_config.get("excluded_tables", [])),
                
                # ì„±ëŠ¥ ì„¤ì •
                "max.batch.size": "4096",
                "max.queue.size": "16384",
                "poll.interval.ms": "500",
                "status.update.interval.ms": "10000",
                
                # ìŠ¤í‚¤ë§ˆ íˆìŠ¤í† ë¦¬
                "schema.history.internal.kafka.bootstrap.servers": environment_config["kafka_bootstrap_servers"],
                "schema.history.internal.kafka.topic": f"schema-changes.{environment_config['server_name']}",
                "include.schema.changes": "true",
                
                # ìŠ¤ëƒ…ìƒ· ì„¤ì •
                "snapshot.mode": "initial",
                "snapshot.lock.timeout.ms": "10000",
                "snapshot.delay.ms": "0",
                "snapshot.include.collection.list": ",".join(environment_config.get("snapshot_tables", [])),
                
                # íŠ¸ëœì­ì…˜ ë©”íƒ€ë°ì´í„°
                "provide.transaction.metadata": "true",
                "transaction.topic": f"transactions.{environment_config['server_name']}",
                
                # í•˜íŠ¸ë¹„íŠ¸
                "heartbeat.interval.ms": "30000",
                "heartbeat.topics.prefix": f"heartbeats.{environment_config['server_name']}",
                
                # ë°ì´í„° íƒ€ì… ë§¤í•‘
                "decimal.handling.mode": "precise",
                "time.precision.mode": "adaptive",
                "binary.handling.mode": "base64"
            }
        }
        
        return {
            "postgresql_server_config": postgresql_server_config,
            "connector_config": connector_config
        }
```

### MongoDB ì»¤ë„¥í„° ì„¤ì •

```python
class MongoDBConnectorSetup:
    def __init__(self):
        self.config_templates = {}
    
    def setup_mongodb_connector(self, environment_config):
        """MongoDB ì»¤ë„¥í„° ì„¤ì •"""
        
        # MongoDB ì„œë²„ ì„¤ì •
        mongodb_server_config = {
            "replication": "enabled",
            "oplog_size": "1GB",
            "enable_majority_read_concern": "true"
        }
        
        # Debezium MongoDB ì»¤ë„¥í„° ì„¤ì •
        connector_config = {
            "name": f"mongodb-connector-{environment_config['server_name']}",
            "config": {
                "connector.class": "io.debezium.connector.mongodb.MongoDbConnector",
                "mongodb.hosts": environment_config["mongodb_hosts"],
                "mongodb.name": environment_config["server_name"],
                "mongodb.user": environment_config.get("mongodb_user"),
                "mongodb.password": environment_config.get("mongodb_password"),
                "mongodb.ssl.enabled": environment_config.get("ssl_enabled", "false"),
                
                # ë°ì´í„°ë² ì´ìŠ¤ ë° ì»¬ë ‰ì…˜ í•„í„°ë§
                "database.include.list": ",".join(environment_config.get("included_databases", [])),
                "collection.include.list": ",".join(environment_config.get("included_collections", [])),
                "database.exclude.list": ",".join(environment_config.get("excluded_databases", [])),
                "collection.exclude.list": ",".join(environment_config.get("excluded_collections", [])),
                
                # ìº¡ì²˜ ëª¨ë“œ ì„¤ì •
                "capture.mode": "change_streams_update_full",
                "capture.scope": "deployment",
                
                # ì„±ëŠ¥ ì„¤ì •
                "max.batch.size": "2048",
                "max.queue.size": "8192",
                "poll.interval.ms": "1000",
                "max.queue.size.in.bytes": "52428800",  # 50MB
                
                # ìŠ¤í‚¤ë§ˆ íˆìŠ¤í† ë¦¬
                "schema.history.internal.kafka.bootstrap.servers": environment_config["kafka_bootstrap_servers"],
                "schema.history.internal.kafka.topic": f"schema-changes.{environment_config['server_name']}",
                "include.schema.changes": "true",
                
                # ìŠ¤ëƒ…ìƒ· ì„¤ì •
                "snapshot.mode": "initial",
                "snapshot.delay.ms": "0",
                
                # í•˜íŠ¸ë¹„íŠ¸
                "heartbeat.interval.ms": "30000",
                "heartbeat.topics.prefix": f"heartbeats.{environment_config['server_name']}",
                
                # MongoDB íŠ¹í™” ì„¤ì •
                "connect.timeout.ms": "30000",
                "socket.timeout.ms": "30000",
                "server.selection.timeout.ms": "30000",
                "cursor.max.await.time.ms": "1000",
                
                # í•„ë“œ ì„¤ì •
                "field.renames": ",".join(environment_config.get("field_renames", [])),
                "field.exclude.list": ",".join(environment_config.get("field_excludes", [])),
                
                # íŠ¸ëœì­ì…˜ ë©”íƒ€ë°ì´í„°
                "provide.transaction.metadata": "true",
                "transaction.topic": f"transactions.{environment_config['server_name']}"
            }
        }
        
        return {
            "mongodb_server_config": mongodb_server_config,
            "connector_config": connector_config
        }
```

## ğŸ“Š ìŠ¤í‚¤ë§ˆ ì§„í™”ì™€ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ {#ìŠ¤í‚¤ë§ˆ-ì§„í™”ì™€-ìŠ¤í‚¤ë§ˆ-ë ˆì§€ìŠ¤íŠ¸ë¦¬}

### ìŠ¤í‚¤ë§ˆ ì§„í™” ê°œë…

ìŠ¤í‚¤ë§ˆ ì§„í™”ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆê°€ ì‹œê°„ì— ë”°ë¼ ë³€ê²½ë˜ë©´ì„œë„ ê¸°ì¡´ ë°ì´í„°ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. Debeziumì€ ìŠ¤í‚¤ë§ˆ ë³€ê²½ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  Kafkaë¡œ ì „íŒŒí•©ë‹ˆë‹¤.

#### ìŠ¤í‚¤ë§ˆ ì§„í™”ì˜ ì¤‘ìš”ì„±

- **í•˜ìœ„ í˜¸í™˜ì„±**: ê¸°ì¡´ ì†Œë¹„ìê°€ ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
- **ìƒìœ„ í˜¸í™˜ì„±**: ìƒˆë¡œìš´ ì†Œë¹„ìê°€ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
- **ì ì§„ì  ë°°í¬**: ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê°€ëŠ¥
- **ë°ì´í„° ì¼ê´€ì„±**: ë³€ê²½ ê³¼ì •ì—ì„œ ë°ì´í„° ì†ì‹¤ ë°©ì§€

### ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í†µí•©

```python
class SchemaRegistryIntegration:
    def __init__(self):
        self.schema_configs = {}
    
    def setup_confluent_schema_registry(self, registry_config):
        """Confluent Schema Registry ì„¤ì •"""
        
        schema_registry_config = {
            "schema.registry.url": registry_config["schema_registry_url"],
            "key.converter": "io.confluent.connect.avro.AvroConverter",
            "key.converter.schema.registry.url": registry_config["schema_registry_url"],
            "value.converter": "io.confluent.connect.avro.AvroConverter",
            "value.converter.schema.registry.url": registry_config["schema_registry_url"],
            "value.converter.auto.register.schemas": "true",
            "value.converter.use.latest.version": "true",
            "transforms": "unwrap",
            "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
            "transforms.unwrap.drop.tombstones": "false",
            "transforms.unwrap.delete.handling.mode": "drop",
            "transforms.unwrap.add.fields": "op,ts_ms,source.ts_ms,source.db,source.table"
        }
        
        return schema_registry_config
```

### ìŠ¤í‚¤ë§ˆ ì§„í™” ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì„¤ëª… | í˜¸í™˜ì„± | ì²˜ë¦¬ ë°©ì‹ | ì˜ˆì‹œ |
|----------|------|--------|-----------|------|
| **ì»¬ëŸ¼ ì¶”ê°€** | ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ | FORWARD | ìƒˆë¡œìš´ ì»¬ëŸ¼ì€ null ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬ | `ALTER TABLE users ADD COLUMN phone VARCHAR(20)` |
| **ì»¬ëŸ¼ ì‚­ì œ** | ì»¬ëŸ¼ ì‚­ì œ | BACKWARD | ê¸°ì¡´ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ ì œê±° | `ALTER TABLE users DROP COLUMN old_field` |
| **ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½** | ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ | NONE | ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ í›„ ê¸°ì¡´ ì»¬ëŸ¼ ì œê±° | `ALTER TABLE users RENAME COLUMN old_name TO new_name` |
| **ì»¬ëŸ¼ íƒ€ì… ë³€ê²½** | ì»¬ëŸ¼ íƒ€ì… ë³€ê²½ | FORWARD_TRANSITIVE | íƒ€ì… í˜¸í™˜ì„± í™•ì¸ í›„ ë³€í™˜ | `ALTER TABLE users MODIFY COLUMN age INT` |
| **í…Œì´ë¸” ì¶”ê°€** | ìƒˆë¡œìš´ í…Œì´ë¸” ì¶”ê°€ | FORWARD | ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ í† í”½ ìƒì„± | `CREATE TABLE new_table (...)` |
| **í…Œì´ë¸” ì‚­ì œ** | í…Œì´ë¸” ì‚­ì œ | BACKWARD | ê¸°ì¡´ í† í”½ì€ ìœ ì§€í•˜ë˜ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì¤‘ë‹¨ | `DROP TABLE old_table` |

### ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ì „ëµ

| ì „ëµ | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ | í•„ë“œ ì¶”ê°€ | í•„ë“œ ì œê±° | íƒ€ì… ë³€ê²½ | ì•ˆì „ì„± |
|------|------|-----------|-----------|-----------|-----------|--------|
| **Backward Compatibility** | ì´ì „ ë²„ì „ê³¼ í˜¸í™˜ | ì†Œë¹„ì ì—…ë°ì´íŠ¸ ì „ì— ìƒì‚°ì ì—…ë°ì´íŠ¸ | âœ… ì„ íƒì  í•„ë“œë§Œ | âŒ ê¸ˆì§€ | âœ… í™•ì¥ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ | ë†’ìŒ |
| **Forward Compatibility** | ë¯¸ë˜ ë²„ì „ê³¼ í˜¸í™˜ | ìƒì‚°ì ì—…ë°ì´íŠ¸ ì „ì— ì†Œë¹„ì ì—…ë°ì´íŠ¸ | âŒ ê¸ˆì§€ | âœ… ì„ íƒì  í•„ë“œë§Œ | âœ… ì¶•ì†Œ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ | ë†’ìŒ |
| **Full Compatibility** | ì–‘ë°©í–¥ í˜¸í™˜ | ê°€ì¥ ì•ˆì „í•œ ì „ëµ | âœ… ì„ íƒì  í•„ë“œë§Œ | âœ… ì„ íƒì  í•„ë“œë§Œ | âœ… í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ | ìµœê³  |
| **No Compatibility** | í˜¸í™˜ì„± ì—†ìŒ | ê°œë°œ í™˜ê²½ ë˜ëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ | âœ… ëª¨ë“  ë³€ê²½ í—ˆìš© | âœ… ëª¨ë“  ë³€ê²½ í—ˆìš© | âœ… ëª¨ë“  ë³€ê²½ í—ˆìš© | ë‚®ìŒ |

### Schema Registry í˜¸í™˜ì„± ë ˆë²¨

| í˜¸í™˜ì„± ë ˆë²¨ | ì„¤ëª… | ì‚¬ìš© ì‹œê¸° | ì—…ë°ì´íŠ¸ ìˆœì„œ |
|-------------|------|-----------|---------------|
| **BACKWARD** | ì´ì „ ë²„ì „ê³¼ í˜¸í™˜ | ì†Œë¹„ì ì—…ë°ì´íŠ¸ ì „ì— ìƒì‚°ì ì—…ë°ì´íŠ¸ | ì†Œë¹„ì â†’ ìƒì‚°ì |
| **FORWARD** | ë¯¸ë˜ ë²„ì „ê³¼ í˜¸í™˜ | ìƒì‚°ì ì—…ë°ì´íŠ¸ ì „ì— ì†Œë¹„ì ì—…ë°ì´íŠ¸ | ìƒì‚°ì â†’ ì†Œë¹„ì |
| **FULL** | ì–‘ë°©í–¥ í˜¸í™˜ | ê°€ì¥ ì•ˆì „í•œ ì „ëµ | ìˆœì„œ ë¬´ê´€ |
| **NONE** | í˜¸í™˜ì„± ê²€ì‚¬ ì—†ìŒ | ê°œë°œ í™˜ê²½ ë˜ëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ | ì œí•œ ì—†ìŒ |

### ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê·œì¹™

| ê·œì¹™ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| **í•„ë“œ ì¶”ê°€** | ìƒˆ í•„ë“œëŠ” optionalì´ì–´ì•¼ í•¨ | `"email": {"type": "string", "optional": true}` |
| **í•„ë“œ ì œê±°** | ê¸°ì¡´ í•„ë“œ ì œê±° ì‹œ í˜¸í™˜ì„± í™•ì¸ | deprecated ë§ˆí‚¹ í›„ ë‹¨ê³„ì  ì œê±° |
| **íƒ€ì… ë³€ê²½** | íƒ€ì… ë³€ê²½ ì‹œ í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ í—ˆìš© | int32 â†’ int64 (ê°€ëŠ¥), string â†’ int (ë¶ˆê°€ëŠ¥) |
| **ê¸°ë³¸ê°’ ì„¤ì •** | ìƒˆ í•„ë“œì— ì ì ˆí•œ ê¸°ë³¸ê°’ ì„¤ì • | `"default": null` ë˜ëŠ” ì ì ˆí•œ ê¸°ë³¸ê°’ |

### ìŠ¤í‚¤ë§ˆ ì§„í™” ì „ëµ

| ì „ëµ ìœ í˜• | ì„¤ëª… | ì˜ˆì‹œ | ì²˜ë¦¬ ë°©ì‹ |
|-----------|------|------|-----------|
| **Additive Changes** | ì•ˆì „í•œ ë³€ê²½ (í˜¸í™˜ì„± ìœ ì§€) | â€¢ ìƒˆë¡œìš´ optional í•„ë“œ ì¶”ê°€<br>â€¢ ìƒˆë¡œìš´ í…Œì´ë¸” ì¶”ê°€<br>â€¢ enum ê°’ ì¶”ê°€ | ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ |
| **Breaking Changes** | í˜¸í™˜ì„± íŒŒê´´ ë³€ê²½ | â€¢ í•„ìˆ˜ í•„ë“œ ì¶”ê°€<br>â€¢ í•„ë“œ íƒ€ì… ë³€ê²½<br>â€¢ í•„ë“œ ì‚­ì œ<br>â€¢ enum ê°’ ì œê±° | ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ ë²„ì „ìœ¼ë¡œ ì²˜ë¦¬ |

```python
class SchemaCompatibilityStrategies:
    def implement_schema_compatibility_strategies(self):
        """ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ì „ëµ êµ¬í˜„"""
        
        # Schema Registry ì„¤ì •
        schema_registry_config = {
            "url": "http://schema-registry:8081",
            "compatibility_levels": {
                "BACKWARD": "ì´ì „ ë²„ì „ê³¼ í˜¸í™˜ (ì†Œë¹„ì ë¨¼ì € ì—…ë°ì´íŠ¸)",
                "FORWARD": "ë¯¸ë˜ ë²„ì „ê³¼ í˜¸í™˜ (ìƒì‚°ì ë¨¼ì € ì—…ë°ì´íŠ¸)", 
                "FULL": "ì–‘ë°©í–¥ í˜¸í™˜ (ê°€ì¥ ì•ˆì „)",
                "NONE": "í˜¸í™˜ì„± ê²€ì‚¬ ì—†ìŒ (ê°œë°œ í™˜ê²½)"
            },
            "validation_rules": {
                "field_addition": "ìƒˆ í•„ë“œëŠ” optionalì´ì–´ì•¼ í•¨",
                "field_removal": "ê¸°ì¡´ í•„ë“œ ì œê±° ì‹œ í˜¸í™˜ì„± í™•ì¸",
                "type_changes": "íƒ€ì… ë³€ê²½ ì‹œ í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ í—ˆìš©",
                "default_values": "ìƒˆ í•„ë“œì— ì ì ˆí•œ ê¸°ë³¸ê°’ ì„¤ì •"
            }
        }
        
        return schema_registry_config
    
    def validate_schema_compatibility(self, old_schema, new_schema):
        """ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦"""
        
        compatibility_results = {
            "is_compatible": True,
            "compatibility_level": "UNKNOWN",
            "issues": [],
            "recommendations": []
        }
        
        # í•„ë“œ ë³€ê²½ ê²€ì¦
        old_fields = set(old_schema.get("fields", {}).keys())
        new_fields = set(new_schema.get("fields", {}).keys())
        
        # ì¶”ê°€ëœ í•„ë“œ ê²€ì¦
        added_fields = new_fields - old_fields
        for field in added_fields:
            field_def = new_schema["fields"][field]
            if not field_def.get("optional", False):
                compatibility_results["issues"].append(
                    f"ìƒˆ í•„ë“œ '{field}'ê°€ requiredì…ë‹ˆë‹¤. optionalë¡œ ë³€ê²½í•˜ì„¸ìš”."
                )
                compatibility_results["is_compatible"] = False
        
        # ì œê±°ëœ í•„ë“œ ê²€ì¦
        removed_fields = old_fields - new_fields
        if removed_fields:
            compatibility_results["issues"].append(
                f"ì œê±°ëœ í•„ë“œ: {list(removed_fields)}. ì´ëŠ” breaking changeì…ë‹ˆë‹¤."
            )
            compatibility_results["is_compatible"] = False
        
        # íƒ€ì… ë³€ê²½ ê²€ì¦
        common_fields = old_fields & new_fields
        for field in common_fields:
            old_type = old_schema["fields"][field].get("type")
            new_type = new_schema["fields"][field].get("type")
            
            if old_type != new_type:
                if not self._is_type_compatible(old_type, new_type):
                    compatibility_results["issues"].append(
                        f"í•„ë“œ '{field}' íƒ€ì…ì´ {old_type}ì—ì„œ {new_type}ë¡œ ë³€ê²½ë¨"
                    )
                    compatibility_results["is_compatible"] = False
        
        # í˜¸í™˜ì„± ë ˆë²¨ ê²°ì •
        if compatibility_results["is_compatible"]:
            if added_fields and not removed_fields:
                compatibility_results["compatibility_level"] = "BACKWARD"
            elif removed_fields and not added_fields:
                compatibility_results["compatibility_level"] = "FORWARD"
            elif not added_fields and not removed_fields:
                compatibility_results["compatibility_level"] = "FULL"
        else:
            compatibility_results["compatibility_level"] = "NONE"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if not compatibility_results["is_compatible"]:
            compatibility_results["recommendations"] = [
                "ìƒˆ í•„ë“œëŠ” optionalë¡œ ì„¤ì •í•˜ì„¸ìš”",
                "í•„ë“œ ì œê±° ëŒ€ì‹  deprecated ë§ˆí‚¹ì„ ê³ ë ¤í•˜ì„¸ìš”",
                "íƒ€ì… ë³€ê²½ ì‹œ í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”",
                "í•„ìš”ì‹œ ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ ë²„ì „ì„ ìƒì„±í•˜ì„¸ìš”"
            ]
        
        return compatibility_results
    
    def _is_type_compatible(self, old_type, new_type):
        """íƒ€ì… í˜¸í™˜ì„± ê²€ì‚¬"""
        compatible_types = {
            "int32": ["int64", "float", "double"],
            "int64": ["float", "double"],
            "float": ["double"],
            "string": ["bytes"],
            "bytes": ["string"]
        }
        
        return new_type in compatible_types.get(old_type, [])
    
    def create_schema_evolution_plan(self, current_schema, target_schema):
        """ìŠ¤í‚¤ë§ˆ ì§„í™” ê³„íš ìƒì„±"""
        
        evolution_plan = {
            "phases": [],
            "estimated_duration": "unknown",
            "risk_level": "low"
        }
        
        # Phase 1: ì•ˆì „í•œ ë³€ê²½
        safe_changes = []
        old_fields = set(current_schema.get("fields", {}).keys())
        new_fields = set(target_schema.get("fields", {}).keys())
        
        added_fields = new_fields - old_fields
        for field in added_fields:
            if target_schema["fields"][field].get("optional", False):
                safe_changes.append(f"í•„ë“œ '{field}' ì¶”ê°€ (optional)")
        
        if safe_changes:
            evolution_plan["phases"].append({
                "phase": 1,
                "type": "safe_changes",
                "description": "ì•ˆì „í•œ ë³€ê²½ (í˜¸í™˜ì„± ìœ ì§€)",
                "changes": safe_changes,
                "duration": "1-2ì£¼"
            })
        
        # Phase 2: Breaking Changes (í•„ìš”ì‹œ)
        breaking_changes = []
        removed_fields = old_fields - new_fields
        if removed_fields:
            breaking_changes.append(f"í•„ë“œ ì œê±°: {list(removed_fields)}")
            evolution_plan["risk_level"] = "high"
        
        if breaking_changes:
            evolution_plan["phases"].append({
                "phase": 2,
                "type": "breaking_changes",
                "description": "Breaking Changes (ìƒˆ ë²„ì „ í•„ìš”)",
                "changes": breaking_changes,
                "duration": "2-4ì£¼",
                "notes": "ì†Œë¹„ì ì—…ë°ì´íŠ¸ í•„ìš”"
            })
        
        return evolution_plan
```

## ğŸ”„ ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ê³¼ ë¼ìš°íŒ… {#ì‹¤ì‹œê°„-ë°ì´í„°-ë³€í™˜ê³¼-ë¼ìš°íŒ…}

### ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸

```python
class DataTransformationPipeline:
    def __init__(self):
        self.transformation_configs = {}
    
    def setup_single_message_transform(self, transform_config):
        """Single Message Transform (SMT) ì„¤ì •"""
        
        smt_config = {
            "transforms": ",".join(transform_config["transforms"]),
            
            # ë ˆì½”ë“œ ì–¸ë˜í•‘
            "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
            "transforms.unwrap.drop.tombstones": "false",
            "transforms.unwrap.delete.handling.mode": "drop",
            "transforms.unwrap.add.fields": "op,ts_ms,source.ts_ms,source.db,source.table",
            "transforms.unwrap.add.headers": "op,source.ts_ms",
            
            # í•„ë“œ ì´ë¦„ ë³€í™˜
            "transforms.rename.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
            "transforms.rename.renames": ",".join(transform_config.get("field_renames", [])),
            
            # í† í”½ ë¼ìš°íŒ…
            "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
            "transforms.route.regex": transform_config["topic_regex"],
            "transforms.route.replacement": transform_config["topic_replacement"],
            
            # í•„í„°ë§
            "transforms.filter.type": "org.apache.kafka.connect.transforms.Filter",
            "transforms.filter.condition": transform_config.get("filter_condition", ""),
            
            # í•„ë“œ ì¶”ê°€
            "transforms.addfield.type": "org.apache.kafka.connect.transforms.InsertField$Value",
            "transforms.addfield.static.field": "processed_at",
            "transforms.addfield.static.value": "$(date:yyyy-MM-dd'T'HH:mm:ss'Z')"
        }
        
        return smt_config
    
    def implement_custom_transformation(self, custom_logic):
        """ì»¤ìŠ¤í…€ ë³€í™˜ ë¡œì§ êµ¬í˜„"""
        
        custom_transform_config = {
            "transforms": "custom",
            "transforms.custom.type": "com.company.debezium.CustomTransform",
            "transforms.custom.config": {
                "business_rules": custom_logic["business_rules"],
                "data_validation": custom_logic["validation_rules"],
                "enrichment": custom_logic["enrichment_config"],
                "masking": custom_logic["masking_rules"]
            }
        }
        
        return custom_transform_config
    
    def setup_topic_routing(self, routing_config):
        """í† í”½ ë¼ìš°íŒ… ì„¤ì •"""
        
        routing_strategies = {
            "database_based_routing": {
                "description": "ë°ì´í„°ë² ì´ìŠ¤ë³„ í† í”½ ë¶„ë¦¬",
                "configuration": {
                    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
                    "transforms.route.replacement": "$1-$2-$3"
                },
                "result": "server.database.table -> server-database-table"
            },
            
            "table_based_routing": {
                "description": "í…Œì´ë¸”ë³„ í† í”½ ë¶„ë¦¬",
                "configuration": {
                    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
                    "transforms.route.replacement": "$3"
                },
                "result": "server.database.table -> table"
            },
            
            "operation_based_routing": {
                "description": "ì—°ì‚° íƒ€ì…ë³„ í† í”½ ë¶„ë¦¬",
                "configuration": {
                    "transforms.route.type": "com.company.debezium.OperationRouter",
                    "transforms.route.insert.topic": "inserts",
                    "transforms.route.update.topic": "updates",
                    "transforms.route.delete.topic": "deletes"
                },
                "result": "INSERT -> inserts, UPDATE -> updates, DELETE -> deletes"
            },
            
            "conditional_routing": {
                "description": "ì¡°ê±´ë¶€ í† í”½ ë¼ìš°íŒ…",
                "configuration": {
                    "transforms.route.type": "com.company.debezium.ConditionalRouter",
                    "transforms.route.conditions": {
                        "high_priority": "priority == 'HIGH'",
                        "low_priority": "priority == 'LOW'"
                    },
                    "transforms.route.default.topic": "normal"
                },
                "result": "ì¡°ê±´ì— ë”°ë¥¸ ë™ì  í† í”½ ë¼ìš°íŒ…"
            }
        }
        
        return routing_strategies[routing_config["strategy"]]
    
    def implement_data_enrichment(self, enrichment_config):
        """ë°ì´í„° í’ë¶€í™” êµ¬í˜„"""
        
        enrichment_pipeline = {
            "lookup_enrichment": {
                "description": "ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ",
                "implementation": {
                    "transforms.enrich.type": "com.company.debezium.LookupTransform",
                    "transforms.enrich.lookup.source": "redis",
                    "transforms.enrich.lookup.key.field": "user_id",
                    "transforms.enrich.lookup.value.fields": "user_name,user_email,user_role"
                }
            },
            
            "calculation_enrichment": {
                "description": "ê³„ì‚°ëœ í•„ë“œ ì¶”ê°€",
                "implementation": {
                    "transforms.calc.type": "com.company.debezium.CalculationTransform",
                    "transforms.calc.formulas": {
                        "total_amount": "price * quantity",
                        "discount_amount": "total_amount * discount_rate",
                        "final_amount": "total_amount - discount_amount"
                    }
                }
            },
            
            "geolocation_enrichment": {
                "description": "ì§€ë¦¬ì  ì •ë³´ í’ë¶€í™”",
                "implementation": {
                    "transforms.geo.type": "com.company.debezium.GeolocationTransform",
                    "transforms.geo.latitude.field": "lat",
                    "transforms.geo.longitude.field": "lng",
                    "transforms.geo.output.fields": "country,region,city,timezone"
                }
            }
        }
        
        return enrichment_pipeline
```

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ {#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì‹¤ì‹œê°„-ë°ì´í„°-ë™ê¸°í™”-ì‹œìŠ¤í…œ}

### í”„ë¡œì íŠ¸ ê°œìš”

ëŒ€ê·œëª¨ ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì„ ìœ„í•œ ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤. MySQL ì£¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë³€ê²½ì‚¬í•­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•˜ì—¬ Elasticsearch ê²€ìƒ‰ ì—”ì§„, Redis ìºì‹œ, ê·¸ë¦¬ê³  ë°ì´í„° ë ˆì´í¬ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.

#### í”„ë¡œì íŠ¸ ëª©í‘œ

- **ì‹¤ì‹œê°„ ê²€ìƒ‰**: ì£¼ë¬¸ ë°ì´í„° ë³€ê²½ì‚¬í•­ì„ ì¦‰ì‹œ Elasticsearchì— ë°˜ì˜
- **ìºì‹œ ë™ê¸°í™”**: Redis ìºì‹œì˜ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ
- **ë°ì´í„° ë ˆì´í¬**: ë¶„ì„ìš© ë°ì´í„°ë¥¼ Parquet í˜•íƒœë¡œ ì €ì¥
- **ëª¨ë‹ˆí„°ë§**: ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

#### ê¸°ìˆ ì  ë„ì „ê³¼ì œ

- **ëŒ€ìš©ëŸ‰ ì²˜ë¦¬**: ì¼ì¼ 100ë§Œ ê±´ ì´ìƒì˜ ì£¼ë¬¸ ì²˜ë¦¬
- **ë‚®ì€ ì§€ì—°ì‹œê°„**: 5ì´ˆ ì´ë‚´ì˜ ë°ì´í„° ë™ê¸°í™”
- **ê³ ê°€ìš©ì„±**: 99.9% ì´ìƒì˜ ì‹œìŠ¤í…œ ê°€ìš©ì„±
- **ë°ì´í„° ì¼ê´€ì„±**: ëª¨ë“  ì‹œìŠ¤í…œ ê°„ì˜ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥

### 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

#### ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤

| ë°ì´í„°ë² ì´ìŠ¤ | ìš©ë„ | í…Œì´ë¸” | ì¼ì¼ ë³¼ë¥¨ | ì§€ì—°ì‹œê°„ ìš”êµ¬ì‚¬í•­ |
|--------------|------|--------|-----------|-------------------|
| **MySQL Orders** | ì£¼ë¬¸ ë°ì´í„° ì €ì¥ | orders, order_items, customers, products | 1M íŠ¸ëœì­ì…˜ | < 5ì´ˆ |
| **MySQL Inventory** | ì¬ê³  ë°ì´í„° ì €ì¥ | inventory, warehouses, stock_movements | 500K ì—…ë°ì´íŠ¸ | < 1ì´ˆ |

#### Kafka í´ëŸ¬ìŠ¤í„° êµ¬ì„±

| êµ¬ì„± ìš”ì†Œ | ì„¤ì •ê°’ | ì„¤ëª… |
|-----------|--------|------|
| **ë¸Œë¡œì»¤ ìˆ˜** | 3ê°œ | ê³ ê°€ìš©ì„± ë³´ì¥ |
| **íŒŒí‹°ì…˜ ìˆ˜** | 12ê°œ | ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ |
| **ë°ì´í„° ë³´ê´€** | 7ì¼ | ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ë³´ê´€ |
| **í† í”½** | 4ê°œ | ì£¼ë¬¸, ì•„ì´í…œ, ê³ ê°, ì¬ê³  ì´ë²¤íŠ¸ |

#### ëŒ€ìƒ ì‹œìŠ¤í…œ

| ì‹œìŠ¤í…œ | ìš©ë„ | ì¸ë±ìŠ¤/ë°ì´í„° íƒ€ì… | ì—…ë°ì´íŠ¸ ë¹ˆë„ |
|--------|------|-------------------|---------------|
| **Elasticsearch** | ì‹¤ì‹œê°„ ê²€ìƒ‰ ë° ë¶„ì„ | orders, customers, products | ì‹¤ì‹œê°„ |
| **Redis** | ìºì‹œ ë° ì„¸ì…˜ ì €ì¥ | customer_sessions, product_cache | ì‹¤ì‹œê°„ |
| **Data Lake** | ë°ì´í„° ë¶„ì„ ë° ML | Parquet í˜•ì‹ | ì‹œê°„ë³„ ë°°ì¹˜ |

```python
class RealTimeDataSyncSystem:
    def __init__(self):
        self.components = {}
        self.data_flows = {}
    
    def design_system_architecture(self):
        """ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„"""
        return {
            "source_databases": ["mysql_orders", "mysql_inventory"],
            "kafka_cluster": {"brokers": 3, "partitions": 12},
            "target_systems": ["elasticsearch", "redis", "data_lake"]
        }
```
    
    def implement_order_sync_pipeline(self):
        """ì£¼ë¬¸ ë™ê¸°í™” íŒŒì´í”„ë¼ì¸ êµ¬í˜„"""
        
        # MySQL ì£¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥í„°
        orders_connector_config = {
            "name": "mysql-orders-connector",
            "config": {
                "connector.class": "io.debezium.connector.mysql.MySqlConnector",
                "database.hostname": "mysql-orders-cluster",
                "database.port": "3306",
                "database.user": "debezium_user",
                "database.password": "secure_password",
                "database.server.id": "184054",
                "topic.prefix": "orders.ecommerce",
                
                # í…Œì´ë¸” í•„í„°ë§
                "table.include.list": "ecommerce.orders,ecommerce.order_items,ecommerce.customers",
                "database.include.list": "ecommerce",
                
                # ì„±ëŠ¥ ìµœì í™”
                "binlog.buffer.size": "32768",
                "max.batch.size": "4096",
                "max.queue.size": "16384",
                "poll.interval.ms": "100",
                
                # ìŠ¤í‚¤ë§ˆ ì„¤ì •
                "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
                "schema.history.internal.kafka.topic": "schema-changes.orders.ecommerce",
                "include.schema.changes": "true",
                
                # ë°ì´í„° ë³€í™˜
                "transforms": "unwrap,route,addTimestamp",
                "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
                "transforms.unwrap.drop.tombstones": "false",
                "transforms.unwrap.delete.handling.mode": "drop",
                "transforms.unwrap.add.fields": "op,ts_ms,source.ts_ms",
                
                # í† í”½ ë¼ìš°íŒ…
                "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                "transforms.route.regex": "orders\\.ecommerce\\.([^.]+)",
                "transforms.route.replacement": "orders.$1",
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                "transforms.addTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
                "transforms.addTimestamp.static.field": "processed_at",
                "transforms.addTimestamp.static.value": "$(date:yyyy-MM-dd'T'HH:mm:ss'Z')",
                
                # íŠ¸ëœì­ì…˜ ë©”íƒ€ë°ì´í„°
                "provide.transaction.metadata": "true",
                "transaction.topic": "transactions.orders.ecommerce"
            }
        }
        
        return orders_connector_config
    
    def implement_elasticsearch_sink(self):
        """Elasticsearch ì‹±í¬ êµ¬í˜„"""
        
        elasticsearch_sink_config = {
            "name": "elasticsearch-orders-sink",
            "config": {
                "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
                "topics": "orders.orders,orders.order_items,orders.customers",
                "connection.url": "http://elasticsearch-cluster:9200",
                
                # ì¸ë±ìŠ¤ ì„¤ì •
                "type.name": "_doc",
                "key.ignore": "false",
                "schema.ignore": "true",
                
                # ì„±ëŠ¥ ì„¤ì •
                "batch.size": "1000",
                "max.in.flight.requests": "5",
                "flush.timeout.ms": "30000",
                "max.retries": "3",
                "retry.backoff.ms": "1000",
                
                # ì¸ë±ìŠ¤ ë§¤í•‘
                "transforms": "route,addTimestamp",
                "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                "transforms.route.regex": "orders\\.([^.]+)",
                "transforms.route.replacement": "$1-index",
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                "transforms.addTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
                "transforms.addTimestamp.static.field": "indexed_at",
                "transforms.addTimestamp.static.value": "$(date:yyyy-MM-dd'T'HH:mm:ss'Z')",
                
                # ì—ëŸ¬ ì²˜ë¦¬
                "errors.tolerance": "all",
                "errors.log.enable": "true",
                "errors.log.include.messages": "true"
            }
        }
        
        return elasticsearch_sink_config
    
    def implement_redis_sink(self):
        """Redis ì‹±í¬ êµ¬í˜„"""
        
        redis_sink_config = {
            "name": "redis-cache-sink",
            "config": {
                "connector.class": "com.company.connect.redis.RedisSinkConnector",
                "topics": "orders.customers,orders.products",
                "redis.hosts": "redis-cluster:6379",
                "redis.password": "redis_password",
                
                # ë°ì´í„° ë§¤í•‘
                "key.converter": "org.apache.kafka.connect.storage.StringConverter",
                "value.converter": "org.apache.kafka.connect.json.JsonConverter",
                "value.converter.schemas.enable": "false",
                
                # ìºì‹œ ì „ëµ
                "cache.strategy": "write_through",
                "cache.ttl": "3600",  # 1 hour
                "cache.key.field": "id",
                "cache.prefix": "cache:",
                
                # ì„±ëŠ¥ ì„¤ì •
                "batch.size": "500",
                "flush.timeout.ms": "10000",
                "max.retries": "3",
                
                # ë°ì´í„° ë³€í™˜
                "transforms": "extractKey,addMetadata",
                "transforms.extractKey.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
                "transforms.extractKey.field": "id",
                
                "transforms.addMetadata.type": "org.apache.kafka.connect.transforms.InsertField$Value",
                "transforms.addMetadata.static.field": "cached_at",
                "transforms.addMetadata.static.value": "$(date:yyyy-MM-dd'T'HH:mm:ss'Z')"
            }
        }
        
        return redis_sink_config
    
    def implement_data_lake_sink(self):
        """ë°ì´í„° ë ˆì´í¬ ì‹±í¬ êµ¬í˜„"""
        
        data_lake_sink_config = {
            "name": "s3-data-lake-sink",
            "config": {
                "connector.class": "io.confluent.connect.s3.S3SinkConnector",
                "topics": "orders.orders,orders.order_items,orders.customers",
                "s3.bucket.name": "ecommerce-data-lake",
                "s3.region": "us-west-2",
                "s3.part.size": "5242880",  # 5MB
                "flush.size": "10000",
                "rotate.interval.ms": "3600000",  # 1 hour
                
                # íŒŒì¼ í¬ë§·
                "format.class": "io.confluent.connect.s3.format.parquet.ParquetFormat",
                "parquet.compression.codec": "snappy",
                
                # íŒŒí‹°ì…”ë‹
                "partitioner.class": "io.confluent.connect.storage.partitioner.TimeBasedPartitioner",
                "partition.duration.ms": "3600000",  # 1 hour
                "path.format": "YYYY/MM/dd/HH",
                "locale": "en_US",
                "timezone": "UTC",
                
                # ìŠ¤í‚¤ë§ˆ ì§„í™”
                "schema.compatibility": "BACKWARD",
                "value.converter": "io.confluent.connect.avro.AvroConverter",
                "value.converter.schema.registry.url": "http://schema-registry:8081",
                
                # ì—ëŸ¬ ì²˜ë¦¬
                "errors.tolerance": "all",
                "errors.log.enable": "true"
            }
        }
        
        return data_lake_sink_config
    
    def setup_monitoring_and_alerting(self):
        """ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •"""
        
        monitoring_config = {
            "kafka_connect_metrics": {
                "connector_status": "RUNNING, PAUSED, FAILED",
                "task_status": "RUNNING, FAILED, UNASSIGNED",
                "throughput_metrics": "records_per_second, bytes_per_second",
                "latency_metrics": "offset_lag, consumer_lag"
            },
            
            "debezium_metrics": {
                "binlog_position": "MySQL binlog position tracking",
                "snapshot_progress": "Initial snapshot progress",
                "transaction_metrics": "Transaction processing metrics",
                "error_metrics": "Error counts and types"
            },
            
            "alerting_rules": {
                "connector_failure": {
                    "condition": "connector_status == 'FAILED'",
                    "severity": "critical",
                    "notification": ["slack", "email", "pagerduty"]
                },
                
                "high_latency": {
                    "condition": "offset_lag > 10000",
                    "severity": "warning",
                    "notification": ["slack", "email"]
                },
                
                "low_throughput": {
                    "condition": "records_per_second < 100",
                    "severity": "warning",
                    "notification": ["slack"]
                },
                
                "snapshot_stalled": {
                    "condition": "snapshot_progress_stalled > 10min",
                    "severity": "warning",
                    "notification": ["slack", "email"]
                }
            },
            
            "dashboard_metrics": {
                "system_overview": [
                    "Total connectors running",
                    "Total records processed",
                    "Average processing latency",
                    "Error rate percentage"
                ],
                
                "per_connector_metrics": [
                    "Records per second",
                    "Bytes per second",
                    "Offset lag",
                    "Last processed timestamp"
                ],
                
                "target_system_health": [
                    "Elasticsearch indexing rate",
                    "Redis cache hit rate",
                    "S3 upload success rate"
                ]
            }
        }
        
        return monitoring_config
```

### 2. ì‹œìŠ¤í…œ ë°°í¬ ë° ì‹¤í–‰

```python
def deploy_realtime_sync_system():
    """ì‹¤ì‹œê°„ ë™ê¸°í™” ì‹œìŠ¤í…œ ë°°í¬"""
    sync_system = RealTimeDataSyncSystem()
    
    print("ğŸš€ Starting Real-time Data Sync System Deployment...")
    
    # 1. ì•„í‚¤í…ì²˜ ì„¤ê³„
    architecture = sync_system.design_system_architecture()
    print("âœ… System architecture designed")
    
    # 2. ì»¤ë„¥í„° ì„¤ì •
    orders_connector = sync_system.implement_order_sync_pipeline()
    elasticsearch_sink = sync_system.implement_elasticsearch_sink()
    redis_sink = sync_system.implement_redis_sink()
    data_lake_sink = sync_system.implement_data_lake_sink()
    print("âœ… Connector configurations created")
    
    # 3. ëª¨ë‹ˆí„°ë§ ì„¤ì •
    monitoring = sync_system.setup_monitoring_and_alerting()
    print("âœ… Monitoring and alerting configured")
    
    # 4. ë°°í¬ ê²€ì¦
    print("âœ… System deployment completed successfully!")
    
    return {
        "architecture": architecture,
        "connectors": {
            "orders_source": orders_connector,
            "elasticsearch_sink": elasticsearch_sink,
            "redis_sink": redis_sink,
            "data_lake_sink": data_lake_sink
        },
        "monitoring": monitoring
    }

if __name__ == "__main__":
    system = deploy_realtime_sync_system()
```

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ íŒŒíŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Change Data Capture ê¸°ì´ˆ ê°œë…**
   - CDCì˜ ê°œë…ê³¼ í•„ìš”ì„±
   - ë°°ì¹˜ ì²˜ë¦¬ vs CDC ë°©ì‹ ë¹„êµ
   - ì£¼ìš” CDC ë„êµ¬ë“¤ ë¹„êµ (Debezium, Kafka Connect, Maxwell, AWS DMS)
   - CDC ë„ì… ROI ê³„ì‚°

2. **Debezium ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê¸°ëŠ¥**
   - Debezium ì•„í‚¤í…ì²˜ ì´í•´
   - ë°ì´í„°ë² ì´ìŠ¤ë³„ ë³€ê²½ ê°ì§€ ë©”ì»¤ë‹ˆì¦˜
   - ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´ (ì´ë²¤íŠ¸ ì†Œì‹±, CQRS, Saga)

3. **Debezium ì»¤ë„¥í„° ì„¤ì •ê³¼ ìš´ì˜**
   - MySQL ì»¤ë„¥í„° ê³ ê¸‰ ì„¤ì •
   - PostgreSQL ì»¤ë„¥í„° ì„¤ì •
   - MongoDB ì»¤ë„¥í„° ì„¤ì •
   - ì„±ëŠ¥ ìµœì í™” ì „ëµ

4. **ìŠ¤í‚¤ë§ˆ ì§„í™”ì™€ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬**
   - ìŠ¤í‚¤ë§ˆ ì§„í™” ê°œë…ê³¼ ì‹œë‚˜ë¦¬ì˜¤
   - Confluent Schema Registry í†µí•©
   - ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ì „ëµ

5. **ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ê³¼ ë¼ìš°íŒ…**
   - Single Message Transform (SMT) í™œìš©
   - ì»¤ìŠ¤í…€ ë³€í™˜ ë¡œì§ êµ¬í˜„
   - í† í”½ ë¼ìš°íŒ… ì „ëµ
   - ë°ì´í„° í’ë¶€í™” êµ¬í˜„

6. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ëŒ€ê·œëª¨ ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ ì‹¤ì‹œê°„ ë™ê¸°í™” ì‹œìŠ¤í…œ
   - MySQL â†’ Elasticsearch/Redis/Data Lake íŒŒì´í”„ë¼ì¸
   - ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì‹œìŠ¤í…œ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ìš©ë„ | ì¤‘ìš”ë„ |
|------|------|--------|
| **Debezium** | CDC í”Œë«í¼ | â­â­â­â­â­ |
| **Apache Kafka** | ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° | â­â­â­â­â­ |
| **Kafka Connect** | ì»¤ë„¥í„° í”„ë ˆì„ì›Œí¬ | â­â­â­â­ |
| **Schema Registry** | ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ | â­â­â­â­ |
| **Elasticsearch** | ê²€ìƒ‰ ì—”ì§„ | â­â­â­â­ |

### ë‹¤ìŒ íŒŒíŠ¸ ì˜ˆê³ 

**Part 2: Kafka Connectì™€ í”„ë¡œë•ì…˜ CDC ìš´ì˜**ì—ì„œëŠ”:
- Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜ì™€ í™•ì¥ì„±
- ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ
- ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜ ì „ëµ
- ì„±ëŠ¥ ìµœì í™”ì™€ ë³‘ëª© í•´ê²°
- ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ê³¼ ê²€ì¦
- ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµ
- ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ CDC ìš´ì˜ ì‹œìŠ¤í…œ êµ¬ì¶•

---

**ì‹œë¦¬ì¦ˆ ì§„í–‰**: [Change Data Capture ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ](/data-engineering/2025/09/19/change-data-capture-debezium-guide.html)

---

*ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”ì˜ í˜ìœ¼ë¡œ í˜„ëŒ€ì ì¸ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”!* ğŸš€

---
layout: post
lang: ko
title: "Part 3: Apache Icebergì™€ ë¹…ë°ì´í„° ìƒíƒœê³„ í†µí•© - ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° í”Œë«í¼"
description: "Apache Icebergì™€ Spark, Flink, Presto/Trino í†µí•©, Delta Lakeì™€ Hudi ë¹„êµ, í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™”, ì‹¤ë¬´ í”„ë¡œì íŠ¸ë¥¼ í†µí•œ ëŒ€ê·œëª¨ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ êµ¬ì¶•ê¹Œì§€ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-09-23
author: Data Droid
category: data-engineering
tags: [Apache-Iceberg, Spark, Flink, Presto, Trino, Delta-Lake, Hudi, í´ë¼ìš°ë“œìŠ¤í† ë¦¬ì§€, ë°ì´í„°ë ˆì´í¬í•˜ìš°ìŠ¤, ë¹…ë°ì´í„°ìƒíƒœê³„]
series: apache-iceberg-complete-guide
series_order: 3
reading_time: "55ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 3: Apache Icebergì™€ ë¹…ë°ì´í„° ìƒíƒœê³„ í†µí•© - ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° í”Œë«í¼

> Apache Icebergì™€ Spark, Flink, Presto/Trino í†µí•©, Delta Lakeì™€ Hudi ë¹„êµ, í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™”, ì‹¤ë¬´ í”„ë¡œì íŠ¸ë¥¼ í†µí•œ ëŒ€ê·œëª¨ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ êµ¬ì¶•ê¹Œì§€ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ {#ëª©ì°¨}

1. [Apache Sparkì™€ Iceberg í†µí•©](#apache-sparkì™€-iceberg-í†µí•©)
2. [Apache Flinkì™€ Iceberg í†µí•©](#apache-flinkì™€-iceberg-í†µí•©)
3. [Presto/Trinoì™€ Iceberg í†µí•©](#prestotrinoì™€-iceberg-í†µí•©)
4. [í…Œì´ë¸” í¬ë§· ë¹„êµ ë¶„ì„](#í…Œì´ë¸”-í¬ë§·-ë¹„êµ-ë¶„ì„)
5. [í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™”](#í´ë¼ìš°ë“œ-ìŠ¤í† ë¦¬ì§€-ìµœì í™”)
6. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ëŒ€ê·œëª¨ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ êµ¬ì¶•](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ëŒ€ê·œëª¨-ë°ì´í„°-ë ˆì´í¬í•˜ìš°ìŠ¤-êµ¬ì¶•)
7. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ”¥ Apache Sparkì™€ Iceberg í†µí•© {#apache-sparkì™€-iceberg-í†µí•©}

### Spark-Iceberg í†µí•© ê°œìš”

Apache SparkëŠ” Icebergì˜ ê°€ì¥ ê°•ë ¥í•œ íŒŒíŠ¸ë„ˆ ì¤‘ í•˜ë‚˜ë¡œ, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì™€ ë¶„ì„ì„ ìœ„í•œ ì™„ë²½í•œ ì¡°í•©ì„ ì œê³µí•©ë‹ˆë‹¤.

### Spark-Iceberg í†µí•© ì „ëµ

| í†µí•© ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | ì¥ì  |
|-----------|------|-----------|------|
| **ë°°ì¹˜ ì²˜ë¦¬** | â€¢ Spark SQL + Iceberg<br>â€¢ DataFrame API í™œìš©<br>â€¢ íŒŒí‹°ì…˜ ìµœì í™” | â€¢ Iceberg ìŠ¤íŒŒí¬ ì»¤ë„¥í„°<br>â€¢ ìë™ íŒŒí‹°ì…˜ í”„ë£¨ë‹<br>â€¢ ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì› | â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬<br>â€¢ ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬<br>â€¢ í™•ì¥ì„± |
| **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬** | â€¢ Structured Streaming<br>â€¢ ë§ˆì´í¬ë¡œ ë°°ì¹˜ ì²˜ë¦¬<br>â€¢ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ | â€¢ Delta Lake ìŠ¤íƒ€ì¼ ì²˜ë¦¬<br>â€¢ ACID íŠ¸ëœì­ì…˜<br>â€¢ ìŠ¤í‚¤ë§ˆ ì§„í™” | â€¢ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬<br>â€¢ ì¼ê´€ì„± ë³´ì¥<br>â€¢ ì¥ì•  ë³µêµ¬ |
| **ML íŒŒì´í”„ë¼ì¸** | â€¢ MLlib í†µí•©<br>â€¢ í”¼ì²˜ ìŠ¤í† ì–´<br>â€¢ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ | â€¢ Iceberg ê¸°ë°˜ í”¼ì²˜ ì €ì¥<br>â€¢ ì‹¤í—˜ ì¶”ì <br>â€¢ ëª¨ë¸ ì„œë¹™ | â€¢ ML ì›Œí¬í”Œë¡œìš° í†µí•©<br>â€¢ ì‹¤í—˜ ê´€ë¦¬<br>â€¢ í”„ë¡œë•ì…˜ ë°°í¬ |

### Spark-Iceberg í†µí•© êµ¬í˜„

```python
class SparkIcebergIntegration:
    def __init__(self):
        self.spark_session = None
        self.iceberg_catalog = None
    
    def setup_spark_iceberg_environment(self):
        """Spark-Iceberg í™˜ê²½ ì„¤ì •"""
        
        # Spark ì„¤ì •
        spark_config = {
            "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
            "spark.sql.catalog.spark_catalog": "org.apache.iceberg.spark.SparkSessionCatalog",
            "spark.sql.catalog.spark_catalog.type": "hadoop",
            "spark.sql.catalog.spark_catalog.warehouse": "/warehouse",
            "spark.sql.defaultCatalog": "spark_catalog"
        }
        
        # Iceberg ì„¤ì •
        iceberg_config = {
            "write.target-file-size-bytes": "134217728",  # 128MB
            "write.parquet.compression-codec": "zstd",
            "write.metadata.delete-after-commit.enabled": "true",
            "write.data.delete-mode": "copy-on-write"
        }
        
        return spark_config, iceberg_config
    
    def demonstrate_spark_iceberg_operations(self):
        """Spark-Iceberg ì‘ì—… ì‹œì—°"""
        
        # í…Œì´ë¸” ìƒì„±
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS spark_catalog.default.user_events (
            user_id BIGINT,
            event_type STRING,
            event_data STRUCT<page_url: STRING, session_id: STRING>,
            timestamp TIMESTAMP
        ) USING iceberg
        PARTITIONED BY (days(timestamp))
        TBLPROPERTIES (
            'write.target-file-size-bytes' = '134217728',
            'write.parquet.compression-codec' = 'zstd'
        )
        """
        
        # ë°ì´í„° ì‚½ì…
        insert_data_sql = """
        INSERT INTO spark_catalog.default.user_events
        SELECT 
            user_id,
            event_type,
            struct(page_url, session_id) as event_data,
            timestamp
        FROM source_table
        WHERE timestamp >= '2023-01-01'
        """
        
        # ìŠ¤í‚¤ë§ˆ ì§„í™”
        evolve_schema_sql = """
        ALTER TABLE spark_catalog.default.user_events
        ADD COLUMN device_type STRING
        """
        
        # íŒŒí‹°ì…˜ ì§„í™”
        evolve_partition_sql = """
        ALTER TABLE spark_catalog.default.user_events
        ADD PARTITION FIELD hours(timestamp)
        """
        
        return {
            "create_table": create_table_sql,
            "insert_data": insert_data_sql,
            "evolve_schema": evolve_schema_sql,
            "evolve_partition": evolve_partition_sql
        }
```

### Spark Structured Streamingê³¼ Iceberg

#### ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì „ëµ

| ì²˜ë¦¬ ëª¨ë“œ | ì„¤ëª… | êµ¬í˜„ ë°©ë²• | ì‚¬ìš© ì‚¬ë¡€ |
|-----------|------|-----------|-----------|
| **Append Mode** | ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€ | â€¢ INSERT INTO<br>â€¢ ë§ˆì´í¬ë¡œ ë°°ì¹˜ | â€¢ ë¡œê·¸ ë°ì´í„°<br>â€¢ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ |
| **Update Mode** | ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ | â€¢ MERGE INTO<br>â€¢ Upsert ì—°ì‚° | â€¢ ì‚¬ìš©ì í”„ë¡œí•„<br>â€¢ ì£¼ë¬¸ ìƒíƒœ |
| **Complete Mode** | ì „ì²´ í…Œì´ë¸” ì¬ì‘ì„± | â€¢ TRUNCATE + INSERT<br>â€¢ ì „ì²´ ìŠ¤ìº” | â€¢ ì§‘ê³„ í…Œì´ë¸”<br>â€¢ ìš”ì•½ ë°ì´í„° |

#### ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ êµ¬í˜„

```python
class SparkStreamingIceberg:
    def __init__(self):
        self.streaming_query = None
    
    def setup_streaming_processing(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì„¤ì •"""
        
        # Kafka ì†ŒìŠ¤ ì„¤ì •
        kafka_source_config = {
            "kafka.bootstrap.servers": "localhost:9092",
            "subscribe": "user_events",
            "startingOffsets": "latest",
            "failOnDataLoss": "false"
        }
        
        # Iceberg ì‹±í¬ ì„¤ì •
        iceberg_sink_config = {
            "checkpointLocation": "/checkpoint/streaming",
            "outputMode": "append",
            "trigger": "processingTime=30 seconds"
        }
        
        return kafka_source_config, iceberg_sink_config
    
    def implement_streaming_pipeline(self):
        """ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ êµ¬í˜„"""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‘ì„±
        streaming_query = """
        (spark
         .readStream
         .format("kafka")
         .option("kafka.bootstrap.servers", "localhost:9092")
         .option("subscribe", "user_events")
         .load()
         .select(
             from_json(col("value").cast("string"), schema).alias("data")
         )
         .select(
             col("data.user_id").cast("long").alias("user_id"),
             col("data.event_type").alias("event_type"),
             struct(
                 col("data.page_url").alias("page_url"),
                 col("data.session_id").alias("session_id")
             ).alias("event_data"),
             col("data.timestamp").cast("timestamp").alias("timestamp")
         )
         .writeStream
         .format("iceberg")
         .option("checkpointLocation", "/checkpoint/streaming")
         .trigger(processingTime="30 seconds")
         .toTable("spark_catalog.default.user_events")
         .start()
        )
        """
        
        return streaming_query
```

## âš¡ Apache Flinkì™€ Iceberg í†µí•© {#apache-flinkì™€-iceberg-í†µí•©}

### Flink-Iceberg í†µí•© ê°œìš”

Apache FlinkëŠ” ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, Icebergì™€ì˜ í†µí•©ì„ í†µí•´ ì‹¤ì‹œê°„ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Flink-Iceberg í†µí•© ì „ëµ

| í†µí•© ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | ì¥ì  |
|-----------|------|-----------|------|
| **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬** | â€¢ DataStream API<br>â€¢ Table API<br>â€¢ SQL API | â€¢ Flink Iceberg ì»¤ë„¥í„°<br>â€¢ ì‹¤ì‹œê°„ ìŠ¤ëƒ…ìƒ·<br>â€¢ Exactly-once ì²˜ë¦¬ | â€¢ ì €ì§€ì—° ì²˜ë¦¬<br>â€¢ ë†’ì€ ì²˜ë¦¬ëŸ‰<br>â€¢ ì¥ì•  ë³µêµ¬ |
| **ë°°ì¹˜ ì²˜ë¦¬** | â€¢ DataSet API<br>â€¢ ë°°ì¹˜ ìŠ¤ëƒ…ìƒ·<br>â€¢ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì²˜ë¦¬ | â€¢ Iceberg í…Œì´ë¸” ì½ê¸°<br>â€¢ íŒŒí‹°ì…˜ ìŠ¤ìº”<br>â€¢ ìŠ¤í‚¤ë§ˆ ì§„í™” | â€¢ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬<br>â€¢ íˆìŠ¤í† ë¦¬ ë¶„ì„<br>â€¢ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ |
| **ìƒíƒœ ê´€ë¦¬** | â€¢ Flink ìƒíƒœ ë°±ì—”ë“œ<br>â€¢ Iceberg ë©”íƒ€ë°ì´í„°<br>â€¢ ì²´í¬í¬ì¸íŠ¸ í†µí•© | â€¢ ìƒíƒœ ì˜ì†ì„±<br>â€¢ ë©”íƒ€ë°ì´í„° ì¼ê´€ì„±<br>â€¢ ë³µêµ¬ ìµœì í™” | â€¢ ìƒíƒœ ë³µêµ¬<br>â€¢ ì¼ê´€ì„± ë³´ì¥<br>â€¢ ì„±ëŠ¥ ìµœì í™” |

### Flink-Iceberg í†µí•© êµ¬í˜„

```python
class FlinkIcebergIntegration:
    def __init__(self):
        self.flink_env = None
        self.table_env = None
    
    def setup_flink_iceberg_environment(self):
        """Flink-Iceberg í™˜ê²½ ì„¤ì •"""
        
        # Flink ì„¤ì •
        flink_config = {
            "execution.runtime-mode": "streaming",
            "execution.checkpointing.interval": "30s",
            "execution.checkpointing.externalized-checkpoint-retention": "retain-on-cancellation",
            "state.backend": "rocksdb",
            "state.checkpoints.dir": "file:///checkpoints"
        }
        
        # Iceberg ì„¤ì •
        iceberg_config = {
            "write.target-file-size-bytes": "134217728",
            "write.parquet.compression-codec": "zstd",
            "write.metadata.delete-after-commit.enabled": "true"
        }
        
        return flink_config, iceberg_config
    
    def implement_flink_streaming_pipeline(self):
        """Flink ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ êµ¬í˜„"""
        
        # Table APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        streaming_pipeline = """
        # Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„±
        CREATE TABLE kafka_source (
            user_id BIGINT,
            event_type STRING,
            page_url STRING,
            session_id STRING,
            timestamp TIMESTAMP(3),
            WATERMARK FOR timestamp AS timestamp - INTERVAL '5' SECOND
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'user_events',
            'properties.bootstrap.servers' = 'localhost:9092',
            'format' = 'json'
        )
        
        # Iceberg ì‹±í¬ í…Œì´ë¸” ìƒì„±
        CREATE TABLE iceberg_sink (
            user_id BIGINT,
            event_type STRING,
            event_data STRUCT<page_url STRING, session_id STRING>,
            timestamp TIMESTAMP
        ) PARTITIONED BY (days(timestamp))
        WITH (
            'connector' = 'iceberg',
            'catalog-name' = 'hadoop_catalog',
            'catalog-type' = 'hadoop',
            'warehouse' = '/warehouse',
            'database-name' = 'default',
            'table-name' = 'user_events'
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
        INSERT INTO iceberg_sink
        SELECT 
            user_id,
            event_type,
            STRUCT(page_url, session_id) as event_data,
            timestamp
        FROM kafka_source
        WHERE event_type IN ('page_view', 'click', 'purchase')
        """
        
        return streaming_pipeline
    
    def implement_flink_batch_processing(self):
        """Flink ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„"""
        
        # ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        batch_pipeline = """
        # íˆìŠ¤í† ë¦¬ ë°ì´í„° ì²˜ë¦¬
        CREATE TABLE historical_data (
            user_id BIGINT,
            event_type STRING,
            event_count BIGINT,
            processing_date DATE
        ) PARTITIONED BY (processing_date)
        WITH (
            'connector' = 'iceberg',
            'catalog-name' = 'hadoop_catalog',
            'catalog-type' = 'hadoop',
            'warehouse' = '/warehouse',
            'database-name' = 'default',
            'table-name' = 'daily_event_summary'
        )
        
        # ì¼ë³„ ì´ë²¤íŠ¸ ì§‘ê³„
        INSERT INTO historical_data
        SELECT 
            user_id,
            event_type,
            COUNT(*) as event_count,
            DATE(timestamp) as processing_date
        FROM iceberg_sink
        WHERE DATE(timestamp) = '2023-01-01'
        GROUP BY user_id, event_type, DATE(timestamp)
        """
        
        return batch_pipeline
```

## ğŸš€ Presto/Trinoì™€ Iceberg í†µí•© {#prestotrinoì™€-iceberg-í†µí•©}

### Presto/Trino-Iceberg í†µí•© ê°œìš”

Prestoì™€ TrinoëŠ” ëŒ€í™”í˜• ë¶„ì„ ì¿¼ë¦¬ì— ìµœì í™”ëœ ì¿¼ë¦¬ ì—”ì§„ìœ¼ë¡œ, Icebergì™€ì˜ í†µí•©ì„ í†µí•´ ë¹ ë¥¸ ì• ë“œí˜¹ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

### Presto/Trino-Iceberg í†µí•© ì „ëµ

| í†µí•© ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | ì¥ì  |
|-----------|------|-----------|------|
| **ëŒ€í™”í˜• ì¿¼ë¦¬** | â€¢ SQL ì¸í„°í˜ì´ìŠ¤<br>â€¢ íŒŒí‹°ì…˜ í”„ë£¨ë‹<br>â€¢ ì»¬ëŸ¼ í”„ë£¨ë‹ | â€¢ Iceberg ì»¤ë„¥í„°<br>â€¢ ë©”íƒ€ë°ì´í„° ìºì‹±<br>â€¢ ì¿¼ë¦¬ ìµœì í™” | â€¢ ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„<br>â€¢ ë³µì¡í•œ ë¶„ì„<br>â€¢ ì‚¬ìš©ì ì¹œí™”ì  |
| **ë¶„ì‚° ì¿¼ë¦¬** | â€¢ MPP ì•„í‚¤í…ì²˜<br>â€¢ ë³‘ë ¬ ì²˜ë¦¬<br>â€¢ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ | â€¢ í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§<br>â€¢ ì¿¼ë¦¬ ìŠ¤ì¼€ì¤„ë§<br>â€¢ ë©”ëª¨ë¦¬ ê´€ë¦¬ | â€¢ ë†’ì€ ì²˜ë¦¬ëŸ‰<br>â€¢ í™•ì¥ì„±<br>â€¢ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± |
| **ë©”íƒ€ë°ì´í„° ê´€ë¦¬** | â€¢ í†µí•© ì¹´íƒˆë¡œê·¸<br>â€¢ ìŠ¤í‚¤ë§ˆ ì¶”ë¡ <br>â€¢ í†µê³„ ì •ë³´ | â€¢ Hive Metastore í†µí•©<br>â€¢ AWS Glue ì§€ì›<br>â€¢ ìë™ ìŠ¤í‚¤ë§ˆ ê°ì§€ | â€¢ í†µí•© ê´€ë¦¬<br>â€¢ ìë™í™”<br>â€¢ í˜¸í™˜ì„± |

### Presto/Trino-Iceberg í†µí•© êµ¬í˜„

```python
class PrestoTrinoIcebergIntegration:
    def __init__(self):
        self.catalog_config = {}
        self.query_optimizer = None
    
    def setup_presto_trino_catalog(self):
        """Presto/Trino ì¹´íƒˆë¡œê·¸ ì„¤ì •"""
        
        # Iceberg ì¹´íƒˆë¡œê·¸ ì„¤ì •
        catalog_config = {
            "connector.name": "iceberg",
            "hive.metastore.uri": "thrift://localhost:9083",
            "iceberg.catalog.type": "hive_metastore",
            "iceberg.catalog.warehouse": "/warehouse",
            "iceberg.file-format": "PARQUET",
            "iceberg.compression-codec": "ZSTD"
        }
        
        # ì¿¼ë¦¬ ìµœì í™” ì„¤ì •
        optimization_config = {
            "optimizer.use-mark-distinct": "true",
            "optimizer.optimize-metadata-queries": "true",
            "optimizer.partition-pruning": "true",
            "optimizer.column-pruning": "true"
        }
        
        return catalog_config, optimization_config
    
    def demonstrate_analytical_queries(self):
        """ë¶„ì„ ì¿¼ë¦¬ ì‹œì—°"""
        
        # ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬
        analytical_queries = {
            "user_behavior_analysis": """
            SELECT 
                user_id,
                COUNT(*) as total_events,
                COUNT(DISTINCT event_type) as unique_event_types,
                COUNT(DISTINCT DATE(timestamp)) as active_days,
                MAX(timestamp) as last_activity,
                AVG(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchase_rate
            FROM iceberg.default.user_events
            WHERE timestamp >= CURRENT_DATE - INTERVAL '30' DAY
            GROUP BY user_id
            HAVING COUNT(*) >= 10
            ORDER BY total_events DESC
            LIMIT 100
            """,
            
            "real_time_metrics": """
            WITH hourly_metrics AS (
                SELECT 
                    DATE_TRUNC('hour', timestamp) as hour,
                    event_type,
                    COUNT(*) as event_count,
                    COUNT(DISTINCT user_id) as unique_users
                FROM iceberg.default.user_events
                WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '24' HOUR
                GROUP BY DATE_TRUNC('hour', timestamp), event_type
            )
            SELECT 
                hour,
                SUM(event_count) as total_events,
                SUM(unique_users) as total_unique_users,
                COUNT(DISTINCT event_type) as event_types
            FROM hourly_metrics
            GROUP BY hour
            ORDER BY hour DESC
            """,
            
            "funnel_analysis": """
            WITH user_journey AS (
                SELECT 
                    user_id,
                    session_id,
                    timestamp,
                    event_type,
                    ROW_NUMBER() OVER (
                        PARTITION BY user_id, session_id 
                        ORDER BY timestamp
                    ) as step_number
                FROM iceberg.default.user_events
                WHERE timestamp >= CURRENT_DATE - INTERVAL '7' DAY
            ),
            funnel_steps AS (
                SELECT 
                    step_number,
                    event_type,
                    COUNT(DISTINCT CONCAT(user_id, '-', session_id)) as sessions
                FROM user_journey
                WHERE step_number <= 5
                GROUP BY step_number, event_type
            )
            SELECT 
                step_number,
                event_type,
                sessions,
                LAG(sessions) OVER (ORDER BY step_number) as previous_step_sessions,
                ROUND(sessions * 100.0 / LAG(sessions) OVER (ORDER BY step_number), 2) as conversion_rate
            FROM funnel_steps
            ORDER BY step_number, event_type
            """
        }
        
        return analytical_queries
    
    def implement_performance_optimization(self):
        """ì„±ëŠ¥ ìµœì í™” êµ¬í˜„"""
        
        # ì¿¼ë¦¬ ìµœì í™” ì „ëµ
        optimization_strategies = {
            "partition_pruning": {
                "description": "íŒŒí‹°ì…˜ í”„ë£¨ë‹ì„ í†µí•œ I/O ìµœì í™”",
                "implementation": "WHERE ì ˆì— íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì¡°ê±´ ì¶”ê°€",
                "benefit": "ìŠ¤ìº”í•  íŒŒí‹°ì…˜ ìˆ˜ ê°ì†Œ"
            },
            "column_pruning": {
                "description": "í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ I/O ìµœì í™”",
                "implementation": "SELECT ì ˆì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ëª…ì‹œ",
                "benefit": "ë„¤íŠ¸ì›Œí¬ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ"
            },
            "predicate_pushdown": {
                "description": "í•„í„° ì¡°ê±´ì„ ìŠ¤í† ë¦¬ì§€ ë ˆë²¨ë¡œ í‘¸ì‹œë‹¤ìš´",
                "implementation": "WHERE ì ˆ ì¡°ê±´ ìµœì í™”",
                "benefit": "ìŠ¤í† ë¦¬ì§€ ë ˆë²¨ í•„í„°ë§ìœ¼ë¡œ I/O ê°ì†Œ"
            },
            "statistics_utilization": {
                "description": "í…Œì´ë¸” í†µê³„ ì •ë³´ í™œìš©",
                "implementation": "ANALYZE TABLE ëª…ë ¹ìœ¼ë¡œ í†µê³„ ê°±ì‹ ",
                "benefit": "ì¿¼ë¦¬ í”Œë˜ë„ˆ ìµœì í™”"
            }
        }
        
        return optimization_strategies
```

## ğŸ”„ í…Œì´ë¸” í¬ë§· ë¹„êµ ë¶„ì„ {#í…Œì´ë¸”-í¬ë§·-ë¹„êµ-ë¶„ì„}

### ì£¼ìš” í…Œì´ë¸” í¬ë§· ë¹„êµ

| íŠ¹ì„± | Apache Iceberg | Delta Lake | Apache Hudi |
|------|----------------|------------|-------------|
| **ê°œë°œì‚¬** | Netflix â†’ Apache | Databricks | Uber â†’ Apache |
| **ì£¼ìš” ì–¸ì–´** | Java, Python, Scala | Scala, Python, Java | Java, Scala |
| **ìŠ¤í‚¤ë§ˆ ì§„í™”** | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› |
| **íŒŒí‹°ì…˜ ì§„í™”** | âœ… ì™„ì „ ì§€ì› | âŒ ì§€ì› ì•ˆí•¨ | âœ… ë¶€ë¶„ ì§€ì› |
| **ACID íŠ¸ëœì­ì…˜** | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› | âœ… ì™„ì „ ì§€ì› |
| **ì‹œê°„ ì—¬í–‰** | âœ… ì§€ì› | âœ… ì§€ì› | âœ… ì§€ì› |
| **í´ë¼ìš°ë“œ ì§€ì›** | âœ… ìš°ìˆ˜ | âœ… ìš°ìˆ˜ | ğŸŸ¡ ë³´í†µ |
| **ì„±ëŠ¥** | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¡ ë³´í†µ |
| **ìƒíƒœê³„** | ğŸŸ¢ ê´‘ë²”ìœ„ | ğŸŸ¢ Spark ì¤‘ì‹¬ | ğŸŸ¡ ì œí•œì  |

### ìƒì„¸ ê¸°ëŠ¥ ë¹„êµ

#### ìŠ¤í‚¤ë§ˆ ê´€ë¦¬

| ê¸°ëŠ¥ | Iceberg | Delta Lake | Hudi |
|------|---------|------------|------|
| **ìŠ¤í‚¤ë§ˆ ì¶”ê°€** | âœ… í•˜ìœ„ í˜¸í™˜ | âœ… í•˜ìœ„ í˜¸í™˜ | âœ… í•˜ìœ„ í˜¸í™˜ |
| **ìŠ¤í‚¤ë§ˆ ì‚­ì œ** | âœ… í•˜ìœ„ í˜¸í™˜ | âœ… í•˜ìœ„ í˜¸í™˜ | âœ… í•˜ìœ„ í˜¸í™˜ |
| **íƒ€ì… ë³€ê²½** | âœ… ì¡°ê±´ë¶€ í˜¸í™˜ | âœ… ì¡°ê±´ë¶€ í˜¸í™˜ | âœ… ì¡°ê±´ë¶€ í˜¸í™˜ |
| **ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬** | âœ… ì§€ì› | âœ… ì§€ì› | âŒ ì§€ì› ì•ˆí•¨ |

#### íŒŒí‹°ì…”ë‹

| ê¸°ëŠ¥ | Iceberg | Delta Lake | Hudi |
|------|---------|------------|------|
| **íŒŒí‹°ì…˜ ì¶”ê°€** | âœ… ëŸ°íƒ€ì„ | âŒ ì¬êµ¬ì„± í•„ìš” | âœ… ëŸ°íƒ€ì„ |
| **íŒŒí‹°ì…˜ ì‚­ì œ** | âœ… ëŸ°íƒ€ì„ | âŒ ì¬êµ¬ì„± í•„ìš” | âœ… ëŸ°íƒ€ì„ |
| **íŒŒí‹°ì…˜ ë³€í™˜** | âœ… ëŸ°íƒ€ì„ | âŒ ì¬êµ¬ì„± í•„ìš” | âœ… ëŸ°íƒ€ì„ |
| **ìˆ¨ê²¨ì§„ íŒŒí‹°ì…”ë‹** | âœ… ì§€ì› | âŒ ì§€ì› ì•ˆí•¨ | âŒ ì§€ì› ì•ˆí•¨ |

#### ì„±ëŠ¥ íŠ¹ì„±

| íŠ¹ì„± | Iceberg | Delta Lake | Hudi |
|------|---------|------------|------|
| **ì½ê¸° ì„±ëŠ¥** | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¡ ë³´í†µ |
| **ì“°ê¸° ì„±ëŠ¥** | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¢ ìµœì í™”ë¨ | ğŸŸ¡ ë³´í†µ |
| **ì»¤ë°‹ ì„±ëŠ¥** | ğŸŸ¢ ë¹ ë¦„ | ğŸŸ¡ ë³´í†µ | ğŸŸ¡ ë³´í†µ |
| **ë©”íƒ€ë°ì´í„° í¬ê¸°** | ğŸŸ¢ ì‘ìŒ | ğŸŸ¡ ë³´í†µ | ğŸ”´ í¼ |

### ì„ íƒ ê°€ì´ë“œ

#### Iceberg ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì´ìœ  | êµ¬í˜„ ë°©ë²• |
|----------|------|-----------|
| **ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì—”ì§„** | â€¢ Spark, Flink, Presto/Trino ì§€ì›<br>â€¢ ë²¤ë” ì¤‘ë¦½ì„± | â€¢ í†µí•© ì¹´íƒˆë¡œê·¸ êµ¬ì¶•<br>â€¢ í‘œì¤€ SQL ì¸í„°í˜ì´ìŠ¤ |
| **íŒŒí‹°ì…˜ ì§„í™”** | â€¢ ëŸ°íƒ€ì„ íŒŒí‹°ì…˜ ë³€ê²½<br>â€¢ ìˆ¨ê²¨ì§„ íŒŒí‹°ì…”ë‹ | â€¢ ì ì§„ì  íŒŒí‹°ì…˜ ì „ëµ<br>â€¢ ìë™ ìµœì í™” |
| **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ** | â€¢ S3, ADLS, GCS ìµœì í™”<br>â€¢ ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì¹œí™”ì  | â€¢ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ í†µí•©<br>â€¢ ë¹„ìš© ìµœì í™” |

#### Delta Lake ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì´ìœ  | êµ¬í˜„ ë°©ë²• |
|----------|------|-----------|
| **Spark ì¤‘ì‹¬** | â€¢ Spark ìƒíƒœê³„ í†µí•©<br>â€¢ Databricks ì§€ì› | â€¢ Spark ê¸°ë°˜ íŒŒì´í”„ë¼ì¸<br>â€¢ Databricks í”Œë«í¼ |
| **ML/AI ì›Œí¬ë¡œë“œ** | â€¢ MLlib í†µí•©<br>â€¢ í”¼ì²˜ ìŠ¤í† ì–´ | â€¢ ML íŒŒì´í”„ë¼ì¸ êµ¬ì¶•<br>â€¢ ì‹¤í—˜ ê´€ë¦¬ |
| **ê¸°ì¡´ Spark ì‚¬ìš©ì** | â€¢ í•™ìŠµ ê³¡ì„  ìµœì†Œí™”<br>â€¢ ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš© | â€¢ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜<br>â€¢ í˜¸í™˜ì„± ìœ ì§€ |

#### Hudi ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì´ìœ  | êµ¬í˜„ ë°©ë²• |
|----------|------|-----------|
| **ì‹¤ì‹œê°„ ì²˜ë¦¬** | â€¢ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”<br>â€¢ ì €ì§€ì—° ì—…ë°ì´íŠ¸ | â€¢ Kafka í†µí•©<br>â€¢ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ |
| **CDC (Change Data Capture)** | â€¢ ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ê°ì§€<br>â€¢ ì‹¤ì‹œê°„ ë™ê¸°í™” | â€¢ Debezium í†µí•©<br>â€¢ CDC íŒŒì´í”„ë¼ì¸ |
| **Upsert ì¤‘ì‹¬** | â€¢ ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸<br>â€¢ ì¤‘ë³µ ì œê±° | â€¢ Upsert ì „ëµ<br>â€¢ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ |

## â˜ï¸ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™” {#í´ë¼ìš°ë“œ-ìŠ¤í† ë¦¬ì§€-ìµœì í™”}

### í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ë¹„êµ

| ìŠ¤í† ë¦¬ì§€ | Iceberg ì§€ì› | ìµœì í™” ê¸°ëŠ¥ | ë¹„ìš© ëª¨ë¸ | ì„±ëŠ¥ |
|----------|--------------|-------------|-----------|------|
| **Amazon S3** | âœ… ì™„ì „ ì§€ì› | â€¢ Intelligent Tiering<br>â€¢ S3 Select<br>â€¢ Transfer Acceleration | â€¢ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ìš”ê¸ˆ<br>â€¢ ìš”ì²­ ê¸°ë°˜ ìš”ê¸ˆ | ğŸŸ¢ ìš°ìˆ˜ |
| **Azure Data Lake Storage** | âœ… ì™„ì „ ì§€ì› | â€¢ Hierarchical Namespace<br>â€¢ Blob Storage í†µí•©<br>â€¢ Azure Analytics | â€¢ Hot/Cool/Archive<br>â€¢ ì•¡ì„¸ìŠ¤ ë¹ˆë„ ê¸°ë°˜ | ğŸŸ¢ ìš°ìˆ˜ |
| **Google Cloud Storage** | âœ… ì™„ì „ ì§€ì› | â€¢ Lifecycle Management<br>â€¢ Nearline/Coldline<br>â€¢ Transfer Service | â€¢ ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ë³„ ìš”ê¸ˆ<br>â€¢ ë„¤íŠ¸ì›Œí¬ ìš”ê¸ˆ | ğŸŸ¢ ìš°ìˆ˜ |

### í´ë¼ìš°ë“œë³„ ìµœì í™” ì „ëµ

#### Amazon S3 ìµœì í™”

| ìµœì í™” ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | íš¨ê³¼ |
|-------------|------|-----------|------|
| **ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤** | â€¢ Intelligent Tiering<br>â€¢ ìë™ ë¼ì´í”„ì‚¬ì´í´ | â€¢ S3 ë¼ì´í”„ì‚¬ì´í´ ì •ì±…<br>â€¢ ì ‘ê·¼ íŒ¨í„´ ë¶„ì„ | â€¢ 40-60% ë¹„ìš© ì ˆì•½<br>â€¢ ìë™ ìµœì í™” |
| **ì „ì†¡ ìµœì í™”** | â€¢ Transfer Acceleration<br>â€¢ ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ | â€¢ CloudFront í†µí•©<br>â€¢ ë³‘ë ¬ ì—…ë¡œë“œ | â€¢ 50-500% ì†ë„ í–¥ìƒ<br>â€¢ ì•ˆì •ì„± ê°œì„  |
| **ìš”ì²­ ìµœì í™”** | â€¢ S3 Select<br>â€¢ Glacier Select | â€¢ ì»¬ëŸ¼ ê¸°ë°˜ ì¿¼ë¦¬<br>â€¢ ì••ì¶• ë°ì´í„° ì§ì ‘ ì¿¼ë¦¬ | â€¢ 80% ë„¤íŠ¸ì›Œí¬ ê°ì†Œ<br>â€¢ ì¿¼ë¦¬ ì†ë„ í–¥ìƒ |

#### Azure Data Lake Storage ìµœì í™”

| ìµœì í™” ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | íš¨ê³¼ |
|-------------|------|-----------|------|
| **ê³„ì¸µì  ë„¤ì„ìŠ¤í˜ì´ìŠ¤** | â€¢ ë””ë ‰í† ë¦¬ ê¸°ë°˜ ì •ì±…<br>â€¢ ë©”íƒ€ë°ì´í„° ìµœì í™” | â€¢ ACL ê¸°ë°˜ ì ‘ê·¼ ì œì–´<br>â€¢ ë””ë ‰í† ë¦¬ë³„ ì •ì±… | â€¢ ë³´ì•ˆ ê°•í™”<br>â€¢ ê´€ë¦¬ íš¨ìœ¨ì„± |
| **ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ** | â€¢ Hot/Cool/Archive<br>â€¢ ìë™ ê³„ì¸µ ì´ë™ | â€¢ ë¼ì´í”„ì‚¬ì´í´ ì •ì±…<br>â€¢ ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ ì´ë™ | â€¢ 30-70% ë¹„ìš© ì ˆì•½<br>â€¢ ìë™ ê´€ë¦¬ |
| **Analytics í†µí•©** | â€¢ Azure Synapse<br>â€¢ Azure Databricks | â€¢ ë„¤ì´í‹°ë¸Œ í†µí•©<br>â€¢ ìµœì í™”ëœ ì»¤ë„¥í„° | â€¢ ì„±ëŠ¥ í–¥ìƒ<br>â€¢ í†µí•© ê´€ë¦¬ |

#### Google Cloud Storage ìµœì í™”

| ìµœì í™” ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | íš¨ê³¼ |
|-------------|------|-----------|------|
| **ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬** | â€¢ ìë™ í´ë˜ìŠ¤ ë³€ê²½<br>â€¢ ì‚­ì œ ì •ì±… | â€¢ ë¼ì´í”„ì‚¬ì´í´ ê·œì¹™<br>â€¢ ì¡°ê±´ ê¸°ë°˜ ì •ì±… | â€¢ 40-80% ë¹„ìš© ì ˆì•½<br>â€¢ ìë™ ê´€ë¦¬ |
| **ì „ì†¡ ìµœì í™”** | â€¢ Transfer Service<br>â€¢ ë³‘ë ¬ ì²˜ë¦¬ | â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì „ì†¡<br>â€¢ ë„¤íŠ¸ì›Œí¬ ìµœì í™” | â€¢ ì „ì†¡ ì†ë„ í–¥ìƒ<br>â€¢ ì•ˆì •ì„± ê°œì„  |
| **ë³´ì•ˆ ìµœì í™”** | â€¢ IAM í†µí•©<br>â€¢ ì•”í˜¸í™” | â€¢ ì„¸ë°€í•œ ê¶Œí•œ ê´€ë¦¬<br>â€¢ ê³ ê° ê´€ë¦¬ í‚¤ | â€¢ ë³´ì•ˆ ê°•í™”<br>â€¢ ì»´í”Œë¼ì´ì–¸ìŠ¤ |

### í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™” êµ¬í˜„

```python
class CloudStorageOptimizer:
    def __init__(self):
        self.storage_configs = {}
        self.optimization_rules = {}
    
    def setup_s3_optimization(self):
        """S3 ìµœì í™” ì„¤ì •"""
        
        # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ìµœì í™”
        storage_class_config = {
            "standard": {
                "use_case": "ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„°",
                "retention": "30_days",
                "cost_per_gb": 0.023
            },
            "standard_ia": {
                "use_case": "ê°€ë” ì ‘ê·¼í•˜ëŠ” ë°ì´í„°",
                "retention": "90_days",
                "cost_per_gb": 0.0125
            },
            "glacier": {
                "use_case": "ì¥ê¸° ë³´ê´€ ë°ì´í„°",
                "retention": "365_days",
                "cost_per_gb": 0.004
            },
            "intelligent_tiering": {
                "use_case": "ì ‘ê·¼ íŒ¨í„´ì´ ë¶ˆê·œì¹™í•œ ë°ì´í„°",
                "automation": True,
                "cost_per_gb": "variable"
            }
        }
        
        # ë¼ì´í”„ì‚¬ì´í´ ì •ì±…
        lifecycle_policy = {
            "rules": [
                {
                    "id": "IcebergDataLifecycle",
                    "status": "Enabled",
                    "transitions": [
                        {
                            "days": 30,
                            "storage_class": "STANDARD_IA"
                        },
                        {
                            "days": 90,
                            "storage_class": "GLACIER"
                        }
                    ],
                    "expiration": {
                        "days": 2555  # 7ë…„
                    }
                }
            ]
        }
        
        return storage_class_config, lifecycle_policy
    
    def setup_azure_optimization(self):
        """Azure Storage ìµœì í™” ì„¤ì •"""
        
        # ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ì„¤ì •
        storage_tiers = {
            "hot": {
                "use_case": "ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„°",
                "retention": "30_days",
                "cost_per_gb": 0.0184
            },
            "cool": {
                "use_case": "ê°€ë” ì ‘ê·¼í•˜ëŠ” ë°ì´í„°",
                "retention": "90_days",
                "cost_per_gb": 0.01
            },
            "archive": {
                "use_case": "ì¥ê¸° ë³´ê´€ ë°ì´í„°",
                "retention": "365_days",
                "cost_per_gb": 0.00099
            }
        }
        
        # ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ ì •ì±…
        lifecycle_management = {
            "rules": [
                {
                    "name": "IcebergDataLifecycle",
                    "enabled": True,
                    "type": "Lifecycle",
                    "definition": {
                        "filters": {
                            "blob_types": ["blockBlob"],
                            "prefix_match": ["iceberg/"]
                        },
                        "actions": {
                            "base_blob": {
                                "tier_to_cool": {
                                    "days_after_modification_greater_than": 30
                                },
                                "tier_to_archive": {
                                    "days_after_modification_greater_than": 90
                                },
                                "delete": {
                                    "days_after_modification_greater_than": 2555
                                }
                            }
                        }
                    }
                }
            ]
        }
        
        return storage_tiers, lifecycle_management
    
    def setup_gcs_optimization(self):
        """Google Cloud Storage ìµœì í™” ì„¤ì •"""
        
        # ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ ì„¤ì •
        storage_classes = {
            "standard": {
                "use_case": "ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„°",
                "retention": "30_days",
                "cost_per_gb": 0.02
            },
            "nearline": {
                "use_case": "ì›” 1íšŒ ì ‘ê·¼ ë°ì´í„°",
                "retention": "30_days",
                "cost_per_gb": 0.01
            },
            "coldline": {
                "use_case": "ë¶„ê¸° 1íšŒ ì ‘ê·¼ ë°ì´í„°",
                "retention": "90_days",
                "cost_per_gb": 0.007
            },
            "archive": {
                "use_case": "ì—° 1íšŒ ì ‘ê·¼ ë°ì´í„°",
                "retention": "365_days",
                "cost_per_gb": 0.0012
            }
        }
        
        # ë¼ì´í”„ì‚¬ì´í´ ê·œì¹™
        lifecycle_rules = {
            "rules": [
                {
                    "action": {
                        "type": "SetStorageClass",
                        "storageClass": "nearline"
                    },
                    "condition": {
                        "age": 30
                    }
                },
                {
                    "action": {
                        "type": "SetStorageClass",
                        "storageClass": "coldline"
                    },
                    "condition": {
                        "age": 90
                    }
                },
                {
                    "action": {
                        "type": "SetStorageClass",
                        "storageClass": "archive"
                    },
                    "condition": {
                        "age": 365
                    }
                },
                {
                    "action": {
                        "type": "Delete"
                    },
                    "condition": {
                        "age": 2555
                    }
                }
            ]
        }
        
        return storage_classes, lifecycle_rules
```

## ğŸ—ï¸ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ëŒ€ê·œëª¨ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ êµ¬ì¶• {#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ëŒ€ê·œëª¨-ë°ì´í„°-ë ˆì´í¬í•˜ìš°ìŠ¤-êµ¬ì¶•}

### í”„ë¡œì íŠ¸ ê°œìš”

ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ í”Œë«í¼ì„ ìœ„í•œ Iceberg ê¸°ë°˜ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³ , ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì—”ì§„ê³¼ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ë¥¼ í†µí•©í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

#### ì „ì²´ ì•„í‚¤í…ì²˜

| ê³„ì¸µ | êµ¬ì„± ìš”ì†Œ | ê¸°ìˆ  ìŠ¤íƒ | ì—­í•  |
|------|-----------|------------|------|
| **ë°ì´í„° ìˆ˜ì§‘** | â€¢ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼<br>â€¢ ë°°ì¹˜ íŒŒì¼<br>â€¢ API ë°ì´í„° | â€¢ Kafka, Flink<br>â€¢ Spark, Airflow<br>â€¢ REST API | â€¢ ë°ì´í„° ìˆ˜ì§‘<br>â€¢ ì‹¤ì‹œê°„ ì²˜ë¦¬<br>â€¢ ë°°ì¹˜ ì²˜ë¦¬ |
| **ë°ì´í„° ì €ì¥** | â€¢ ì›ì‹œ ë°ì´í„°<br>â€¢ ì •ì œëœ ë°ì´í„°<br>â€¢ ì§‘ê³„ ë°ì´í„° | â€¢ Iceberg Tables<br>â€¢ S3/ADLS/GCS<br>â€¢ íŒŒí‹°ì…”ë‹ | â€¢ ë°ì´í„° ì €ì¥<br>â€¢ ë²„ì „ ê´€ë¦¬<br>â€¢ ìŠ¤í‚¤ë§ˆ ì§„í™” |
| **ë°ì´í„° ì²˜ë¦¬** | â€¢ ETL/ELT<br>â€¢ ì‹¤ì‹œê°„ ë¶„ì„<br>â€¢ ML íŒŒì´í”„ë¼ì¸ | â€¢ Spark, Flink<br>â€¢ Presto/Trino<br>â€¢ MLlib, TensorFlow | â€¢ ë°ì´í„° ë³€í™˜<br>â€¢ ë¶„ì„ ì²˜ë¦¬<br>â€¢ ML ëª¨ë¸ë§ |
| **ë°ì´í„° ì„œë¹™** | â€¢ BI ë„êµ¬<br>â€¢ API ì„œë¹„ìŠ¤<br>â€¢ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ | â€¢ Tableau, PowerBI<br>â€¢ REST API<br>â€¢ Grafana, Kibana | â€¢ ë°ì´í„° ì‹œê°í™”<br>â€¢ API ì œê³µ<br>â€¢ ëª¨ë‹ˆí„°ë§ |

#### ë°ì´í„° ë„ë©”ì¸ ì„¤ê³„

| ë°ì´í„° ë„ë©”ì¸ | í…Œì´ë¸” ìˆ˜ | ë°ì´í„° ë³¼ë¥¨ | íŒŒí‹°ì…˜ ì „ëµ | ë³´ì¡´ ì •ì±… |
|---------------|-----------|-------------|-------------|-----------|
| **ì‚¬ìš©ì ë¶„ì„** | 25ê°œ | 500TB | ë‚ ì§œ + ì‚¬ìš©ì ë²„í‚· | 7ë…„ |
| **ì£¼ë¬¸ ë¶„ì„** | 15ê°œ | 300TB | ë‚ ì§œ + ì§€ì—­ | 10ë…„ |
| **ì œí’ˆ ì¹´íƒˆë¡œê·¸** | 10ê°œ | 50TB | ì¹´í…Œê³ ë¦¬ | ì˜êµ¬ |
| **ë§ˆì¼€íŒ… ë¶„ì„** | 20ê°œ | 200TB | ìº í˜ì¸ + ë‚ ì§œ | 5ë…„ |
| **ì¬ë¬´ ë¶„ì„** | 12ê°œ | 100TB | ì›”ë³„ | 15ë…„ |

### í”„ë¡œì íŠ¸ êµ¬í˜„

```python
class EnterpriseDataLakehouse:
    def __init__(self):
        self.catalog_manager = CatalogManager()
        self.schema_registry = SchemaRegistry()
        self.data_governance = DataGovernance()
    
    def design_data_architecture(self):
        """ë°ì´í„° ì•„í‚¤í…ì²˜ ì„¤ê³„"""
        
        architecture = {
            "data_layers": {
                "bronze_layer": {
                    "purpose": "ì›ì‹œ ë°ì´í„° ì €ì¥",
                    "tables": [
                        "user_events_raw",
                        "order_events_raw", 
                        "product_updates_raw",
                        "marketing_events_raw"
                    ],
                    "partitioning": "hourly",
                    "retention": "30_days",
                    "format": "parquet",
                    "compression": "snappy"
                },
                "silver_layer": {
                    "purpose": "ì •ì œëœ ë°ì´í„° ì €ì¥",
                    "tables": [
                        "user_events_cleaned",
                        "order_events_cleaned",
                        "product_catalog",
                        "marketing_campaigns"
                    ],
                    "partitioning": "daily",
                    "retention": "7_years",
                    "format": "parquet",
                    "compression": "zstd"
                },
                "gold_layer": {
                    "purpose": "ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ìš© ì§‘ê³„ ë°ì´í„°",
                    "tables": [
                        "user_behavior_summary",
                        "daily_sales_summary",
                        "product_performance",
                        "marketing_effectiveness"
                    ],
                    "partitioning": "monthly",
                    "retention": "10_years",
                    "format": "parquet",
                    "compression": "zstd"
                }
            },
            "integration_patterns": {
                "real_time_ingestion": {
                    "source": "Kafka topics",
                    "processing": "Apache Flink",
                    "destination": "Bronze layer",
                    "latency": "< 5 minutes"
                },
                "batch_processing": {
                    "source": "External systems",
                    "processing": "Apache Spark",
                    "destination": "Silver/Gold layers",
                    "frequency": "daily"
                },
                "streaming_analytics": {
                    "source": "Bronze layer",
                    "processing": "Apache Flink + Spark",
                    "destination": "Gold layer",
                    "latency": "< 30 minutes"
                }
            }
        }
        
        return architecture
    
    def implement_multi_engine_integration(self):
        """ë‹¤ì¤‘ ì—”ì§„ í†µí•© êµ¬í˜„"""
        
        integration_config = {
            "spark_integration": {
                "use_cases": [
                    "ETL ì‘ì—…",
                    "ë°°ì¹˜ ë¶„ì„",
                    "ML íŒŒì´í”„ë¼ì¸"
                ],
                "tables": [
                    "user_events_processed",
                    "order_analytics",
                    "ml_features"
                ],
                "optimization": {
                    "target_file_size": "128MB",
                    "compression": "zstd",
                    "partitioning": "adaptive"
                }
            },
            "flink_integration": {
                "use_cases": [
                    "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°",
                    "ì´ë²¤íŠ¸ ì²˜ë¦¬",
                    "ì‹¤ì‹œê°„ ì§‘ê³„"
                ],
                "tables": [
                    "real_time_metrics",
                    "streaming_events",
                    "live_dashboards"
                ],
                "optimization": {
                    "checkpoint_interval": "30s",
                    "parallelism": "auto",
                    "state_backend": "rocksdb"
                }
            },
            "presto_trino_integration": {
                "use_cases": [
                    "ëŒ€í™”í˜• ë¶„ì„",
                    "ì• ë“œí˜¹ ì¿¼ë¦¬",
                    "BI ë„êµ¬ ì—°ë™"
                ],
                "tables": [
                    "analytical_views",
                    "summary_tables",
                    "reporting_data"
                ],
                "optimization": {
                    "metadata_caching": True,
                    "query_optimization": True,
                    "parallel_execution": True
                }
            }
        }
        
        return integration_config
    
    def setup_cloud_optimization(self):
        """í´ë¼ìš°ë“œ ìµœì í™” ì„¤ì •"""
        
        cloud_optimization = {
            "storage_optimization": {
                "s3_optimization": {
                    "storage_classes": {
                        "standard": "ìì£¼ ì ‘ê·¼ ë°ì´í„° (30ì¼)",
                        "standard_ia": "ê°€ë” ì ‘ê·¼ ë°ì´í„° (90ì¼)",
                        "glacier": "ì¥ê¸° ë³´ê´€ ë°ì´í„° (365ì¼)"
                    },
                    "lifecycle_policies": {
                        "automated_tiering": True,
                        "cost_optimization": True,
                        "retention_management": True
                    }
                },
                "performance_optimization": {
                    "intelligent_tiering": True,
                    "transfer_acceleration": True,
                    "s3_select": True
                }
            },
            "compute_optimization": {
                "auto_scaling": {
                    "spark_cluster": "CPU ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§",
                    "flink_cluster": "ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§",
                    "presto_cluster": "ì¿¼ë¦¬ í ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§"
                },
                "resource_optimization": {
                    "spot_instances": "70% ë¹„ìš© ì ˆì•½",
                    "reserved_instances": "30% ì•ˆì •ì„±",
                    "right_sizing": "ì›”ê°„ ìµœì í™”"
                }
            },
            "cost_optimization": {
                "storage_costs": {
                    "current_monthly": "$15,000",
                    "optimized_monthly": "$8,500",
                    "savings_percentage": "43%"
                },
                "compute_costs": {
                    "current_monthly": "$25,000",
                    "optimized_monthly": "$18,000",
                    "savings_percentage": "28%"
                },
                "total_savings": {
                    "monthly": "$13,500",
                    "annual": "$162,000",
                    "savings_percentage": "34%"
                }
            }
        }
        
        return cloud_optimization
```

### ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ì™€ í’ˆì§ˆ ê´€ë¦¬

#### ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë ˆì„ì›Œí¬

| ê±°ë²„ë„ŒìŠ¤ ì˜ì—­ | ì •ì±… | êµ¬í˜„ ë°©ë²• | ì±…ì„ì |
|---------------|------|-----------|--------|
| **ë°ì´í„° í’ˆì§ˆ** | â€¢ ì™„ì „ì„± 95% ì´ìƒ<br>â€¢ ì •í™•ì„± 99% ì´ìƒ<br>â€¢ ì¼ê´€ì„± ê²€ì¦ | â€¢ ìë™ í’ˆì§ˆ ê²€ì‚¬<br>â€¢ ë°ì´í„° í”„ë¡œíŒŒì¼ë§<br>â€¢ ì´ìƒì¹˜ íƒì§€ | ë°ì´í„° í’ˆì§ˆ íŒ€ |
| **ë°ì´í„° ë³´ì•ˆ** | â€¢ ì•”í˜¸í™” (ì €ì¥/ì „ì†¡)<br>â€¢ ì ‘ê·¼ ì œì–´ (RBAC)<br>â€¢ ê°ì‚¬ ë¡œê¹… | â€¢ KMS í‚¤ ê´€ë¦¬<br>â€¢ IAM ì •ì±…<br>â€¢ CloudTrail ë¡œê¹… | ë³´ì•ˆ íŒ€ |
| **ë°ì´í„° ë¼ì´í”„ì‚¬ì´í´** | â€¢ ë³´ì¡´ ì •ì±…<br>â€¢ ì‚­ì œ ì •ì±…<br>â€¢ ì•„ì¹´ì´ë¸Œ ì •ì±… | â€¢ ìë™ ë¼ì´í”„ì‚¬ì´í´<br>â€¢ ì •ì±… ì—”ì§„<br>â€¢ ì»´í”Œë¼ì´ì–¸ìŠ¤ ì²´í¬ | ë°ì´í„° ì•„í‚¤í…íŠ¸ |
| **ë©”íƒ€ë°ì´í„° ê´€ë¦¬** | â€¢ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬<br>â€¢ ë°ì´í„° ê³„ë³´<br>â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ì§‘ | â€¢ ìë™ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘<br>â€¢ ê³„ë³´ ì¶”ì <br>â€¢ ìš©ì–´ì§‘ ê´€ë¦¬ | ë°ì´í„° ìŠ¤íŠœì–´ë“œ |

#### ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

| í’ˆì§ˆ ì§€í‘œ | ì¸¡ì • ë°©ë²• | ì„ê³„ê°’ | ì•¡ì…˜ |
|-----------|-----------|--------|------|
| **ì™„ì „ì„±** | NULL ê°’ ë¹„ìœ¨ | < 5% | ë°ì´í„° ìˆ˜ì§‘ ê²€í†  |
| **ì •í™•ì„±** | ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦ | > 99% | ë°ì´í„° ë³€í™˜ ë¡œì§ ê²€í†  |
| **ì¼ê´€ì„±** | ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬ | 100% | ê´€ê³„í˜• ì œì•½ ì¡°ê±´ ê²€í†  |
| **ì ì‹œì„±** | ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì§€ì—° | < 1ì‹œê°„ | íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™” |
| **ìœ íš¨ì„±** | ë°ì´í„° íƒ€ì… ê²€ì¦ | 100% | ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê°•í™” |

### ìš´ì˜ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼

#### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

| ëŒ€ì‹œë³´ë“œ | ëŒ€ìƒ | ì£¼ìš” ë©”íŠ¸ë¦­ | ìƒˆë¡œê³ ì¹¨ ê°„ê²© |
|----------|------|-------------|----------------|
| **ìš´ì˜ ëŒ€ì‹œë³´ë“œ** | ìš´ì˜íŒ€ | â€¢ ì‹œìŠ¤í…œ ìƒíƒœ<br>â€¢ ì²˜ë¦¬ëŸ‰<br>â€¢ ì˜¤ë¥˜ìœ¨ | 1ë¶„ |
| **ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ** | ë¹„ì¦ˆë‹ˆìŠ¤íŒ€ | â€¢ ë°ì´í„° í’ˆì§ˆ<br>â€¢ ì²˜ë¦¬ ì§€ì—°<br>â€¢ ë¹„ìš© íŠ¸ë Œë“œ | 5ë¶„ |
| **ê°œë°œì ëŒ€ì‹œë³´ë“œ** | ê°œë°œíŒ€ | â€¢ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥<br>â€¢ ì¿¼ë¦¬ ì„±ëŠ¥<br>â€¢ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  | 1ë¶„ |

#### ì•Œë¦¼ ê·œì¹™

| ì•Œë¦¼ ìœ í˜• | ì¡°ê±´ | ì‹¬ê°ë„ | ì•¡ì…˜ |
|-----------|------|--------|------|
| **ì‹œìŠ¤í…œ ì•Œë¦¼** | CPU > 80% | ê²½ê³  | ìŠ¤ì¼€ì¼ ì—… |
| **ë°ì´í„° ì•Œë¦¼** | í’ˆì§ˆ ì ìˆ˜ < 90% | ì¹˜ëª…ì  | ë°ì´í„° íŒ€ ì•Œë¦¼ |
| **ì„±ëŠ¥ ì•Œë¦¼** | ì¿¼ë¦¬ ì‹œê°„ > 5ë¶„ | ê²½ê³  | ì¿¼ë¦¬ ìµœì í™” |
| **ë¹„ìš© ì•Œë¦¼** | ì¼ì¼ ë¹„ìš© > $2,000 | ê²½ê³  | ë¹„ìš© ê²€í†  |

## ğŸ“š í•™ìŠµ ìš”ì•½ {#í•™ìŠµ-ìš”ì•½}

### ì´ë²ˆ Partì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Apache Sparkì™€ Iceberg í†µí•©**
   - ë°°ì¹˜ ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬, ML íŒŒì´í”„ë¼ì¸ í†µí•©
   - Structured Streamingê³¼ Iceberg ì—°ë™
   - ì„±ëŠ¥ ìµœì í™” ì „ëµ

2. **Apache Flinkì™€ Iceberg í†µí•©**
   - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í†µí•©
   - ìƒíƒœ ê´€ë¦¬ì™€ ì²´í¬í¬ì¸íŠ¸ í†µí•©
   - ë°°ì¹˜ ì²˜ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¡°í•©

3. **Presto/Trinoì™€ Iceberg í†µí•©**
   - ëŒ€í™”í˜• ë¶„ì„ ì¿¼ë¦¬ ìµœì í™”
   - ë¶„ì‚° ì¿¼ë¦¬ ì²˜ë¦¬
   - ë©”íƒ€ë°ì´í„° ê´€ë¦¬ í†µí•©

4. **í…Œì´ë¸” í¬ë§· ë¹„êµ ë¶„ì„**
   - Iceberg vs Delta Lake vs Hudi ìƒì„¸ ë¹„êµ
   - ì„ íƒ ê°€ì´ë“œì™€ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ
   - ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

5. **í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìµœì í™”**
   - S3, ADLS, GCS ìµœì í™” ì „ëµ
   - ë¹„ìš© ìµœì í™”ì™€ ì„±ëŠ¥ ìµœì í™”
   - ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

6. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ëŒ€ê·œëª¨ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ êµ¬ì¶•
   - ë‹¤ì¤‘ ì—”ì§„ í†µí•© ì•„í‚¤í…ì²˜
   - ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ì™€ í’ˆì§ˆ ê´€ë¦¬

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ì—­í•  | ì¤‘ìš”ë„ | í•™ìŠµ í¬ì¸íŠ¸ |
|------|------|--------|-------------|
| **Spark-Iceberg** | ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ | â­â­â­â­â­ | ë°°ì¹˜/ìŠ¤íŠ¸ë¦¬ë° í†µí•©, ML íŒŒì´í”„ë¼ì¸ |
| **Flink-Iceberg** | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° | â­â­â­â­â­ | ì €ì§€ì—° ì²˜ë¦¬, ìƒíƒœ ê´€ë¦¬, ì²´í¬í¬ì¸íŠ¸ |
| **Presto/Trino-Iceberg** | ëŒ€í™”í˜• ë¶„ì„ | â­â­â­â­ | ì¿¼ë¦¬ ìµœì í™”, ë©”íƒ€ë°ì´í„° ìºì‹± |
| **í´ë¼ìš°ë“œ ìµœì í™”** | ë¹„ìš©/ì„±ëŠ¥ ìµœì í™” | â­â­â­â­â­ | ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ, ë¼ì´í”„ì‚¬ì´í´, ìë™í™” |
| **ë°ì´í„° ê±°ë²„ë„ŒìŠ¤** | í’ˆì§ˆ/ë³´ì•ˆ ê´€ë¦¬ | â­â­â­â­ | í’ˆì§ˆ ëª¨ë‹ˆí„°ë§, ë³´ì•ˆ ì •ì±…, ë©”íƒ€ë°ì´í„° |

### ì‹œë¦¬ì¦ˆ ì™„ë£Œ ìš”ì•½

**Apache Iceberg Complete Guide ì‹œë¦¬ì¦ˆ**ë¥¼ í†µí•´ ë‹¤ìŒì„ ì™„ì „íˆ ì •ë³µí–ˆìŠµë‹ˆë‹¤:

1. **Part 1: ê¸°ì´ˆì™€ í…Œì´ë¸” í¬ë§·** - Icebergì˜ í•µì‹¬ ê°œë…ê³¼ ê¸°ë³¸ ê¸°ëŠ¥
2. **Part 2: ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ì„±ëŠ¥ ìµœì í™”** - í”„ë¡œë•ì…˜ê¸‰ ìµœì í™”ì™€ ìš´ì˜ ê´€ë¦¬
3. **Part 3: ë¹…ë°ì´í„° ìƒíƒœê³„ í†µí•©** - ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° í”Œë«í¼ êµ¬ì¶•

### ë‹¤ìŒ ë‹¨ê³„

ì´ì œ Apache Icebergë¥¼ ì™„ì „íˆ ë§ˆìŠ¤í„°í–ˆìœ¼ë¯€ë¡œ, ë‹¤ìŒ ì£¼ì œë“¤ì„ í•™ìŠµí•´ë³´ì„¸ìš”:

- **Apache Kafka Complete Guide** - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼
- **Apache Spark Advanced Guide** - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹¬í™”
- **Cloud Data Platform Architecture** - í´ë¼ìš°ë“œ ë°ì´í„° í”Œë«í¼ ì„¤ê³„

---

**ì‹œë¦¬ì¦ˆ ì™„ë£Œ**: [Apache Iceberg Complete Guide Series](/data-engineering/2025/09/23/apache-iceberg-ecosystem-integration.html)

---

*Apache Icebergì™€ ë¹…ë°ì´í„° ìƒíƒœê³„ í†µí•©ì„ í†µí•´ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°ì´í„° í”Œë«í¼ì„ ì™„ì „íˆ ì •ë³µí•˜ì„¸ìš”!* ğŸ§Šâœ¨


---
layout: post
lang: ko
title: "Part 2: Kafka Connectì™€ í”„ë¡œë•ì…˜ CDC ìš´ì˜ - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸"
description: "Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜, ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ, ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜ ì „ëµ, ì„±ëŠ¥ ìµœì í™”ì™€ ì¥ì•  ë³µêµ¬ê¹Œì§€ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤."
date: 2025-09-20
author: Data Droid
category: data-engineering
tags: [Kafka-Connect, CDC-ìš´ì˜, ì»¤ìŠ¤í…€ì»¤ë„¥í„°, ì„±ëŠ¥ìµœì í™”, ëª¨ë‹ˆí„°ë§, ì¥ì• ë³µêµ¬, ì—”í„°í”„ë¼ì´ì¦ˆ, í”„ë¡œë•ì…˜]
series: change-data-capture-complete-guide
series_order: 2
reading_time: "55ë¶„"
difficulty: "ê³ ê¸‰"
---

# Part 2: Kafka Connectì™€ í”„ë¡œë•ì…˜ CDC ìš´ì˜ - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸

> Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜, ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ, ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜ ì „ëµ, ì„±ëŠ¥ ìµœì í™”ì™€ ì¥ì•  ë³µêµ¬ê¹Œì§€ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜](#kafka-connect-ê³ ê¸‰-ì•„í‚¤í…ì²˜)
2. [ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ](#ì»¤ìŠ¤í…€-ì»¤ë„¥í„°-ê°œë°œ)
3. [ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜](#ëŒ€ê·œëª¨-cdc-íŒŒì´í”„ë¼ì¸-ìš´ì˜)
4. [ì„±ëŠ¥ ìµœì í™”ì™€ ë³‘ëª© í•´ê²°](#ì„±ëŠ¥-ìµœì í™”ì™€-ë³‘ëª©-í•´ê²°)
5. [ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ê³¼ ê²€ì¦](#ë°ì´í„°-ì¼ê´€ì„±-ë³´ì¥ê³¼-ê²€ì¦)
6. [ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµ](#ëª¨ë‹ˆí„°ë§ê³¼-ì¥ì• -ë³µêµ¬-ì „ëµ)
7. [ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì—”í„°í”„ë¼ì´ì¦ˆ CDC ìš´ì˜ ì‹œìŠ¤í…œ](#ì‹¤ë¬´-í”„ë¡œì íŠ¸-ì—”í„°í”„ë¼ì´ì¦ˆ-cdc-ìš´ì˜-ì‹œìŠ¤í…œ)
8. [í•™ìŠµ ìš”ì•½](#í•™ìŠµ-ìš”ì•½)

## ğŸ—ï¸ Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜

### Kafka Connect ì•„í‚¤í…ì²˜ ì‹¬í™”

Kafka ConnectëŠ” ë¶„ì‚° ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼ìœ¼ë¡œ, ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹œìŠ¤í…œ ê°„ì˜ ë°ì´í„° ë™ê¸°í™”ë¥¼ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ë¶„ì„

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | í™•ì¥ì„± ê³ ë ¤ì‚¬í•­ | ìš´ì˜ í¬ì¸íŠ¸ |
|----------|------|----------------|-------------|
| **Connect Workers** | ì»¤ë„¥í„° ì‹¤í–‰ í™˜ê²½ | â€¢ ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥<br>â€¢ CPU/ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ | â€¢ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§<br>â€¢ ì¥ì•  ë³µêµ¬ ìë™í™” |
| **Connectors** | ë°ì´í„° ì†ŒìŠ¤/ì‹±í¬ ë¡œì§ | â€¢ í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜<br>â€¢ ë…ë¦½ì  ë°°í¬ | â€¢ ë²„ì „ ê´€ë¦¬<br>â€¢ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ |
| **Tasks** | ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ | â€¢ íŒŒí‹°ì…˜ ê¸°ë°˜ ë³‘ë ¬í™”<br>â€¢ ë™ì  íƒœìŠ¤í¬ í• ë‹¹ | â€¢ ë¶€í•˜ ë¶„ì‚°<br>â€¢ ì¥ì•  ê²©ë¦¬ |
| **Transforms** | ë°ì´í„° ë³€í™˜ ë¡œì§ | â€¢ ì²´ì¸ ê°€ëŠ¥í•œ ë³€í™˜<br>â€¢ ì»¤ìŠ¤í…€ SMT ì§€ì› | â€¢ ì„±ëŠ¥ ìµœì í™”<br>â€¢ ë©”ëª¨ë¦¬ ê´€ë¦¬ |

### í´ëŸ¬ìŠ¤í„° êµ¬ì„± ì „ëµ

```python
class KafkaConnectClusterManager:
    def __init__(self):
        self.cluster_config = {}
    
    def design_cluster_architecture(self, requirements):
        """ëŒ€ê·œëª¨ CDC ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜ ì„¤ê³„"""
        
        architecture = {
            "cluster_size": {
                "workers": self._calculate_worker_count(requirements),
                "connectors": requirements["estimated_connectors"],
                "tasks_per_connector": requirements["max_concurrency"]
            },
            "resource_allocation": {
                "worker_specs": {
                    "cpu": "4 cores",
                    "memory": "8GB",
                    "heap": "4GB",
                    "disk": "100GB SSD"
                },
                "jvm_settings": {
                    "Xmx": "4g",
                    "Xms": "4g",
                    "GC_algorithm": "G1GC",
                    "GC_tuning": "-XX:+UseG1GC -XX:MaxGCPauseMillis=200"
                }
            },
            "network_topology": {
                "worker_distribution": "multi_zone",
                "replication_strategy": "rack_aware",
                "load_balancing": "round_robin"
            }
        }
        
        return architecture
    
    def _calculate_worker_count(self, requirements):
        """ì›Œì»¤ ìˆ˜ ê³„ì‚°"""
        
        # ê¸°ë³¸ ê³„ì‚° ê³µì‹
        base_workers = max(3, requirements["estimated_connectors"] // 10)
        
        # ê³ ê°€ìš©ì„±ì„ ìœ„í•œ ìµœì†Œ ì›Œì»¤ ìˆ˜
        ha_workers = max(base_workers, 3)
        
        # ë¶€í•˜ ë¶„ì‚°ì„ ìœ„í•œ ì—¬ìœ ë¶„
        buffer_workers = int(ha_workers * 0.3)
        
        return ha_workers + buffer_workers
    
    def configure_distributed_mode(self, cluster_config):
        """ë¶„ì‚° ëª¨ë“œ ì„¤ì •"""
        
        distributed_config = {
            "bootstrap.servers": cluster_config["kafka_bootstrap_servers"],
            "group.id": cluster_config["connect_cluster_id"],
            "key.converter": "org.apache.kafka.connect.json.JsonConverter",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "key.converter.schemas.enable": "false",
            "value.converter.schemas.enable": "false",
            "offset.storage.topic": f"{cluster_config['connect_cluster_id']}-offsets",
            "offset.storage.replication.factor": "3",
            "offset.storage.partitions": "25",
            "config.storage.topic": f"{cluster_config['connect_cluster_id']}-configs",
            "config.storage.replication.factor": "3",
            "status.storage.topic": f"{cluster_config['connect_cluster_id']}-status",
            "status.storage.replication.factor": "3",
            "status.storage.partitions": "5",
            "rest.host.name": "0.0.0.0",
            "rest.port": "8083",
            "plugin.path": "/opt/connectors",
            "connector.client.config.override.policy": "All",
            "producer.security.protocol": "SASL_SSL",
            "consumer.security.protocol": "SASL_SSL"
        }
        
        return distributed_config
```

### ê³ ê°€ìš©ì„±ê³¼ ì¥ì•  ë³µêµ¬

| ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ | ë³µêµ¬ ì „ëµ | ìë™í™” ìˆ˜ì¤€ | ë³µêµ¬ ì‹œê°„ |
|---------------|-----------|-------------|-----------|
| **Worker ë…¸ë“œ ì¥ì• ** | â€¢ ìë™ íƒœìŠ¤í¬ ì¬ë°°ì¹˜<br>â€¢ í—¬ìŠ¤ì²´í¬ ê¸°ë°˜ ê°ì§€ | ì™„ì „ ìë™ | 30-60ì´ˆ |
| **Connector ì¥ì• ** | â€¢ ë°±ì˜¤í”„ ì¬ì‹œë„<br>â€¢ ì¥ì•  ê²©ë¦¬ | ìë™ | 1-5ë¶„ |
| **Kafka ë¸Œë¡œì»¤ ì¥ì• ** | â€¢ ë¦¬ë” ì„ ì¶œ<br>â€¢ íŒŒí‹°ì…˜ ì¬í• ë‹¹ | ì™„ì „ ìë™ | 10-30ì´ˆ |
| **ë„¤íŠ¸ì›Œí¬ ë¶„í• ** | â€¢ ì¿ ëŸ¼ ê¸°ë°˜ í•©ì˜<br>â€¢ ì¥ì•  ë³µêµ¬ | ìë™ | 1-2ë¶„ |

## ğŸ”§ ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ

### ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ì•„í‚¤í…ì²˜

ì»¤ìŠ¤í…€ ì»¤ë„¥í„°ëŠ” íŠ¹ì • ë°ì´í„° ì†ŒìŠ¤ë‚˜ ì‹±í¬ì— ìµœì í™”ëœ ì „ìš© ì»¤ë„¥í„°ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### Source Connector ê°œë°œ

```python
class CustomSourceConnector(SourceConnector):
    """ì»¤ìŠ¤í…€ ì†ŒìŠ¤ ì»¤ë„¥í„° ì˜ˆì œ"""
    
    def __init__(self):
        self.config = {}
        self.version = "1.0.0"
    
    def start(self, props):
        """ì»¤ë„¥í„° ì‹œì‘"""
        
        self.config = {
            "database.hostname": props.get("database.hostname"),
            "database.port": props.get("database.port"),
            "database.user": props.get("database.user"),
            "database.password": props.get("database.password"),
            "database.name": props.get("database.name"),
            "batch.size": int(props.get("batch.size", "1000")),
            "poll.interval.ms": int(props.get("poll.interval.ms", "1000")),
            "topic.prefix": props.get("topic.prefix", "custom"),
            "table.whitelist": props.get("table.whitelist", "").split(",")
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
        self._validate_connection()
    
    def task_configs(self, maxTasks):
        """íƒœìŠ¤í¬ ì„¤ì • ìƒì„±"""
        
        # í…Œì´ë¸”ì„ íƒœìŠ¤í¬ ìˆ˜ì— ë”°ë¼ ë¶„í• 
        tables = self.config["table.whitelist"]
        tasks_config = []
        
        for i in range(maxTasks):
            task_config = self.config.copy()
            task_config["task.id"] = str(i)
            
            # í…Œì´ë¸” ë¶„í•  ë¡œì§
            if tables:
                start_idx = i * len(tables) // maxTasks
                end_idx = (i + 1) * len(tables) // maxTasks
                task_config["table.whitelist"] = tables[start_idx:end_idx]
            
            tasks_config.append(task_config)
        
        return tasks_config
    
    def stop(self):
        """ì»¤ë„¥í„° ì¤‘ì§€"""
        pass
    
    def config(self):
        """ì„¤ì • ìŠ¤í‚¤ë§ˆ ì •ì˜"""
        
        return ConfigDef() \
            .define("database.hostname", Type.STRING, ConfigDef.Importance.HIGH,
                   "ë°ì´í„°ë² ì´ìŠ¤ í˜¸ìŠ¤íŠ¸ëª…") \
            .define("database.port", Type.INT, 3306, ConfigDef.Importance.HIGH,
                   "ë°ì´í„°ë² ì´ìŠ¤ í¬íŠ¸") \
            .define("database.user", Type.STRING, ConfigDef.Importance.HIGH,
                   "ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ìëª…") \
            .define("database.password", Type.PASSWORD, ConfigDef.Importance.HIGH,
                   "ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸") \
            .define("database.name", Type.STRING, ConfigDef.Importance.HIGH,
                   "ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„") \
            .define("batch.size", Type.INT, 1000, ConfigDef.Importance.MEDIUM,
                   "ë°°ì¹˜ í¬ê¸°") \
            .define("poll.interval.ms", Type.INT, 1000, ConfigDef.Importance.MEDIUM,
                   "í´ë§ ê°„ê²© (ë°€ë¦¬ì´ˆ)") \
            .define("topic.prefix", Type.STRING, "custom", ConfigDef.Importance.HIGH,
                   "í† í”½ ì ‘ë‘ì‚¬") \
            .define("table.whitelist", Type.STRING, ConfigDef.Importance.HIGH,
                   "í…Œì´ë¸” í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ì‰¼í‘œ êµ¬ë¶„)")
    
    def _validate_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê²€ì¦"""
        
        try:
            # ì‹¤ì œ ì—°ê²° ê²€ì¦ ë¡œì§
            connection = self._create_connection()
            connection.close()
            self.log.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê²€ì¦ ì„±ê³µ")
        except Exception as e:
            raise ConnectException(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _create_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        # ì‹¤ì œ ì—°ê²° ìƒì„± ë¡œì§
        pass


class CustomSourceTask(SourceTask):
    """ì»¤ìŠ¤í…€ ì†ŒìŠ¤ íƒœìŠ¤í¬"""
    
    def __init__(self):
        self.config = {}
        self.connection = None
        self.last_offset = {}
    
    def start(self, props):
        """íƒœìŠ¤í¬ ì‹œì‘"""
        
        self.config = props
        self.connection = self._create_connection()
        
        # ì˜¤í”„ì…‹ ë³µì›
        self._restore_offset()
    
    def poll(self):
        """ë°ì´í„° í´ë§"""
        
        try:
            # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°ì´í„° ì¡°íšŒ
            records = self._fetch_records()
            
            if not records:
                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì§§ì€ ëŒ€ê¸°
                time.sleep(self.config.get("poll.interval.ms", 1000) / 1000.0)
                return []
            
            # SourceRecordë¡œ ë³€í™˜
            source_records = []
            for record in records:
                source_record = self._convert_to_source_record(record)
                source_records.append(source_record)
                
                # ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
                self._update_offset(record)
            
            return source_records
            
        except Exception as e:
            self.log.error(f"ë°ì´í„° í´ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def stop(self):
        """íƒœìŠ¤í¬ ì¤‘ì§€"""
        
        if self.connection:
            self.connection.close()
    
    def _fetch_records(self):
        """ì‹¤ì œ ë°ì´í„° ì¡°íšŒ"""
        
        # ë°°ì¹˜ í¬ê¸°
        batch_size = int(self.config.get("batch.size", 1000))
        
        # í…Œì´ë¸” ëª©ë¡
        tables = self.config.get("table.whitelist", "").split(",")
        
        records = []
        for table in tables:
            if table.strip():
                table_records = self._fetch_table_records(table.strip(), batch_size)
                records.extend(table_records)
        
        return records
    
    def _convert_to_source_record(self, record):
        """ë ˆì½”ë“œë¥¼ SourceRecordë¡œ ë³€í™˜"""
        
        topic = f"{self.config['topic.prefix']}.{record['table']}"
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        key_schema = SchemaBuilder.struct() \
            .field("id", Schema.INT64_SCHEMA) \
            .build()
        
        value_schema = SchemaBuilder.struct() \
            .field("id", Schema.INT64_SCHEMA) \
            .field("name", Schema.STRING_SCHEMA) \
            .field("created_at", Schema.STRING_SCHEMA) \
            .build()
        
        # í‚¤ì™€ ê°’ ìƒì„±
        key = Struct(key_schema).put("id", record["id"])
        value = Struct(value_schema) \
            .put("id", record["id"]) \
            .put("name", record["name"]) \
            .put("created_at", record["created_at"])
        
        # ì˜¤í”„ì…‹ ì •ë³´
        offset = {
            "table": record["table"],
            "id": record["id"],
            "timestamp": record["timestamp"]
        }
        
        return SourceRecord(
            partition=None,
            offset=offset,
            topic=topic,
            key_schema=key_schema,
            key=key,
            value_schema=value_schema,
            value=value,
            timestamp=record["timestamp"]
        )
    
    def _update_offset(self, record):
        """ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸"""
        
        self.last_offset[record["table"]] = {
            "id": record["id"],
            "timestamp": record["timestamp"]
        }
    
    def _restore_offset(self):
        """ì˜¤í”„ì…‹ ë³µì›"""
        
        # Kafka Connectì—ì„œ ìë™ìœ¼ë¡œ ì˜¤í”„ì…‹ ê´€ë¦¬
        pass
```

### Sink Connector ê°œë°œ

```python
class CustomSinkConnector(SinkConnector):
    """ì»¤ìŠ¤í…€ ì‹±í¬ ì»¤ë„¥í„°"""
    
    def __init__(self):
        self.config = {}
    
    def start(self, props):
        """ì»¤ë„¥í„° ì‹œì‘"""
        
        self.config = {
            "target.hostname": props.get("target.hostname"),
            "target.port": props.get("target.port"),
            "target.database": props.get("target.database"),
            "target.username": props.get("target.username"),
            "target.password": props.get("target.password"),
            "batch.size": int(props.get("batch.size", "1000")),
            "flush.timeout.ms": int(props.get("flush.timeout.ms", "5000")),
            "auto.create": props.get("auto.create", "true").lower() == "true",
            "delete.enabled": props.get("delete.enabled", "false").lower() == "true"
        }
    
    def task_configs(self, maxTasks):
        """íƒœìŠ¤í¬ ì„¤ì • ìƒì„±"""
        
        tasks_config = []
        for i in range(maxTasks):
            task_config = self.config.copy()
            task_config["task.id"] = str(i)
            tasks_config.append(task_config)
        
        return tasks_config
    
    def stop(self):
        """ì»¤ë„¥í„° ì¤‘ì§€"""
        pass
    
    def config(self):
        """ì„¤ì • ìŠ¤í‚¤ë§ˆ"""
        
        return ConfigDef() \
            .define("target.hostname", Type.STRING, ConfigDef.Importance.HIGH,
                   "ëŒ€ìƒ ì‹œìŠ¤í…œ í˜¸ìŠ¤íŠ¸ëª…") \
            .define("target.port", Type.INT, ConfigDef.Importance.HIGH,
                   "ëŒ€ìƒ ì‹œìŠ¤í…œ í¬íŠ¸") \
            .define("target.database", Type.STRING, ConfigDef.Importance.HIGH,
                   "ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤") \
            .define("target.username", Type.STRING, ConfigDef.Importance.HIGH,
                   "ëŒ€ìƒ ì‹œìŠ¤í…œ ì‚¬ìš©ìëª…") \
            .define("target.password", Type.PASSWORD, ConfigDef.Importance.HIGH,
                   "ëŒ€ìƒ ì‹œìŠ¤í…œ ë¹„ë°€ë²ˆí˜¸") \
            .define("batch.size", Type.INT, 1000, ConfigDef.Importance.MEDIUM,
                   "ë°°ì¹˜ í¬ê¸°") \
            .define("flush.timeout.ms", Type.INT, 5000, ConfigDef.Importance.MEDIUM,
                   "í”ŒëŸ¬ì‹œ íƒ€ì„ì•„ì›ƒ") \
            .define("auto.create", Type.BOOLEAN, True, ConfigDef.Importance.LOW,
                   "ìë™ í…Œì´ë¸” ìƒì„±") \
            .define("delete.enabled", Type.BOOLEAN, False, ConfigDef.Importance.MEDIUM,
                   "ì‚­ì œ ì‘ì—… í™œì„±í™”")


class CustomSinkTask(SinkTask):
    """ì»¤ìŠ¤í…€ ì‹±í¬ íƒœìŠ¤í¬"""
    
    def __init__(self):
        self.config = {}
        self.connection = None
        self.batch = []
        self.last_flush_time = time.time()
    
    def start(self, props):
        """íƒœìŠ¤í¬ ì‹œì‘"""
        
        self.config = props
        self.connection = self._create_connection()
        
        # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìºì‹œ ì´ˆê¸°í™”
        self._initialize_table_schemas()
    
    def put(self, records):
        """ë ˆì½”ë“œ ì²˜ë¦¬"""
        
        for record in records:
            self.batch.append(record)
            
            # ë°°ì¹˜ í¬ê¸° ë˜ëŠ” íƒ€ì„ì•„ì›ƒì— ë„ë‹¬í•˜ë©´ í”ŒëŸ¬ì‹œ
            if (len(self.batch) >= int(self.config["batch.size"]) or
                time.time() - self.last_flush_time > int(self.config["flush.timeout.ms"]) / 1000.0):
                self._flush_batch()
    
    def flush(self, offsets):
        """ìˆ˜ë™ í”ŒëŸ¬ì‹œ"""
        
        self._flush_batch()
    
    def stop(self):
        """íƒœìŠ¤í¬ ì¤‘ì§€"""
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if self.batch:
            self._flush_batch()
        
        if self.connection:
            self.connection.close()
    
    def _flush_batch(self):
        """ë°°ì¹˜ í”ŒëŸ¬ì‹œ"""
        
        if not self.batch:
            return
        
        try:
            # ë°°ì¹˜ë¥¼ í…Œì´ë¸”ë³„ë¡œ ê·¸ë£¹í™”
            grouped_records = self._group_by_table(self.batch)
            
            # ê° í…Œì´ë¸”ë³„ë¡œ ì²˜ë¦¬
            for table, records in grouped_records.items():
                self._process_table_batch(table, records)
            
            # ë°°ì¹˜ ì´ˆê¸°í™”
            self.batch = []
            self.last_flush_time = time.time()
            
        except Exception as e:
            self.log.error(f"ë°°ì¹˜ í”ŒëŸ¬ì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _process_table_batch(self, table, records):
        """í…Œì´ë¸”ë³„ ë°°ì¹˜ ì²˜ë¦¬"""
        
        # INSERT, UPDATE, DELETEë¡œ ë¶„ë¥˜
        inserts = [r for r in records if r.value() is not None]
        deletes = [r for r in records if r.value() is None]
        
        # INSERT/UPDATE ì²˜ë¦¬
        if inserts:
            self._upsert_records(table, inserts)
        
        # DELETE ì²˜ë¦¬
        if deletes and self.config.get("delete.enabled", "false").lower() == "true":
            self._delete_records(table, deletes)
    
    def _upsert_records(self, table, records):
        """ë ˆì½”ë“œ ì—…ì„œíŠ¸"""
        
        # ì‹¤ì œ ì—…ì„œíŠ¸ ë¡œì§ êµ¬í˜„
        pass
    
    def _delete_records(self, table, records):
        """ë ˆì½”ë“œ ì‚­ì œ"""
        
        # ì‹¤ì œ ì‚­ì œ ë¡œì§ êµ¬í˜„
        pass
```

### ì»¤ìŠ¤í…€ Transform ê°œë°œ

```python
class CustomTransform(Transform):
    """ì»¤ìŠ¤í…€ Single Message Transform"""
    
    def __init__(self):
        self.config = {}
    
    def configure(self, configs):
        """ì„¤ì • êµ¬ì„±"""
        
        self.config = {
            "field.mapping": configs.get("field.mapping", ""),
            "data.type": configs.get("data.type", "json"),
            "validation.enabled": configs.get("validation.enabled", "true").lower() == "true"
        }
        
        # í•„ë“œ ë§¤í•‘ íŒŒì‹±
        if self.config["field.mapping"]:
            self.field_mapping = self._parse_field_mapping(self.config["field.mapping"])
        else:
            self.field_mapping = {}
    
    def apply(self, record):
        """ë ˆì½”ë“œ ë³€í™˜ ì ìš©"""
        
        if record is None:
            return None
        
        try:
            # ê°’ ì¶”ì¶œ
            value = record.value()
            
            if value is None:
                return record
            
            # ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬
            if self.config["data.type"] == "json":
                transformed_value = self._transform_json(value)
            elif self.config["data.type"] == "avro":
                transformed_value = self._transform_avro(value)
            else:
                transformed_value = value
            
            # ê²€ì¦
            if self.config["validation.enabled"]:
                self._validate_record(transformed_value)
            
            # ìƒˆ ë ˆì½”ë“œ ìƒì„±
            return record.new_record(
                topic=record.topic(),
                partition=record.kafkaPartition(),
                key_schema=record.keySchema(),
                key=record.key(),
                value_schema=record.valueSchema(),
                value=transformed_value,
                timestamp=record.timestamp()
            )
            
        except Exception as e:
            self.log.error(f"ë ˆì½”ë“œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return record  # ì›ë³¸ ë ˆì½”ë“œ ë°˜í™˜
    
    def _transform_json(self, value):
        """JSON ë°ì´í„° ë³€í™˜"""
        
        if isinstance(value, dict):
            transformed = value.copy()
            
            # í•„ë“œ ë§¤í•‘ ì ìš©
            for old_field, new_field in self.field_mapping.items():
                if old_field in transformed:
                    transformed[new_field] = transformed.pop(old_field)
            
            # ì¶”ê°€ ë³€í™˜ ë¡œì§
            transformed = self._apply_business_rules(transformed)
            
            return transformed
        
        return value
    
    def _transform_avro(self, value):
        """Avro ë°ì´í„° ë³€í™˜"""
        
        # Avro ìŠ¤í‚¤ë§ˆ ë³€í™˜ ë¡œì§
        return value
    
    def _apply_business_rules(self, data):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©"""
        
        # ì˜ˆ: íƒ€ì„ìŠ¤íƒ¬í”„ í¬ë§· ë³€í™˜
        if "created_at" in data:
            data["created_at"] = self._format_timestamp(data["created_at"])
        
        # ì˜ˆ: ë°ì´í„° ì •ê·œí™”
        if "email" in data:
            data["email"] = data["email"].lower().strip()
        
        return data
    
    def _validate_record(self, value):
        """ë ˆì½”ë“œ ê²€ì¦"""
        
        if not isinstance(value, dict):
            return
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ["id", "name"]
        for field in required_fields:
            if field not in value:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _parse_field_mapping(self, mapping_str):
        """í•„ë“œ ë§¤í•‘ íŒŒì‹±"""
        
        mapping = {}
        for pair in mapping_str.split(","):
            if ":" in pair:
                old_field, new_field = pair.split(":", 1)
                mapping[old_field.strip()] = new_field.strip()
        
        return mapping
    
    def _format_timestamp(self, timestamp):
        """íƒ€ì„ìŠ¤íƒ¬í”„ í¬ë§·íŒ…"""
        
        # ì‹¤ì œ í¬ë§·íŒ… ë¡œì§
        return timestamp
```

## ğŸš€ ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜

### íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ì€ ìˆ˜ë°± ê°œì˜ í…Œì´ë¸”ê³¼ ìˆ˜ì‹­ ê°œì˜ ì‹œìŠ¤í…œì„ ë™ì‹œì— ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

### íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ

| êµ¬ì„± ìš”ì†Œ | ì—­í•  | í™•ì¥ì„± ì „ëµ | ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ |
|-----------|------|-------------|-----------------|
| **Source Connectors** | ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³€ê²½ì‚¬í•­ ìº¡ì²˜ | â€¢ í…Œì´ë¸”ë³„ ì»¤ë„¥í„° ë¶„ë¦¬<br>â€¢ íŒŒí‹°ì…˜ ê¸°ë°˜ ë³‘ë ¬í™” | â€¢ ì§€ì—°ì‹œê°„<br>â€¢ ì²˜ë¦¬ëŸ‰<br>â€¢ ì˜¤ë¥˜ìœ¨ |
| **Transform Layer** | ë°ì´í„° ë³€í™˜ ë° ì •ì œ | â€¢ ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ ë³€í™˜<br>â€¢ ë³‘ë ¬ ì²˜ë¦¬ | â€¢ ë³€í™˜ ì§€ì—°ì‹œê°„<br>â€¢ ë°ì´í„° í’ˆì§ˆ |
| **Sink Connectors** | ëŒ€ìƒ ì‹œìŠ¤í…œìœ¼ë¡œ ë°ì´í„° ì „ì†¡ | â€¢ ëŒ€ìƒë³„ ì»¤ë„¥í„° ë¶„ë¦¬<br>â€¢ ë°°ì¹˜ ìµœì í™” | â€¢ ì „ì†¡ ì§€ì—°ì‹œê°„<br>â€¢ ì„±ê³µë¥  |
| **Schema Registry** | ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ë° í˜¸í™˜ì„± | â€¢ ë¶„ì‚° ìºì‹±<br>â€¢ ë²„ì „ ê´€ë¦¬ | â€¢ ìŠ¤í‚¤ë§ˆ ë³€ê²½<br>â€¢ í˜¸í™˜ì„± ê²€ì‚¬ |

### íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

```python
class CDCPipelineOrchestrator:
    def __init__(self):
        self.pipeline_configs = {}
        self.connector_manager = ConnectorManager()
        self.monitoring_system = MonitoringSystem()
    
    def deploy_large_scale_pipeline(self, pipeline_spec):
        """ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ë°°í¬"""
        
        deployment_plan = {
            "phase_1": self._deploy_source_connectors(pipeline_spec["sources"]),
            "phase_2": self._configure_transformations(pipeline_spec["transforms"]),
            "phase_3": self._deploy_sink_connectors(pipeline_spec["sinks"]),
            "phase_4": self._setup_monitoring(pipeline_spec["monitoring"])
        }
        
        return deployment_plan
    
    def _deploy_source_connectors(self, sources):
        """ì†ŒìŠ¤ ì»¤ë„¥í„° ë°°í¬"""
        
        source_deployment = []
        
        for source in sources:
            # ë°ì´í„°ë² ì´ìŠ¤ë³„ ì»¤ë„¥í„° ì„¤ì •
            connector_config = {
                "name": f"source-{source['database']}-{source['schema']}",
                "connector.class": self._get_connector_class(source["type"]),
                "tasks.max": source.get("tasks_max", 4),
                "database.hostname": source["hostname"],
                "database.port": source["port"],
                "database.user": source["username"],
                "database.password": source["password"],
                "database.server.id": source["server_id"],
                "topic.prefix": f"{source['database']}.{source['schema']}",
                "table.include.list": ",".join(source["tables"]),
                "transforms": "unwrap,route,addTimestamp",
                "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
                "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                "transforms.route.regex": f"{source['database']}\\.{source['schema']}\\.([^.]+)",
                "transforms.route.replacement": f"{source['database']}.$1",
                "max.batch.size": source.get("batch_size", 4096),
                "poll.interval.ms": source.get("poll_interval", 100)
            }
            
            # ì»¤ë„¥í„° ë°°í¬
            result = self.connector_manager.deploy_connector(connector_config)
            source_deployment.append({
                "connector_name": connector_config["name"],
                "status": result["status"],
                "tables": source["tables"]
            })
        
        return source_deployment
    
    def _configure_transformations(self, transforms):
        """ë³€í™˜ ì„¤ì •"""
        
        transform_configs = []
        
        for transform in transforms:
            config = {
                "name": f"transform-{transform['name']}",
                "transforms": ",".join(transform["steps"]),
                "topics": transform["input_topics"],
                "output_topic": transform["output_topic"]
            }
            
            # ê° ë³€í™˜ ë‹¨ê³„ë³„ ì„¤ì •
            for i, step in enumerate(transform["steps"]):
                step_config = transform["step_configs"][i]
                config.update({
                    f"transforms.{step}.type": step_config["class"],
                    f"transforms.{step}.field.mapping": step_config.get("field_mapping", ""),
                    f"transforms.{step}.validation.enabled": step_config.get("validation", "true")
                })
            
            transform_configs.append(config)
        
        return transform_configs
    
    def _deploy_sink_connectors(self, sinks):
        """ì‹±í¬ ì»¤ë„¥í„° ë°°í¬"""
        
        sink_deployment = []
        
        for sink in sinks:
            sink_config = {
                "name": f"sink-{sink['target']}-{sink['database']}",
                "connector.class": self._get_sink_connector_class(sink["type"]),
                "tasks.max": sink.get("tasks_max", 4),
                "topics": ",".join(sink["topics"]),
                "batch.size": sink.get("batch_size", 1000),
                "flush.timeout.ms": sink.get("flush_timeout", 5000)
            }
            
            # ëŒ€ìƒ ì‹œìŠ¤í…œë³„ ì„¤ì • ì¶”ê°€
            if sink["type"] == "elasticsearch":
                sink_config.update({
                    "connection.url": sink["connection_url"],
                    "type.name": sink.get("type_name", "_doc"),
                    "key.ignore": "false",
                    "schema.ignore": "true"
                })
            elif sink["type"] == "postgresql":
                sink_config.update({
                    "connection.url": sink["connection_url"],
                    "auto.create": "true",
                    "delete.enabled": "false"
                })
            elif sink["type"] == "s3":
                sink_config.update({
                    "s3.bucket.name": sink["bucket"],
                    "s3.region": sink["region"],
                    "flush.size": sink.get("flush_size", 10000),
                    "format.class": "io.confluent.connect.s3.format.parquet.ParquetFormat"
                })
            
            # ì»¤ë„¥í„° ë°°í¬
            result = self.connector_manager.deploy_connector(sink_config)
            sink_deployment.append({
                "connector_name": sink_config["name"],
                "status": result["status"],
                "target": sink["target"]
            })
        
        return sink_deployment
    
    def _setup_monitoring(self, monitoring_config):
        """ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        
        monitoring_setup = {
            "metrics": self.monitoring_system.setup_metrics(monitoring_config),
            "alerts": self.monitoring_system.setup_alerts(monitoring_config),
            "dashboards": self.monitoring_system.setup_dashboards(monitoring_config)
        }
        
        return monitoring_setup
```

### íŒŒì´í”„ë¼ì¸ ìš´ì˜ ì „ëµ

| ìš´ì˜ ì˜ì—­ | ì „ëµ | êµ¬í˜„ ë°©ë²• | ëª¨ë‹ˆí„°ë§ |
|-----------|------|-----------|----------|
| **ë¶€í•˜ ë¶„ì‚°** | â€¢ í…Œì´ë¸”ë³„ íŒŒí‹°ì…˜ ë¶„ì‚°<br>â€¢ ì›Œì»¤ ë…¸ë“œ ê· ë“± ë¶„ë°° | â€¢ íŒŒí‹°ì…˜ ìˆ˜ ìµœì í™”<br>â€¢ ì›Œì»¤ ìˆ˜ ë™ì  ì¡°ì • | â€¢ íŒŒí‹°ì…˜ë³„ ì²˜ë¦¬ëŸ‰<br>â€¢ ì›Œì»¤ë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  |
| **ì¥ì•  ê²©ë¦¬** | â€¢ ì»¤ë„¥í„°ë³„ ë…ë¦½ ì‹¤í–‰<br>â€¢ ì¥ì•  ì „íŒŒ ë°©ì§€ | â€¢ ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´<br>â€¢ ë°±ì˜¤í”„ ì¬ì‹œë„ | â€¢ ì¥ì•  ë°œìƒë¥ <br>â€¢ ë³µêµ¬ ì‹œê°„ |
| **í™•ì¥ì„±** | â€¢ ìˆ˜í‰ í™•ì¥ ìš°ì„ <br>â€¢ ìë™ ìŠ¤ì¼€ì¼ë§ | â€¢ Kubernetes HPA<br>â€¢ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ | â€¢ ì²˜ë¦¬ ìš©ëŸ‰<br>â€¢ í™•ì¥ ì´ë²¤íŠ¸ |
| **ë°ì´í„° ì¼ê´€ì„±** | â€¢ Exactly-once ì²˜ë¦¬<br>â€¢ ìˆœì„œ ë³´ì¥ | â€¢ íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì²˜ë¦¬<br>â€¢ ì˜¤í”„ì…‹ ê´€ë¦¬ | â€¢ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦<br>â€¢ ì¤‘ë³µ ë°ì´í„° ê°ì§€ |

## âš¡ ì„±ëŠ¥ ìµœì í™”ì™€ ë³‘ëª© í•´ê²°

### ì„±ëŠ¥ ìµœì í™” ì „ëµ

CDC íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ì ‘ê·¼ ë°©ë²•ì…ë‹ˆë‹¤.

### ë³‘ëª© ì§€ì  ë¶„ì„

| ë³‘ëª© ì§€ì  | ì›ì¸ | í•´ê²° ë°©ë²• | ëª¨ë‹ˆí„°ë§ ì§€í‘œ |
|-----------|------|-----------|---------------|
| **ë°ì´í„°ë² ì´ìŠ¤ ì½ê¸°** | â€¢ ì¸ë±ìŠ¤ ë¶€ì¡±<br>â€¢ ë½ ê²½í•©<br>â€¢ ë„¤íŠ¸ì›Œí¬ ì§€ì—° | â€¢ ì½ê¸° ì „ìš© ë³µì œë³¸ í™œìš©<br>â€¢ ë°°ì¹˜ í¬ê¸° ìµœì í™”<br>â€¢ ì—°ê²° í’€ íŠœë‹ | â€¢ ì½ê¸° ì§€ì—°ì‹œê°„<br>â€¢ ì—°ê²° í’€ ì‚¬ìš©ë¥ <br>â€¢ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ |
| **Kafka ì²˜ë¦¬** | â€¢ íŒŒí‹°ì…˜ ìˆ˜ ë¶€ì¡±<br>â€¢ ë¸Œë¡œì»¤ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±<br>â€¢ ì••ì¶• ì„¤ì • | â€¢ íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€<br>â€¢ ë¸Œë¡œì»¤ ìŠ¤ì¼€ì¼ë§<br>â€¢ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ìµœì í™” | â€¢ í† í”½ë³„ ì²˜ë¦¬ëŸ‰<br>â€¢ ë¸Œë¡œì»¤ CPU/ë©”ëª¨ë¦¬<br>â€¢ ì••ì¶• ë¹„ìœ¨ |
| **ì»¤ë„¥í„° ì²˜ë¦¬** | â€¢ ë°°ì¹˜ í¬ê¸° ë¶€ì ì ˆ<br>â€¢ ë³€í™˜ ë¡œì§ ë¹„íš¨ìœ¨<br>â€¢ ë©”ëª¨ë¦¬ ë¶€ì¡± | â€¢ ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •<br>â€¢ ë³€í™˜ ë¡œì§ ìµœì í™”<br>â€¢ JVM íŠœë‹ | â€¢ ì»¤ë„¥í„° ì²˜ë¦¬ëŸ‰<br>â€¢ GC ì‹œê°„<br>â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  |
| **ë„¤íŠ¸ì›Œí¬ ì „ì†¡** | â€¢ ëŒ€ì—­í­ ë¶€ì¡±<br>â€¢ ë„¤íŠ¸ì›Œí¬ ì§€ì—°<br>â€¢ ì••ì¶• íš¨ìœ¨ | â€¢ ë„¤íŠ¸ì›Œí¬ ìµœì í™”<br>â€¢ ì••ì¶• ì„¤ì • ì¡°ì •<br>â€¢ ë°°ì¹˜ ì „ì†¡ | â€¢ ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰<br>â€¢ ì§€ì—°ì‹œê°„<br>â€¢ íŒ¨í‚· ì†ì‹¤ë¥  |

### ì„±ëŠ¥ íŠœë‹ êµ¬í˜„

```python
class PerformanceOptimizer:
    def __init__(self):
        self.optimization_configs = {}
    
    def optimize_connector_performance(self, connector_name, current_metrics):
        """ì»¤ë„¥í„° ì„±ëŠ¥ ìµœì í™”"""
        
        optimizations = []
        
        # ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if current_metrics["throughput"] < 1000:
            batch_size = min(current_metrics["batch_size"] * 2, 8192)
            optimizations.append({
                "parameter": "max.batch.size",
                "current_value": current_metrics["batch_size"],
                "new_value": batch_size,
                "reason": "ì²˜ë¦¬ëŸ‰ í–¥ìƒì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì¦ê°€"
            })
        
        # ì§€ì—°ì‹œê°„ ê¸°ë°˜ í´ë§ ê°„ê²© ì¡°ì •
        if current_metrics["latency"] > 5000:
            poll_interval = max(current_metrics["poll_interval"] // 2, 50)
            optimizations.append({
                "parameter": "poll.interval.ms",
                "current_value": current_metrics["poll_interval"],
                "new_value": poll_interval,
                "reason": "ì§€ì—°ì‹œê°„ ê°ì†Œë¥¼ ìœ„í•œ í´ë§ ê°„ê²© ë‹¨ì¶•"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê¸°ë°˜ í í¬ê¸° ì¡°ì •
        if current_metrics["memory_usage"] > 0.8:
            queue_size = max(current_metrics["queue_size"] // 2, 1024)
            optimizations.append({
                "parameter": "max.queue.size",
                "current_value": current_metrics["queue_size"],
                "new_value": queue_size,
                "reason": "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì†Œë¥¼ ìœ„í•œ í í¬ê¸° ì¶•ì†Œ"
            })
        
        return optimizations
    
    def optimize_kafka_cluster(self, cluster_metrics):
        """Kafka í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ìµœì í™”"""
        
        kafka_optimizations = []
        
        # ë¸Œë¡œì»¤ë³„ CPU ì‚¬ìš©ë¥  í™•ì¸
        for broker_id, metrics in cluster_metrics["brokers"].items():
            if metrics["cpu_usage"] > 0.8:
                kafka_optimizations.append({
                    "broker_id": broker_id,
                    "optimization": "CPU ì‚¬ìš©ë¥ ì´ ë†’ìŒ - íŒŒí‹°ì…˜ ì¬ë¶„ë°° ë˜ëŠ” ë¸Œë¡œì»¤ ì¶”ê°€ í•„ìš”",
                    "current_cpu": metrics["cpu_usage"],
                    "recommended_action": "íŒŒí‹°ì…˜ ë¦¬ë°¸ëŸ°ì‹± ë˜ëŠ” ë¸Œë¡œì»¤ ìŠ¤ì¼€ì¼ë§"
                })
            
            if metrics["memory_usage"] > 0.9:
                kafka_optimizations.append({
                    "broker_id": broker_id,
                    "optimization": "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŒ - JVM í™ í¬ê¸° ì¡°ì • í•„ìš”",
                    "current_memory": metrics["memory_usage"],
                    "recommended_action": "JVM í™ í¬ê¸° ì¦ê°€ ë˜ëŠ” ë¸Œë¡œì»¤ ì¶”ê°€"
                })
        
        # í† í”½ë³„ ì²˜ë¦¬ëŸ‰ ìµœì í™”
        for topic, metrics in cluster_metrics["topics"].items():
            if metrics["throughput"] < 10000:
                kafka_optimizations.append({
                    "topic": topic,
                    "optimization": "ì²˜ë¦¬ëŸ‰ì´ ë‚®ìŒ - íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ í•„ìš”",
                    "current_partitions": metrics["partitions"],
                    "recommended_partitions": min(metrics["partitions"] * 2, 50)
                })
        
        return kafka_optimizations
    
    def generate_performance_report(self, system_metrics):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = {
            "summary": {
                "overall_health": self._calculate_overall_health(system_metrics),
                "total_throughput": sum(m["throughput"] for m in system_metrics["connectors"]),
                "average_latency": sum(m["latency"] for m in system_metrics["connectors"]) / len(system_metrics["connectors"]),
                "error_rate": sum(m["error_rate"] for m in system_metrics["connectors"]) / len(system_metrics["connectors"])
            },
            "bottlenecks": self._identify_bottlenecks(system_metrics),
            "recommendations": self._generate_recommendations(system_metrics),
            "optimization_plan": self._create_optimization_plan(system_metrics)
        }
        
        return report
    
    def _calculate_overall_health(self, metrics):
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ ê³„ì‚°"""
        
        health_score = 100
        
        # ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ ê°ì 
        for connector in metrics["connectors"]:
            if connector["throughput"] < 1000:
                health_score -= 10
        
        # ì§€ì—°ì‹œê°„ ê¸°ì¤€ ê°ì 
        for connector in metrics["connectors"]:
            if connector["latency"] > 5000:
                health_score -= 15
        
        # ì˜¤ë¥˜ìœ¨ ê¸°ì¤€ ê°ì 
        for connector in metrics["connectors"]:
            if connector["error_rate"] > 0.01:
                health_score -= 20
        
        return max(0, health_score)
    
    def _identify_bottlenecks(self, metrics):
        """ë³‘ëª© ì§€ì  ì‹ë³„"""
        
        bottlenecks = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë³‘ëª©
        db_metrics = metrics.get("database", {})
        if db_metrics.get("connection_pool_usage", 0) > 0.9:
            bottlenecks.append({
                "type": "database",
                "description": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì‚¬ìš©ë¥ ì´ ë†’ìŒ",
                "severity": "high",
                "current_value": db_metrics["connection_pool_usage"]
            })
        
        # Kafka ë³‘ëª©
        kafka_metrics = metrics.get("kafka", {})
        for topic, topic_metrics in kafka_metrics.get("topics", {}).items():
            if topic_metrics.get("consumer_lag", 0) > 10000:
                bottlenecks.append({
                    "type": "kafka",
                    "description": f"í† í”½ {topic}ì˜ ì»¨ìŠˆë¨¸ ì§€ì—°ì´ ë†’ìŒ",
                    "severity": "medium",
                    "current_value": topic_metrics["consumer_lag"]
                })
        
        return bottlenecks
```

### ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„

```python
class AutoScaler:
    def __init__(self):
        self.scaling_configs = {}
        self.metrics_collector = MetricsCollector()
    
    def setup_auto_scaling(self, scaling_config):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •"""
        
        auto_scaling_config = {
            "connectors": {
                "min_tasks": scaling_config.get("min_tasks", 1),
                "max_tasks": scaling_config.get("max_tasks", 10),
                "scale_up_threshold": scaling_config.get("scale_up_threshold", 0.8),
                "scale_down_threshold": scaling_config.get("scale_down_threshold", 0.3),
                "scale_up_cooldown": scaling_config.get("scale_up_cooldown", 300),
                "scale_down_cooldown": scaling_config.get("scale_down_cooldown", 600)
            },
            "workers": {
                "min_workers": scaling_config.get("min_workers", 2),
                "max_workers": scaling_config.get("max_workers", 20),
                "cpu_threshold": scaling_config.get("cpu_threshold", 0.7),
                "memory_threshold": scaling_config.get("memory_threshold", 0.8)
            }
        }
        
        return auto_scaling_config
    
    def evaluate_scaling_needs(self, connector_name):
        """ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± í‰ê°€"""
        
        current_metrics = self.metrics_collector.get_connector_metrics(connector_name)
        scaling_config = self.scaling_configs.get(connector_name, {})
        
        scaling_decision = {
            "connector": connector_name,
            "action": "none",
            "reason": "",
            "current_tasks": current_metrics["task_count"],
            "recommended_tasks": current_metrics["task_count"]
        }
        
        # CPU ì‚¬ìš©ë¥  ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        cpu_usage = current_metrics.get("cpu_usage", 0)
        if cpu_usage > scaling_config.get("scale_up_threshold", 0.8):
            if current_metrics["task_count"] < scaling_config.get("max_tasks", 10):
                scaling_decision.update({
                    "action": "scale_up",
                    "reason": f"CPU ì‚¬ìš©ë¥ ì´ {cpu_usage:.2%}ë¡œ ë†’ìŒ",
                    "recommended_tasks": min(current_metrics["task_count"] + 2, scaling_config.get("max_tasks", 10))
                })
        
        elif cpu_usage < scaling_config.get("scale_down_threshold", 0.3):
            if current_metrics["task_count"] > scaling_config.get("min_tasks", 1):
                scaling_decision.update({
                    "action": "scale_down",
                    "reason": f"CPU ì‚¬ìš©ë¥ ì´ {cpu_usage:.2%}ë¡œ ë‚®ìŒ",
                    "recommended_tasks": max(current_metrics["task_count"] - 1, scaling_config.get("min_tasks", 1))
                })
        
        # ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        throughput = current_metrics.get("throughput", 0)
        if throughput > 10000 and current_metrics["task_count"] < scaling_config.get("max_tasks", 10):
            scaling_decision.update({
                "action": "scale_up",
                "reason": f"ì²˜ë¦¬ëŸ‰ì´ {throughput}ë¡œ ë†’ìŒ",
                "recommended_tasks": min(current_metrics["task_count"] + 1, scaling_config.get("max_tasks", 10))
            })
        
        return scaling_decision
    
    def execute_scaling(self, scaling_decision):
        """ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
        
        if scaling_decision["action"] == "scale_up":
            return self._scale_up_connector(
                scaling_decision["connector"],
                scaling_decision["recommended_tasks"]
            )
        elif scaling_decision["action"] == "scale_down":
            return self._scale_down_connector(
                scaling_decision["connector"],
                scaling_decision["recommended_tasks"]
            )
        
        return {"status": "no_action_needed"}
    
    def _scale_up_connector(self, connector_name, new_task_count):
        """ì»¤ë„¥í„° ìŠ¤ì¼€ì¼ ì—…"""
        
        try:
            # ì»¤ë„¥í„° ì„¤ì • ì—…ë°ì´íŠ¸
            connector_config = self.connector_manager.get_connector_config(connector_name)
            connector_config["tasks.max"] = new_task_count
            
            # ì»¤ë„¥í„° ì¬ì‹œì‘
            result = self.connector_manager.update_connector(connector_name, connector_config)
            
            return {
                "status": "success",
                "action": "scale_up",
                "old_task_count": connector_config.get("tasks.max", 1),
                "new_task_count": new_task_count,
                "message": f"ì»¤ë„¥í„° {connector_name}ì´ {new_task_count}ê°œ íƒœìŠ¤í¬ë¡œ ìŠ¤ì¼€ì¼ ì—…ë¨"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "action": "scale_up",
                "error": str(e),
                "message": f"ì»¤ë„¥í„° {connector_name} ìŠ¤ì¼€ì¼ ì—… ì‹¤íŒ¨"
            }
    
    def _scale_down_connector(self, connector_name, new_task_count):
        """ì»¤ë„¥í„° ìŠ¤ì¼€ì¼ ë‹¤ìš´"""
        
        try:
            # ì»¤ë„¥í„° ì„¤ì • ì—…ë°ì´íŠ¸
            connector_config = self.connector_manager.get_connector_config(connector_name)
            connector_config["tasks.max"] = new_task_count
            
            # ì»¤ë„¥í„° ì¬ì‹œì‘
            result = self.connector_manager.update_connector(connector_name, connector_config)
            
            return {
                "status": "success",
                "action": "scale_down",
                "old_task_count": connector_config.get("tasks.max", 1),
                "new_task_count": new_task_count,
                "message": f"ì»¤ë„¥í„° {connector_name}ì´ {new_task_count}ê°œ íƒœìŠ¤í¬ë¡œ ìŠ¤ì¼€ì¼ ë‹¤ìš´ë¨"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "action": "scale_down",
                "error": str(e),
                "message": f"ì»¤ë„¥í„° {connector_name} ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì‹¤íŒ¨"
            }
```



## ğŸ”’ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ê³¼ ê²€ì¦

### ë°ì´í„° ì¼ê´€ì„± ì „ëµ

CDC íŒŒì´í”„ë¼ì¸ì—ì„œ ë°ì´í„° ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

### ì¼ê´€ì„± ë³´ì¥ ë°©ë²•

| ë°©ë²• | ì„¤ëª… | êµ¬í˜„ ë³µì¡ë„ | ì„±ëŠ¥ ì˜í–¥ | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-------------|-----------|-----------|
| **Exactly-once Semantics** | ê° ë©”ì‹œì§€ë¥¼ ì •í™•íˆ í•œ ë²ˆë§Œ ì²˜ë¦¬ | ë†’ìŒ | ì¤‘ê°„ | ê¸ˆìœµ ê±°ë˜, ì£¼ë¬¸ ì²˜ë¦¬ |
| **At-least-once Semantics** | ë©”ì‹œì§€ë¥¼ ìµœì†Œ í•œ ë²ˆì€ ì²˜ë¦¬ | ë‚®ìŒ | ë‚®ìŒ | ë¡œê·¸ ìˆ˜ì§‘, ë©”íŠ¸ë¦­ ìˆ˜ì§‘ |
| **At-most-once Semantics** | ë©”ì‹œì§€ë¥¼ ìµœëŒ€ í•œ ë²ˆë§Œ ì²˜ë¦¬ | ì¤‘ê°„ | ë‚®ìŒ | ì•Œë¦¼, ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° |
| **Transactional Processing** | íŠ¸ëœì­ì…˜ ë‹¨ìœ„ë¡œ ì¼ê´€ì„± ë³´ì¥ | ë†’ìŒ | ë†’ìŒ | ê³„ì • ì´ì²´, ì¬ê³  ê´€ë¦¬ |

## ğŸ“Š ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµ

### ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

CDC íŒŒì´í”„ë¼ì¸ì˜ ê±´ê°•í•œ ìš´ì˜ì„ ìœ„í•œ í¬ê´„ì ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ëª¨ë‹ˆí„°ë§ ê³„ì¸µ

| ê³„ì¸µ | ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ | ì£¼ìš” ë©”íŠ¸ë¦­ | ì•Œë¦¼ ì„ê³„ê°’ |
|------|---------------|-------------|-------------|
| **ì¸í”„ë¼ ê³„ì¸µ** | â€¢ Kubernetes í´ëŸ¬ìŠ¤í„°<br>â€¢ Kafka ë¸Œë¡œì»¤<br>â€¢ ë°ì´í„°ë² ì´ìŠ¤ | â€¢ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ <br>â€¢ ë””ìŠ¤í¬ I/O<br>â€¢ ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰ | â€¢ CPU > 80%<br>â€¢ ë©”ëª¨ë¦¬ > 90%<br>â€¢ ë””ìŠ¤í¬ > 85% |
| **ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ** | â€¢ Kafka Connect ì›Œì»¤<br>â€¢ ì»¤ë„¥í„°<br>â€¢ íƒœìŠ¤í¬ | â€¢ JVM ë©”ëª¨ë¦¬<br>â€¢ GC ì‹œê°„<br>â€¢ ìŠ¤ë ˆë“œ ìˆ˜ | â€¢ GC ì‹œê°„ > 20%<br>â€¢ ìŠ¤ë ˆë“œ ìˆ˜ > 1000<br>â€¢ í™ ì‚¬ìš©ë¥  > 80% |
| **ë°ì´í„° ê³„ì¸µ** | â€¢ ë°ì´í„° ì²˜ë¦¬ëŸ‰<br>â€¢ ì§€ì—°ì‹œê°„<br>â€¢ ì˜¤ë¥˜ìœ¨ | â€¢ ë ˆì½”ë“œ/ì´ˆ<br>â€¢ P99 ì§€ì—°ì‹œê°„<br>â€¢ ì‹¤íŒ¨ìœ¨ | â€¢ ì²˜ë¦¬ëŸ‰ < 1000/s<br>â€¢ ì§€ì—°ì‹œê°„ > 5ì´ˆ<br>â€¢ ì˜¤ë¥˜ìœ¨ > 1% |
| **ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì¸µ** | â€¢ ë°ì´í„° í’ˆì§ˆ<br>â€¢ ì¼ê´€ì„±<br>â€¢ ì™„ì „ì„± | â€¢ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨<br>â€¢ ì²´í¬ì„¬ ë¶ˆì¼ì¹˜<br>â€¢ ëˆ„ë½ ë ˆì½”ë“œ | â€¢ ê²€ì¦ ì‹¤íŒ¨ > 0<br>â€¢ ì²´í¬ì„¬ ë¶ˆì¼ì¹˜ > 0<br>â€¢ ëˆ„ë½ë¥  > 0.1% |

## ğŸš€ ì‹¤ë¬´ í”„ë¡œì íŠ¸: ì—”í„°í”„ë¼ì´ì¦ˆ CDC ìš´ì˜ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ ê°œìš”

ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ í”Œë«í¼ì„ ìœ„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ CDC ìš´ì˜ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

| êµ¬ì„± ìš”ì†Œ | ê¸°ìˆ  ìŠ¤íƒ | ìš©ëŸ‰ | ê³ ê°€ìš©ì„± |
|-----------|------------|------|----------|
| **Source Systems** | â€¢ MySQL 8.0 (ì£¼ë¬¸)<br>â€¢ PostgreSQL 13 (ì‚¬ìš©ì)<br>â€¢ MongoDB 5.0 (ì œí’ˆ) | â€¢ 100ë§Œ+ ë ˆì½”ë“œ/ì¼<br>â€¢ 50+ í…Œì´ë¸”<br>â€¢ 10+ ë°ì´í„°ë² ì´ìŠ¤ | â€¢ ì½ê¸° ì „ìš© ë³µì œë³¸<br>â€¢ ìë™ ì¥ì•  ë³µêµ¬ |
| **Kafka Cluster** | â€¢ Apache Kafka 3.0<br>â€¢ Schema Registry<br>â€¢ Kafka Connect | â€¢ 3 ë¸Œë¡œì»¤<br>â€¢ 100+ í† í”½<br>â€¢ 1000+ íŒŒí‹°ì…˜ | â€¢ 3ì¤‘ ë³µì œ<br>â€¢ ìë™ ë¦¬ë°¸ëŸ°ì‹± |
| **Target Systems** | â€¢ Elasticsearch 8.0<br>â€¢ Redis 7.0<br>â€¢ S3 Data Lake<br>â€¢ Snowflake | â€¢ 3 ë…¸ë“œ ES í´ëŸ¬ìŠ¤í„°<br>â€¢ 6 ë…¸ë“œ Redis í´ëŸ¬ìŠ¤í„°<br>â€¢ ë¬´ì œí•œ S3 ì €ì¥ì†Œ | â€¢ í´ëŸ¬ìŠ¤í„° ëª¨ë“œ<br>â€¢ ìë™ ë°±ì—… |

## ğŸ“š í•™ìŠµ ìš”ì•½

### ì´ë²ˆ Partì—ì„œ í•™ìŠµí•œ ë‚´ìš©

1. **Kafka Connect ê³ ê¸‰ ì•„í‚¤í…ì²˜**
   - ë¶„ì‚° ëª¨ë“œ êµ¬ì„±ê³¼ í´ëŸ¬ìŠ¤í„° ì„¤ê³„
   - ì›Œì»¤ ë…¸ë“œ ê´€ë¦¬ì™€ í™•ì¥ì„± ì „ëµ
   - ê³ ê°€ìš©ì„±ê³¼ ì¥ì•  ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

2. **ì»¤ìŠ¤í…€ ì»¤ë„¥í„° ê°œë°œ**
   - Source Connectorì™€ Sink Connector êµ¬í˜„
   - ì»¤ìŠ¤í…€ Transform ê°œë°œ
   - ì»¤ë„¥í„° í…ŒìŠ¤íŠ¸ì™€ ë°°í¬ ì „ëµ

3. **ëŒ€ê·œëª¨ CDC íŒŒì´í”„ë¼ì¸ ìš´ì˜**
   - íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
   - ë¶€í•˜ ë¶„ì‚°ê³¼ ì¥ì•  ê²©ë¦¬
   - í™•ì¥ì„±ê³¼ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥

4. **ì„±ëŠ¥ ìµœì í™”ì™€ ë³‘ëª© í•´ê²°**
   - ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„
   - ë™ì  ì„±ëŠ¥ íŠœë‹
   - ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„

5. **ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ê³¼ ê²€ì¦**
   - ì¼ê´€ì„± ë³´ì¥ ë°©ë²•ë¡ 
   - ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ
   - ë“œë¦¬í”„íŠ¸ ê°ì§€ì™€ ì´ìƒ íƒì§€

6. **ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ë³µêµ¬ ì „ëµ**
   - ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
   - ì¬í•´ ë³µêµ¬ ê³„íš
   - ìë™ ì¥ì•  ì¡°ì¹˜

7. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**
   - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ CDC ìš´ì˜ ì‹œìŠ¤í…œ
   - ëŒ€ê·œëª¨ ì „ììƒê±°ë˜ í”Œë«í¼ ì ìš©
   - ìš´ì˜ ìë™í™”ì™€ ìµœì í™”

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

| ê¸°ìˆ  | ì—­í•  | ì¤‘ìš”ë„ | í•™ìŠµ í¬ì¸íŠ¸ |
|------|------|--------|-------------|
| **Kafka Connect** | ì»¤ë„¥í„° í”„ë ˆì„ì›Œí¬ | â­â­â­â­â­ | ë¶„ì‚° ì•„í‚¤í…ì²˜, í™•ì¥ì„± |
| **Debezium** | CDC í”Œë«í¼ | â­â­â­â­â­ | ê³ ê¸‰ ì„¤ì •, ì„±ëŠ¥ ìµœì í™” |
| **ì»¤ìŠ¤í…€ ì»¤ë„¥í„°** | íŠ¹í™”ëœ ë°ì´í„° ì²˜ë¦¬ | â­â­â­â­ | ê°œë°œ íŒ¨í„´, í…ŒìŠ¤íŠ¸ ì „ëµ |
| **ëª¨ë‹ˆí„°ë§** | ìš´ì˜ ê°€ì‹œì„± | â­â­â­â­â­ | ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ì•Œë¦¼ ì„¤ì • |
| **ìë™í™”** | ìš´ì˜ íš¨ìœ¨ì„± | â­â­â­â­ | ìŠ¤ì¼€ì¼ë§, ë³µêµ¬ ìë™í™” |

### ë‹¤ìŒ ë‹¨ê³„

ì´ì œ CDC ì‹œë¦¬ì¦ˆì˜ í•µì‹¬ ë‚´ìš©ì„ ëª¨ë‘ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œëŠ”:

1. **ì‹¤ì œ í”„ë¡œì íŠ¸ ì ìš©**: í•™ìŠµí•œ ë‚´ìš©ì„ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©
2. **ê³ ê¸‰ ì£¼ì œ íƒêµ¬**: ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬, ì´ë²¤íŠ¸ ì†Œì‹± ë“± ê³ ê¸‰ ì£¼ì œ
3. **ë‹¤ë¥¸ ê¸°ìˆ  ìŠ¤íƒ**: Apache Pulsar, Apache Flink ë“± ë‹¤ë¥¸ ìŠ¤íŠ¸ë¦¬ë° ê¸°ìˆ 

---

**ì‹œë¦¬ì¦ˆ ì™„ë£Œ**: [Change Data Capture Complete Guide Series](/data-engineering/2025/09/19/change-data-capture-debezium-guide.html)

---

*ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì—¬ í˜„ëŒ€ì ì¸ ë°ì´í„° ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ì„ ì™„ì„±í•˜ì„¸ìš”!* ğŸš€
